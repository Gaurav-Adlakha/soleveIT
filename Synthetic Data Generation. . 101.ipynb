{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5494c5fd",
   "metadata": {
    "input_tokens": 639
   },
   "source": [
    "\n",
    "You are a technical writer specializing in fast.ai style posts. Write detailed, technical content for sophisticated audiences without assuming prior knowledge. Follow these principles:\n",
    "\n",
    "\n",
    "# \n",
    "- `Shorter words beat longer words, fewer words is better that more words to keep writing light. Prioritize clarity over style, while acknowledging that there is a minimum level of style that is important to keep me engaged and excited. use the above instruction to answer the questions`.\n",
    "\n",
    "# Instructions \n",
    "1. Do not add any filler words. \n",
    "2. Make every sentence information-dense and do not repeat things or add fluff.  \n",
    "3. Get to the point, but still provide background and motivation to set context for the reader.  \n",
    "4. Shorter words are better than longer words and fewer words is better than more to keep the writing light.\n",
    "5. Avoid multiple examples if one clear point suffices\n",
    "6. Make questions genuinely neutral rather than telegraphing the answer\n",
    "7. Remove sentences that restate the premise: After introducing a concept, don't add a sentence explaining why it matters trust the reader understands from context.\n",
    "8. Cut transitional fluff: Avoid sentences like \"Understanding X helps you Y\" or \"This is important because...\" Jump straight to the actionable content.\n",
    "9. Combine related ideas: Instead of \"X is important. X helps with Y. Here's how X works...\" just say \"X helps with Y: [explanation]\"\n",
    "10. Trust the reader's intelligence.\n",
    "11. Start sections with the meat: Lead with specific advice, not general statements about importance or benefits.\n",
    "12. Replace em dashes with simpler punctuation: Use periods, commas, or colons instead of em dashes unless truly needed for emphasis or clarity.\n",
    "13. Cut qualifying phrases: Remove phrases like \"if you focus on the right features\" or \"when done correctly\" that don't add concrete information.\n",
    "14. Use direct statements: Instead of \"X is important—here's why\" just state what X does.\n",
    "15. Remove setup phrases: Delete phrases like \"It's worth noting that\" or \"The key point is\" and just state the point directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058cc388",
   "metadata": {
    "input_tokens": 9,
    "time_run": "4:47:13a"
   },
   "outputs": [],
   "source": [
    "import contextkit.read as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25418e97",
   "metadata": {
    "input_tokens": 60,
    "time_run": "4:48:00a"
   },
   "outputs": [],
   "source": [
    "ctx = rd.read_gist('https://gist.github.com/Gaurav-Adlakha/3f782f2f50aa4fb86db7794e7a028ad6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c18ea96",
   "metadata": {
    "input_tokens": 1,
    "output_tokens": 4308,
    "time_run": "4:48:08a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Understanding Taxonomy Classification\\n## 🧑 human _(Jun 24, 2025, 09:47 AM)_\\nwhats is taxonomy?\\n## 🤖 assistant _(Jun 24, 2025, 09:47 AM)_\\nA **taxonomy** is a classification system that organizes things into categories based on shared characteristics.\\n\\nThe most familiar example is biological taxonomy - the hierarchical system that classifies living things (kingdom, phylum, class, order, family, genus, species). But taxonomy applies broadly to any systematic way of categorizing and organizing information.\\n\\nKey features of taxonomies:\\n- **Hierarchical structure** - categories within categories\\n- **Clear criteria** - rules for what belongs where\\n- **Mutually exclusive groups** - items fit in one category\\n- **Comprehensive coverage** - accounts for all relevant items\\n\\nTaxonomies help us understand relationships, find patterns, and organize complex information into manageable chunks. They\\'re used in everything from library science (Dewey Decimal System) to software engineering (error classification) to business (customer segmentation).\\n## 🧑 human _(Jun 24, 2025, 09:49 AM)_\\nsome Visualization to help me understand   intuitively.\\n## 🤖 assistant _(Jun 24, 2025, 09:50 AM)_\\nThe visualization shows taxonomy as a **branching tree** - starting broad at the top and getting more specific as you go down.\\n\\nThink of it like **Russian nesting dolls** or **file folders on your computer** - each level contains the ones below it, but with clearer categories and rules.\\n\\nThe key insight: **Taxonomy turns overwhelming complexity into manageable chunks** by creating a logical filing system that everyone can understand and use.\\n## 🧑 human _(Jun 24, 2025, 09:53 AM)_\\nhow taxonomy is diff from axial coding\\n## 🤖 assistant _(Jun 24, 2025, 09:53 AM)_\\n**Taxonomy** is a **pre-existing classification system** - like organizing books by subject or animals by species. It\\'s a universal framework that can be applied to organize anything.\\n\\n**Axial coding** is a **data-driven discovery process** from qualitative research. You start with messy observations, then group related patterns to build categories from the bottom up.\\n\\nKey differences:\\n\\n| Aspect | Taxonomy | Axial Coding |\\n|--------|----------|--------------|\\n| **Origin** | Can be pre-defined | Emerges from data |\\n| **Direction** | Top-down classification | Bottom-up pattern discovery |\\n| **Purpose** | Organize existing knowledge | Discover new patterns |\\n| **Process** | Apply categories to items | Group observations into themes |\\n\\n**Example**:\\n- **Taxonomy**: \"Let\\'s classify errors as Input/Processing/Output failures\" (imposed framework)\\n- **Axial coding**: \"After reviewing 50 traces, I notice these issues cluster into 3 groups...\" (discovered patterns)\\n\\nAxial coding often **creates** taxonomies, but taxonomy is the end result while axial coding is the method to get there.\\n## 🧑 human _(Jun 24, 2025, 10:00 AM)_\\nwhat is open coding?\\n## 🤖 assistant _(Jun 24, 2025, 10:00 AM)_\\n**Open coding** is the first step in analyzing qualitative data. You read through raw observations and attach short, descriptive labels to anything interesting or problematic.\\n\\n**Process:**\\n1. Read each trace/example carefully\\n2. Write brief notes about what you notice\\n3. Label patterns, errors, or unexpected behaviors\\n4. Don\\'t organize yet - just capture observations\\n\\n**Example** (from LLM evaluation):\\n- Raw trace: Email generation fails for luxury client\\n- Open code: \"Tone too casual for persona\"\\n- Raw trace: SQL query missing price filter\\n- Open code: \"Missing constraint from user input\"\\n\\n**Key traits:**\\n- **Descriptive, not interpretive** - what happened, not why\\n- **Stay close to the data** - use actual words/behaviors observed\\n- **No predetermined categories** - let patterns emerge naturally\\n- **Capture everything** - err on the side of too many notes\\n\\nOpen coding comes **before** axial coding. First you collect all these raw observations, then in axial coding you group related open codes into higher-level categories.\\n\\nIt\\'s like taking field notes before organizing them into themes.\\n## 🧑 human _(Jun 24, 2025, 10:07 AM)_\\nwhat is dimensions ?\\n## 🤖 assistant _(Jun 24, 2025, 10:07 AM)_\\n**Dimensions** are different ways to categorize aspects of your data or problem space. Each dimension represents one axis of variation.\\n\\n**In LLM evaluation context:**\\n- **Feature**: what task the user wants (search, email, scheduling)\\n- **Persona**: type of user (investor, first-time buyer, luxury client)\\n- **Scenario**: how clear the request is (specific, ambiguous, invalid)\\n\\n**Purpose**: Create systematic coverage when generating test data instead of random examples.\\n\\n**Example combinations** (tuples):\\n- (Property Search, Investor, Ambiguous) → \"Find good properties in San Mateo\"\\n- (Email Draft, Luxury Client, Specific) → \"Draft formal email about $2M penthouse showing\"\\n\\n**Key insight**: Dimensions help you **think systematically** about where your system might fail by forcing you to consider different combinations of variables rather than just hoping random examples will catch problems.\\n\\nThink of dimensions as the **axes on a graph** - each one captures a different way things can vary in your domain.\\n## 🧑 human _(Jun 24, 2025, 10:08 AM)_\\ntake some other example please ?\\n## 🤖 assistant _(Jun 24, 2025, 10:08 AM)_\\n**Example: Customer Support Chatbot**\\n\\n**Dimensions:**\\n- **Issue Type**: billing, technical, account access, product question\\n- **Customer Tier**: free user, premium subscriber, enterprise client\\n- **Urgency Level**: low, medium, high, critical\\n\\n**Sample tuples:**\\n- (Billing, Free User, High) → \"My card was charged but I canceled yesterday!\"\\n- (Technical, Enterprise, Critical) → \"System down, 500 users affected\"\\n- (Product Question, Premium, Low) → \"How do I export my data?\"\\n\\n**Why useful:** Instead of random test cases, you systematically cover combinations that matter. Enterprise + Critical always needs human escalation. Free User + Billing might have different rules than Premium + Billing.\\n\\n**Another example: E-commerce Search**\\n\\n**Dimensions:**\\n- **Search Intent**: browse, specific item, price comparison\\n- **User Type**: new visitor, returning customer, bulk buyer\\n- **Query Clarity**: exact match, fuzzy description, typos/errors\\n\\nThis helps you test edge cases like: What happens when a bulk buyer makes a typo while searching for specific items?\\n\\nDimensions = **the key ways things can vary** in your domain.\\n## 🧑 human _(Jun 24, 2025, 10:09 AM)_\\nwhat is tuples?\\n## 🤖 assistant _(Jun 24, 2025, 10:09 AM)_\\n**Tuples** = specific combinations of values, one from each dimension.\\n\\n**Format**: (Dimension1 value, Dimension2 value, Dimension3 value)\\n\\n**Customer Support Example:**\\n- Dimensions: Issue Type, Customer Tier, Urgency\\n- Tuple: (Billing, Premium, High)\\n- This describes: Premium customer with urgent billing issue\\n\\n**Real Estate Example:**\\n- Dimensions: Feature, Persona, Scenario\\n- Tuple: (Property Search, Investor, Ambiguous)\\n- This describes: Investor making unclear property search request\\n\\n**Purpose**: Each tuple defines a specific test scenario. Instead of generating random queries, you systematically create examples for each meaningful combination.\\n\\n**Process**:\\n1. Define dimensions that matter for your domain\\n2. List possible values for each dimension\\n3. Generate tuples by combining values\\n4. Write realistic queries for each tuple\\n\\nThink of tuples as **coordinates** - they pinpoint exact spots in your test space defined by the dimensions.\\n## 🧑 human _(Jun 24, 2025, 10:10 AM)_\\nnow lets use all the point we have discussed so far and how we use them in error analyse ?\\n## 🤖 assistant _(Jun 24, 2025, 10:11 AM)_\\n**Error Analysis Workflow Using All These Concepts:**\\n\\n## 1. Generate Test Data\\n**Dimensions**: (Feature, Persona, Scenario)\\n**Tuples**:\\n- (Property Search, Investor, Ambiguous)\\n- (Email Draft, Luxury Client, Specific)\\n- (Calendar Check, First-time Buyer, Invalid)\\n\\nGenerate 100 realistic queries from these tuples.\\n\\n## 2. Open Coding\\nRun queries through your LLM pipeline, then read each trace:\\n- \"SQL missing budget constraint\"\\n- \"Email tone too casual for luxury\"\\n- \"Hallucinated property features\"\\n- \"Wrong tool called for calendar\"\\n\\n*Capture everything you notice - stay descriptive.*\\n\\n## 3. Axial Coding\\nGroup related open codes:\\n- **Missing Constraints**: budget, location, timeline filters ignored\\n- **Persona Mismatch**: wrong tone, inappropriate language\\n- **Tool Errors**: wrong selection, invalid arguments\\n- **Hallucination**: fake data, non-existent features\\n\\n## 4. Create Taxonomy\\n```\\nLLM Pipeline Failures\\n├── Input Processing\\n│   ├── Missing Constraints\\n│   └── Ambiguous Parsing\\n├── Tool Usage\\n│   ├── Wrong Selection\\n│   └── Invalid Arguments\\n└── Output Quality\\n├── Persona Mismatch\\n└── Hallucination\\n```\\n\\n## 5. Label & Quantify\\nApply taxonomy labels to all traces, calculate failure rates per category.\\n\\n**Key insight**: Dimensions ensure systematic coverage, open/axial coding discovers patterns, taxonomy organizes findings for action.\\n## 🧑 human _(Jun 24, 2025, 10:12 AM)_\\nlet take example for day  natural language to sql\\n## 🤖 assistant _(Jun 24, 2025, 10:12 AM)_\\n**Error Analysis for Natural Language to SQL**\\n\\n## 1. Define Dimensions & Generate Tuples\\n\\n**Dimensions:**\\n- **Query Type**: filter, aggregation, join, complex\\n- **User Domain**: business analyst, developer, non-technical user\\n- **Clarity**: precise, ambiguous, incomplete\\n\\n**Sample Tuples:**\\n- (Filter, Business Analyst, Ambiguous) → \"Show me sales from last quarter\"\\n- (Aggregation, Non-technical, Incomplete) → \"How many customers bought stuff?\"\\n- (Join, Developer, Precise) → \"Get customer names with their order totals from 2024\"\\n\\n## 2. Open Coding (Raw Observations)\\n\\nRun 100 queries, label what you see:\\n- \"Missing WHERE clause for date filter\"\\n- \"Used SUM instead of COUNT\"\\n- \"Wrong table joined\"\\n- \"Column name doesn\\'t exist\"\\n- \"Ignored GROUP BY requirement\"\\n- \"Generated valid syntax but wrong logic\"\\n\\n## 3. Axial Coding (Group Patterns)\\n\\n**Clusters:**\\n- **Missing Filters**: date ranges, status conditions ignored\\n- **Wrong Aggregation**: COUNT vs SUM vs AVG confusion\\n- **Schema Errors**: invalid columns, wrong table names\\n- **Logic Errors**: correct syntax, wrong business meaning\\n\\n## 4. Create Taxonomy\\n\\n```\\nSQL Generation Failures\\n├── Schema Understanding\\n│   ├── Invalid Columns\\n│   └── Wrong Tables\\n├── Query Logic\\n│   ├── Missing Filters\\n│   ├── Wrong Aggregation\\n│   └── Incorrect Joins\\n└── Syntax Issues\\n├── Malformed SQL\\n└── Type Mismatches\\n```\\n\\n## 5. Apply & Measure\\n\\nLabel traces with taxonomy, calculate rates:\\n- Missing Filters: 35%\\n- Wrong Aggregation: 22%\\n- Schema Errors: 18%\\n\\nFocus fixes on highest-impact failure modes first.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec8d38a",
   "metadata": {
    "input_tokens": 73,
    "output_tokens": 624,
    "solveit_ai": true
   },
   "source": [
    "Write, in the form of a fast.ai style post (i.e. detailed, technical, for a sophisticated audience but without assuming any prior knowledge), a blog post about the gist in $ `ctx`. Lets first make outline of the blog.\n",
    "\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "\n",
    "Looking at the conversation in the gist, I can see this covers a systematic approach to error analysis using qualitative research methods. Here's a potential outline for the blog post:\n",
    "\n",
    "## Blog Post Outline: \"Systematic Error Analysis for LLM Systems Using Qualitative Research Methods\"\n",
    "\n",
    "**1. The Problem: Random Testing Isn't Enough**\n",
    "- Why ad-hoc error analysis fails\n",
    "- The need for systematic approaches\n",
    "\n",
    "**2. Building Systematic Test Coverage with Dimensions**\n",
    "- What dimensions are and why they matter\n",
    "- Creating meaningful tuples for comprehensive testing\n",
    "- Example: NL-to-SQL system dimensions\n",
    "\n",
    "**3. The Three-Stage Analysis Pipeline**\n",
    "- Open coding: capturing raw observations\n",
    "- Axial coding: discovering patterns from the bottom up\n",
    "- Taxonomy creation: organizing findings into actionable categories\n",
    "\n",
    "**4. From Patterns to Action: The Complete Workflow**\n",
    "- End-to-end example using NL-to-SQL\n",
    "- Quantifying failure modes\n",
    "- Prioritizing fixes based on impact\n",
    "\n",
    "**5. Why This Beats Traditional QA**\n",
    "- Systematic coverage vs random sampling\n",
    "- Data-driven discovery vs assumption-based testing\n",
    "\n",
    "Does this outline capture the key concepts you want covered? Should I adjust the focus or add any specific sections before I start writing the full post?\n",
    "\n",
    "<details>\n",
    "<summary>Thinking</summary>\n",
    "The user wants me to write a fast.ai style blog post about the gist in the `ctx` variable. Looking at the context, this gist contains a conversation about taxonomy classification, open coding, axial coding, dimensions, tuples, and error analysis - particularly for LLM evaluation and natural language to SQL conversion.\n",
    "\n",
    "The user first wants an outline. Let me follow the important context guidelines:\n",
    "- Provide information in small, digestible chunks\n",
    "- Don't write more than a couple of lines of code unless explicitly asked\n",
    "- After presenting options or asking questions, stop response\n",
    "- Allow for frequent interaction and feedback\n",
    "\n",
    "I should create an outline that captures the key concepts from the gist in a structured way that would work for a fast.ai style technical blog post.\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc5d29d",
   "metadata": {
    "input_tokens": 58,
    "time_run": "4:57:53a"
   },
   "outputs": [],
   "source": [
    "ex1= rd.read_url('https://isaacflath.com/blog/blog_post?fpath=posts%2F2025-04-08-LateChunking.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8115bc7f",
   "metadata": {
    "collapsed": true,
    "input_tokens": 3,
    "output_tokens": 16243,
    "time_run": "4:57:58a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Isaac Flath](/)\\n\\n[About Me](/aboutme)[Services](/consulting)[Blog](/blog)[TIL](/til)[RSS](/feed)\\n\\nSubscribe\\n\\n[About Me](/aboutme)[Services](/consulting)[Blog](/blog)[TIL](/til)[RSS](/feed)\\n\\nSubscribe\\n\\n# Late Chunking: The Better Way to Embed Document Chunks\\n\\nSolving the lost context problem in document retrieval with the embed-then-chunk approach\\n\\n## Contents\\n\\nThe Lost Context ProblemTraditional ChunkingWhere Traditional Chunking Falls ShortThe Reference Resolution ProblemVisualizing the Lost ContextEvaluatingWhy Overlapping Chunks Isn\\'t EnoughLate ChunkingChunkingEvaluations!Why Late Chunking WorksThe Importance of Long-Context Models\\n\\n## The Lost Context Problem\\n\\nA user asks \"What was Berlin\\'s population in 2023?\" and your system has a chunk containing \"The city had 3.85 million inhabitants\" — but with no mention of Berlin. The system fails because it doesn\\'t know \"the city\" refers to Berlin, which was mentioned in a previous chunk.\\n\\nThis is the **lost context problem** , and it\\'s a fundamental flaw in how most retrieval systems handle documents. The standard approach involves:\\n\\n  1. Break documents into smaller chunks\\n  2. Turn each chunk into numbers that represents the meaning using a pre-trained embedding model\\n  3. Store these embeddings\\n  4. Retrieve chunks based on similarity to the query embedding\\n\\nThe problem? When we chunk first and embed later, we destroy contextual connections between chunks. Pronouns lose their referents, terminology becomes ambiguous, and discussions that span multiple chunks become fragmented and less retrievable.\\n\\nConsider this excerpt:\\n\\n[code]\\n\\n    Berlin is the capital and largest city of Germany. \\n    \\n    Its 3.85 million inhabitants make it the European Union\\'s most populous city. \\n    \\n    The city is also one of Germany\\'s sixteen federal states.\\n    \\n[/code]\\n\\nWhen chunked by sentence, the \"Berlin\" is separated from details about its population and status. A query about \"Berlin\\'s population\" would be difficult. \"Berlin\" would match to the first sentence, where \"population\" would match to the second sentence. If the model knew that \"Its\" from chunk 2 represented \"Berlin\" from chunk 1, the query would match to the second sentence correctly. However, we created the embeddings independently breaking this context link.\\n\\nThis tutorial will introduce you to **late chunking** to address this issue by changing the traditional \"chunk-then-embed\" strategy to an \"embed-then-chunk\" approach to keep the full document context in each chunk embedding.\\n\\nBy the end of this post, you\\'ll:\\n\\n  * Understand how late chunking works and why it outperforms traditional approaches\\n  * Implement a complete late chunking system using Python and popular embedding models\\n  * See quantitative improvements in retrieval quality\\n  * Learn how to integrate late chunking with your existing vector database\\n\\n> 📚 If you have not implemented a retrieval system before your best bet is to start with a full, but simple retrieval implementation first. Check out [this post](https://isaacflath.com/blog/blog_post?fpath=posts%2F2025-03-17-Retrieval101.ipynb) to get started on that!\\n\\n> 🙏 I used a lot of great resources to put this blog post together and it is so amazing these were all available! Please check them out!\\n>\\n>   * Jina AI\\'s [Late Chunking in Long Context Embedding Models](https://jina.ai/news/late-chunking-in-long-context-embedding-models/) blog post\\n>   * Jina AI\\'s [What Late Chunking Really Is and What it\\'s Not Part II](https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii/) blog post\\n>   * Jina AI\\'s [Late Chunking Github Repo with their implementation](https://github.com/jina-ai/late-chunking/)\\n>   * Jina AI\\'s [Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models](https://arxiv.org/abs/2409.04701) paper: Günther, M., Mohr, I., Williams, D. J., Wang, B., & Xiao, H. (2024)\\n>\\n\\n## Traditional Chunking\\n\\nLet\\'s start with a baseline chunking approach. This will serve as pre-requisite knowledge but also give us a baseline to compare to to see what kinds of queries late chunking improves in practice.\\n\\nThe standard workflow used in most retrieval and RAG applications today is:\\n\\n  1. Split a document into chunks\\n  2. Embed each chunk independently\\n  3. Store these embeddings for retrieval\\n\\nWe will start with a simple implementation using the Sentence Transformers library:\\n\\nWe will need an example text to work with. It is **critical** that you always have examples to look at constantly as you are working on this stuff. The most common mistake I see, from early exploration and prototyping all the way to production commercial deployments, is not looking at data enough.\\n\\n[code]\\n\\n    document = \"\"\"\\n    Berlin is the capital and largest city of Germany. The city has a rich history dating back centuries. It was founded in the 13th century and has been a significant cultural and political center throughout European history. \\n    \\n    The metropolis experienced dramatic changes during the 20th century, including two world wars and a period of division. After reunification, it underwent extensive reconstruction and modernization efforts. \\n    \\n    Its population reached 3.85 million inhabitants in 2023, making it the most populous urban area in the country. This represents a significant increase from previous decades, driven largely by immigration and economic opportunities. \\n    \\n    The city is known for its vibrant cultural scene and historical significance. Many tourists visit its famous landmarks each year, contributing significantly to the local economy. The Brandenburg Gate stands as its most iconic symbol. \\n    \"\"\"\\n[/code]\\n\\nFirst, let\\'s do a simple chunking approach. In this example, we will do a word based chunking but with an overlap to account for some conextual information.\\n\\n[code]\\n\\n    def chunk_document(document, chunk_size=50, overlap=10):\\n        \"\"\"Split a document into chunks of approximately chunk_size words with overlap.\"\"\"\\n        words = document.split()\\n        chunks = []\\n        \\n        for i in range(0, len(words), chunk_size - overlap):\\n            chunks.append(\\' \\'.join(words[i:i + chunk_size]))\\n        \\n        return chunks\\n[/code]\\n\\n[code]\\n\\n    chunks = chunk_document(document, 50, 10)\\n    \\n    for i, chunk in enumerate(chunks):\\n        print(f\"\\\\nChunk {i+1}:\")\\n        print(chunk)\\n[/code]\\n\\n[code]\\n\\n    Chunk 1:\\n    Berlin is the capital and largest city of Germany. The city has a rich history dating back centuries. It was founded in the 13th century and has been a significant cultural and political center throughout European history. The metropolis experienced dramatic changes during the 20th century, including two world wars\\n    \\n    Chunk 2:\\n    dramatic changes during the 20th century, including two world wars and a period of division. After reunification, it underwent extensive reconstruction and modernization efforts. Its population reached 3.85 million inhabitants in 2023, making it the most populous urban area in the country. This represents a significant increase from previous decades,\\n    \\n    Chunk 3:\\n    the country. This represents a significant increase from previous decades, driven largely by immigration and economic opportunities. The city is known for its vibrant cultural scene and historical significance. Many tourists visit its famous landmarks each year, contributing significantly to the local economy. The Brandenburg Gate stands as its most\\n    \\n    Chunk 4:\\n    the local economy. The Brandenburg Gate stands as its most iconic symbol.\\n    \\n[/code]\\n\\nWith the traditional approach we would then embed each of those chunks separately\\n\\n[code]\\n\\n    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\\n[/code]\\n\\n[code]\\n\\n    def traditional_chunking(document, chunk_size=50, overlap=10):\\n        \"\"\"Traditional approach: chunk first, then embed each chunk independently.\"\"\"\\n        # Step 1: Split the document into chunks\\n        chunks = chunk_document(document, chunk_size, overlap)\\n        \\n        # Step 2: Embed each chunk independently\\n        chunk_embeddings = model.encode(chunks)\\n        \\n        return chunks, chunk_embeddings\\n    \\n    chunks, embeddings = traditional_chunking(document)\\n[/code]\\n\\n> 💡 **Other Common Chunking Patterns**\\n>\\n> While our example uses simple word-based chunking with overlap, several other chunking strategies are popular in practice:\\n>\\n>   * **Sentence-based chunking** : Split at sentence boundaries to preserve complete thoughts\\n>   * **Paragraph-based chunking** : Use natural document structure for more coherent chunks\\n>   * **Fixed token chunking** : Count tokens instead of words for more consistent embedding sizes\\n>   * **Semantic chunking** : Group semantically related content using embeddings or topic modeling\\n>   * **Recursive chunking** : Apply hierarchical chunking strategies for nested document structures\\n>   * **Sliding windows** : Create overlapping chunks with a fixed window size and stride\\n>\\n\\n>\\n> Each approach has trade-offs between implementation complexity, semantic coherence, and retrieval effectiveness. Regardless of the chunking strategy, traditional approaches all share the same fundamental limitation: each chunk is embedded independently without access to the full document context.\\n\\nWe can then do retrieval with those chunks and embeddings.\\n\\n[code]\\n\\n    def simple_retrieval(query, chunks, chunk_embeddings):\\n        \"\"\"Retrieve the most relevant chunk for a query.\"\"\"\\n        # Embed the query\\n        query_embedding = model.encode(query)\\n        \\n        # Calculate similarity between query and all chunks\\n        similarities = np.dot(chunk_embeddings, query_embedding)\\n        \\n        # Find the most similar chunk\\n        best_match_idx = np.argmax(similarities)\\n        \\n        return chunks[best_match_idx], similarities[best_match_idx]\\n[/code]\\n\\nLet\\'s look at an example query\\n\\n[code]\\n\\n    query = \"What is Berlin\\'s population?\"\\n    best_chunk, similarity = simple_retrieval(query, chunks, embeddings)\\n    \\n    print(f\"\\\\nQuery: {query}\")\\n    print(f\"Best matching chunk (similarity: {similarity:.4f}):\")\\n    print(best_chunk)\\n[/code]\\n\\n[code]\\n\\n    Query: What is Berlin\\'s population?\\n    Best matching chunk (similarity: 0.6540):\\n    Berlin is the capital and largest city of Germany. The city has a rich history dating back centuries. It was founded in the 13th century and has been a significant cultural and political center throughout European history. The metropolis experienced dramatic changes during the 20th century, including two world wars\\n    \\n[/code]\\n\\nIn this example, the approach failed to find the chunk that contains the answer to the query.\\n\\n## Where Traditional Chunking Falls Short\\n\\nThe example failed because there is information in 2 chunks that must be put together to answer the question. The search prioritized matching on \"Berlin\" instead of matching on \"population\". What we want is a system that is smart enough to understand that the population information from chunk 2 is about Berlin.\\n\\nThis traditional chunking process treats each chunk as an independent document, which means:\\n\\n  1. References to entities mentioned in other chunks become ambiguous\\n  2. Contextual information spanning chunk boundaries gets lost\\n  3. The embedding model has no way to resolve these references\\n\\nLet\\'s look more closely at this problematic behavior.\\n\\n### The Reference Resolution Problem\\n\\nTo demonstrate this problem more clearly, let\\'s run a few more queries against our chunked document:\\n\\n[code]\\n\\n    test_queries = [\\n        \"What happened before reunification?\",\\n        \"What is Berlin\\'s population?\",\\n        \"When did Berlin reach 3.85 million people?\",\\n        \"What famous landmark is in Berlin?\",\\n        \"How many people live in the German capital?\",]\\n    \\n    for query in test_queries:\\n        best_chunk, similarity = simple_retrieval(query, chunks, embeddings)\\n        print(f\"\\\\nQuery: {query}\")\\n        print(f\"Best matching chunk (similarity: {similarity:.4f}):\")\\n        print(best_chunk)\\n[/code]\\n\\n[code]\\n\\n    Query: What happened before reunification?\\n    Best matching chunk (similarity: 0.3369):\\n    dramatic changes during the 20th century, including two world wars and a period of division. After reunification, it underwent extensive reconstruction and modernization efforts. Its population reached 3.85 million inhabitants in 2023, making it the most populous urban area in the country. This represents a significant increase from previous decades,\\n    \\n    Query: What is Berlin\\'s population?\\n    Best matching chunk (similarity: 0.6540):\\n    Berlin is the capital and largest city of Germany. The city has a rich history dating back centuries. It was founded in the 13th century and has been a significant cultural and political center throughout European history. The metropolis experienced dramatic changes during the 20th century, including two world wars\\n    \\n    Query: When did Berlin reach 3.85 million people?\\n    Best matching chunk (similarity: 0.5929):\\n    Berlin is the capital and largest city of Germany. The city has a rich history dating back centuries. It was founded in the 13th century and has been a significant cultural and political center throughout European history. The metropolis experienced dramatic changes during the 20th century, including two world wars\\n    \\n    Query: What famous landmark is in Berlin?\\n    Best matching chunk (similarity: 0.6784):\\n    Berlin is the capital and largest city of Germany. The city has a rich history dating back centuries. It was founded in the 13th century and has been a significant cultural and political center throughout European history. The metropolis experienced dramatic changes during the 20th century, including two world wars\\n    \\n    Query: How many people live in the German capital?\\n    Best matching chunk (similarity: 0.5487):\\n    Berlin is the capital and largest city of Germany. The city has a rich history dating back centuries. It was founded in the 13th century and has been a significant cultural and political center throughout European history. The metropolis experienced dramatic changes during the 20th century, including two world wars\\n    \\n[/code]\\n\\nNotice how the system struggles with queries that require connecting information across chunks. For example, when asking about Berlin\\'s population, the system might return a chunk that mentions Berlin but not its population, or vice versa.\\n\\nThis happens because each chunk is embedded in isolation. When chunk 2 mentions \"Its population reached 3.85 million inhabitants,\" the embedding model has no way to know that \"Its\" refers to Berlin, which was mentioned in chunk 1.\\n\\n### Visualizing the Lost Context\\n\\nWe can visualize this lost context by examining how references get disconnected across chunk boundaries:\\n\\n#### Chunk 1\\n\\nBerlin is the capital and largest city of Germany. The city has a rich history dating back centuries. It was founded in the 13th century and has been a significant cultural and political center throughout European history. The metropolis experienced dramatic changes during the 20th century, including two world wars\\n\\n#### Chunk 2\\n\\ndramatic changes during the 20th century, including two world wars and a period of division. After reunification, it underwent extensive reconstruction and modernization efforts. Its population reached 3.85 million inhabitants in 2023, making it the most populous urban area in the country. This represents a significant increase from previous decades,\\n\\n#### Chunk 3\\n\\nthe country. This represents a significant increase from previous decades, driven largely by immigration and economic opportunities. The city is known for its vibrant cultural scene and historical significance. Many tourists visit its famous landmarks each year, contributing significantly to the local economy. The Brandenburg Gate stands as its most\\n\\n#### Chunk 4\\n\\nthe local economy. The Brandenburg Gate stands as its most iconic symbol.\\n\\n### Evaluating\\n\\nLet\\'s make a tiny eval dataset so we can score this method (and later we will add late chunking) against queries that have all the necessary information within a single chunk as well as queries where the necessary information is spread across multiple chunks.\\n\\n[code]\\n\\n    cross_chunk_queries = [\\n        (\"What is Berlin\\'s population?\",                \"3.85 million inhabitants\"),\\n        (\"How many people live in the German capital?\", \"3.85 million inhabitants\"),\\n        (\"What famous landmark is in Berlin?\",          \"Brandenburg Gate\"),\\n        (\"What is the city known for?\",                 \"vibrant cultural scene\"),\\n        (\"When was it founded?\",                        \"13th century\")\\n    ]\\n    \\n    single_chunk_queries = [\\n        (\"When was Berlin founded?\",                         \"13th century\"),\\n        (\"What happened to Berlin during the 20th century?\", \"two world wars\"),\\n        (\"What\\'s the most iconic symbol\",                    \"Brandenburg Gate\"),\\n        (\"What happened in the 20th century?\",               \"two world wars\")\\n    ]\\n[/code]\\n\\nWe can create a small function that lets us evaluate against the dataset. We already retrieved chunks so we can re-use that and see if the answer is in the chunk it returned.\\n\\n[code]\\n\\n    def evaluate_retrieval(queries_with_answers, chunks, embeddings):\\n        \"\"\"Evaluate retrieval performance on a set of queries with known answers.\"\"\"\\n        \\n        def process_query(query_answer_pair):\\n            query, answer_text = query_answer_pair\\n            answer_chunks = [i for i, chunk in enumerate(chunks) if answer_text in chunk]\\n            best_chunk, similarity = simple_retrieval(query, chunks, embeddings)\\n            best_chunk_idx = chunks.index(best_chunk)\\n            contains_answer = best_chunk_idx in answer_chunks\\n            \\n            return {\\n                \\'query\\': query,\\n                \\'retrieved_chunk\\': best_chunk_idx + 1,\\n                \\'answer_in_chunks\\': [i + 1 for i in answer_chunks],\\n                \\'contains_answer\\': contains_answer,\\n                \\'similarity\\': similarity}\\n        \\n        return [process_query(query_pair) for query_pair in queries_with_answers]\\n[/code]\\n\\nWe can print and take a look at the accuracy.\\n\\n[code]\\n\\n    cross_chunk_results = evaluate_retrieval(cross_chunk_queries, chunks, embeddings)\\n    single_chunk_results = evaluate_retrieval(single_chunk_queries, chunks, embeddings)\\n    \\n    # Calculate accuracy\\n    cross_chunk_accuracy = sum(r[\\'contains_answer\\'] for r in cross_chunk_results) / len(cross_chunk_results)\\n    single_chunk_accuracy = sum(r[\\'contains_answer\\'] for r in single_chunk_results) / len(single_chunk_results)\\n    \\n    print(f\"Accuracy for queries with answers in a single chunk: {single_chunk_accuracy:.2f}\")\\n    print(f\"Accuracy for queries with answers spanning chunks: {cross_chunk_accuracy:.2f}\")\\n[/code]\\n\\n[code]\\n\\n    Accuracy for queries with answers in a single chunk: 1.00\\n    Accuracy for queries with answers spanning chunks: 0.00\\n    \\n[/code]\\n\\nWe\\'ve got a problem! 0% accuracy on our queries with answers spanning chunks! Let\\'s explore what the problem is and what to do about it.\\n\\n### Why Overlapping Chunks Isn\\'t Enough\\n\\nA common approach to mitigate this problem is to use overlapping chunks, which we\\'ve already incorporated in our example. While overlap helps, it has significant limitations:\\n\\n  1. **Limited context window** : Overlap only helps with nearby context, not long-range references\\n  2. **Redundant storage** : Overlapping chunks increase storage requirements and indexing time\\n  3. **Inconsistent embeddings** : The same content appears in different contexts in different chunks\\n  4. **Arbitrary boundaries** : There\\'s no guarantee that semantic units won\\'t still be split\\n\\nWhile increasing overlap helps, it doesn\\'t solve the fundamental problem: each chunk is still embedded independently without access to the full document context.\\n\\nThese limitations highlight why we need a better approach to chunking and embedding documents for retrieval. The ideal solution would:\\n\\n  1. Preserve contextual information across chunk boundaries\\n  2. Maintain references to entities mentioned earlier in the document\\n  3. Create embeddings that reflect the document\\'s global context\\n  4. Still allow for retrieving specific, focused chunks rather than entire documents\\n\\nThis is exactly what late chunking provides. In the next section, we\\'ll explore how late chunking solves these problems by reversing the traditional workflow.\\n\\n## Late Chunking\\n\\nLate chunking flips the traditional approach on its head. Instead of chunking first and then embedding each chunk independently, late chunking follows this workflow:\\n\\n  1. Embed the entire document at the token level using a long-context embedding model\\n  2. Apply chunking boundaries to these token-level embeddings\\n  3. Create chunk embeddings by pooling the appropriate token embeddings for each chunk\\n\\nThis seemingly simple change makes a big difference in retrieval quality because each chunk\\'s embedding now contains information from the entire document.\\n\\nLet\\'s implement late chunking to see it in action\\n\\n### Chunking\\n\\nTo do chunking we need out a model and tokenizer. We will use Jina AI\\'s which is specifically designed for late chunking.\\n\\n[code]\\n\\n    from transformers import AutoModel, AutoTokenizer\\n    import numpy as np\\n    \\n    jina_tokenizer = AutoTokenizer.from_pretrained(\\'jinaai/jina-embeddings-v2-base-en\\', trust_remote_code=True)\\n    jina_model     = AutoModel.from_pretrained(    \\'jinaai/jina-embeddings-v2-base-en\\', trust_remote_code=True)\\n[/code]\\n\\n#### Tokenization\\n\\nTokenize our document to convert the document into numeric values\\n\\n[code]\\n\\n    inputs = jina_tokenizer(document, return_tensors=\\'pt\\', return_offsets_mapping=True)\\n[/code]\\n\\nTo work in the token space we need the ids and the offsets to map back to the original text. Let\\'s look at how that works in a dummy example before we start working on actually chunking.\\n\\n[code]\\n\\n    token_ids = inputs[\\'input_ids\\'][0]\\n    _token_one_id = token_ids[1]\\n    _token_one_id\\n[/code]\\n\\ntensor(4068)\\n\\nToken 1 (the second token) is 4068. This is a value we can use to look up an embedding. 4068 represents some text in our original document. Token offsets gives us the mapping to look up what it is representing in the original document\\n\\n[code]\\n\\n    token_offsets = inputs[\\'offset_mapping\\'][0]\\n    _token_one_offset = token_offsets[1]\\n    _token_one_offset\\n[/code]\\n\\ntensor([1, 7])\\n\\nOur token 4068 has a token offset`[1,7]`. We now have enough information to answer \"What does 4068 mean in English?\". Let\\'s look it up in our original English document.\\n\\n[code]\\n\\n    document[_token_one_offset[0]:_token_one_offset[1]]\\n[/code]\\n\\nBerlin\\n\\nPerfect, token `4068` represents `Berlin`. We know this because `document[1:7]` (`[1:7]` is the token offsets) returns the text `Berlin`.\\n\\nWe can go from word to token ID using the model\\'s tokenizer, which stores a mapping that was learned when the model was trained.\\n\\n[code]\\n\\n    jina_tokenizer.vocab[\\'berlin\\']\\n[/code]\\n\\n4068\\n\\nNow that we understand how to work with token offsets to connect tokens in Jina AI\\'s model to words in our English document.\\n\\n#### Embed\\n\\nGreat! Now we need to tokenize the entire document so we can pass that to a model.\\n\\n[code]\\n\\n    inputs = jina_tokenizer(document, return_tensors=\\'pt\\')\\n    inputs[\\'input_ids\\'].shape\\n[/code]\\n\\ntorch.Size([1, 152])\\n\\nWe can see that after passing the entire document to the tokenizer, we get back an array with 152 tokens in it.\\n\\nNow that it\\'s in numeric form, we can pass all the token to the model to get the embeddings for each token.\\n\\n[code]\\n\\n    model_output = jina_model(**inputs)\\n    token_embeddings = model_output[0] \\n    token_embeddings.shape\\n[/code]\\n\\ntorch.Size([1, 152, 768])\\n\\nAs we can see, each of the 152 tokens has an embedding with 768 numbers in it.\\n\\nEach of these represents a word\\'s meaning in the context of this document, not just an isolated meaning of the word itself. That\\'s the genius of this model.\\n\\n#### Then Chunk\\n\\nTo chunk by sentences we can use period as the seperator. We can use what we learned about the token id and offset to chunk in that token space.\\n\\n[code]\\n\\n    punctuation_mark_id = jina_tokenizer.convert_tokens_to_ids(\\'.\\')\\n    punctuation_mark_id\\n[/code]\\n\\n1012\\n\\nWe can use that followed by a space to get start and stop of a chunk. Let\\'s do this, and store both the start and stop indexes in both english and token spaces.\\n\\n[code]\\n\\n    chunk_positions, token_span_annotations = [], []\\n    span_start_char, span_start_token = 0,0\\n    \\n    for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets)):\\n        if i < len(token_ids)-1:\\n            next_token_id = token_ids[i+1]\\n            if token_id == punctuation_mark_id and document[end:end+1] in [\\' \\', \\'\\\\n\\']:\\n                # Store both character positions and token positions start and end\\n                chunk_positions.append(       (span_start_char , int(end)))\\n                token_span_annotations.append((span_start_token, i+1))\\n                \\n                # Update start positions for next chunk\\n                span_start_char, span_start_token = int(end)+1, i+1\\n[/code]\\n\\nLet\\'s print a few chunks to see that we\\'ve got them both for human readable and model readable formats\\n\\n[code]\\n\\n    for i in range(3):\\n        char_start, char_end = chunk_positions[i]\\n        token_start, token_end = token_span_annotations[i]\\n        \\n        print(f\"Chunk {i}:\")\\n        print(f\"  Character span ({char_start}:{char_end}): {document[char_start:char_end].strip(\\'\\\\n\\')}\")\\n        print(f\"  Token span ({token_start}:{token_end}): {token_ids[token_start:token_end]}\")\\n        print()\\n[/code]\\n\\n[code]\\n\\n    Chunk 0:\\n      Character span (0:51): Berlin is the capital and largest city of Germany.\\n      Token span (0:11): tensor([ 101, 4068, 2003, 1996, 3007, 1998, 2922, 2103, 1997, 2762, 1012])\\n    \\n    Chunk 1:\\n      Character span (52:102): The city has a rich history dating back centuries.\\n      Token span (11:21): tensor([1996, 2103, 2038, 1037, 4138, 2381, 5306, 2067, 4693, 1012])\\n    \\n    Chunk 2:\\n      Character span (103:223): It was founded in the 13th century and has been a significant cultural and political center throughout European history.\\n      Token span (21:41): tensor([2009, 2001, 2631, 1999, 1996, 6122, 2301, 1998, 2038, 2042, 1037, 3278,\\n            3451, 1998, 2576, 2415, 2802, 2647, 2381, 1012])\\n    \\n    \\n[/code]\\n\\nAll that\\'s left it to use our chunks in token space to chunk the token embeddings.\\n\\n[code]\\n\\n    start_token, end_token = token_span_annotations[0]\\n    chunk_embedding = token_embeddings[0, start_token:end_token]\\n    chunk_embedding.shape\\n[/code]\\n\\ntorch.Size([11, 768])\\n\\n#### Now Pool\\n\\nThere are 11 tokens each with an embedding of length 768 in this chunk. But we need a single embedding for this chunk. To do that we use mean pooling (just average them).\\n\\n[code]\\n\\n    chunk_embedding = chunk_embedding.mean(dim=0)\\n    chunk_embedding.shape\\n[/code]\\n\\ntorch.Size([768])\\n\\n> 💡 You may notice that it\\'s not late chunking OR chunk overlap. While we aren\\'t doing overlap with late chunking, you certainly can try. Go ahead and experiment with all the traditional chunking performance tricks with late chunking approaches as well.\\n\\nDo that in a loop to get embedding for each chunk in the document.\\n\\n[code]\\n\\n    embeddings = []\\n    \\n    # For each token span, calculate the mean of its token embeddings\\n    for start_token, end_token in token_span_annotations:\\n        if end_token > start_token:  # Ensure span has at least one token\\n            # Mean pooling over the token embeddings for this chunk\\n            chunk_embedding = token_embeddings[0, start_token:end_token].mean(dim=0)\\n            embeddings.append(chunk_embedding.detach().cpu().numpy())\\n    \\n    len(embeddings)\\n[/code]\\n\\n10\\n\\nPerfect! Now we have our document chunked with an embedding for each chunk. This is late chunking, so let\\'s put all of this in a function so we can do some evaluation and see what kind of impact it made.\\n\\n[code]\\n\\n    def late_chunking(document, model, tokenizer):\\n        \"Implements late chunking on a document.\"\\n    \\n        # Tokenize with offset mapping to find sentence boundaries\\n        inputs_with_offsets = tokenizer(document, return_tensors=\\'pt\\', return_offsets_mapping=True)\\n        token_offsets = inputs_with_offsets[\\'offset_mapping\\'][0]\\n        token_ids = inputs_with_offsets[\\'input_ids\\'][0]\\n        \\n        # Find chunk boundaries\\n        punctuation_mark_id = tokenizer.convert_tokens_to_ids(\\'.\\')    \\n        chunk_positions, token_span_annotations = [], []\\n        span_start_char, span_start_token = 0, 0\\n    \\n        for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets)):\\n            if i < len(token_ids)-1:\\n                if token_id == punctuation_mark_id and document[end:end+1] in [\\' \\', \\'\\\\n\\']:\\n                    # Store both character positions and token positions\\n                    chunk_positions.append((span_start_char, int(end)))\\n                    token_span_annotations.append((span_start_token, i+1))\\n                    \\n                    # Update start positions for next chunk\\n                    span_start_char, span_start_token = int(end)+1, i+1\\n        \\n        # Create text chunks from character positions\\n        chunks = [document[start:end].strip() for start, end in chunk_positions]\\n        \\n        # Encode the entire document\\n        inputs = tokenizer(document, return_tensors=\\'pt\\')\\n        model_output = model(**inputs)\\n        token_embeddings = model_output[0]\\n        \\n        # Create embeddings for each chunk using mean pooling\\n        embeddings = []\\n        for start_token, end_token in token_span_annotations:\\n            if end_token > start_token:  # Ensure span has at least one token\\n                chunk_embedding = token_embeddings[0, start_token:end_token].mean(dim=0)\\n                embeddings.append(chunk_embedding.detach().cpu().numpy())\\n        \\n        return chunks, embeddings\\n[/code]\\n\\n[code]\\n\\n    late_chunks, late_embeddings = late_chunking(document, jina_model, jina_tokenizer)\\n[/code]\\n\\n[code]\\n\\n    late_chunks\\n[/code]\\n\\n[\\'Berlin is the capital and largest city of Germany.\\', \\'The city has a rich history dating back centuries.\\', \\'It was founded in the 13th century and has been a significant cultural and political center throughout European history.\\', \\'The metropolis experienced dramatic changes during the 20th century, including two world wars and a period of division.\\', \\'After reunification, it underwent extensive reconstruction and modernization efforts.\\', \\'Its population reached 3.85 million inhabitants in 2023, making it the most populous urban area in the country.\\', \\'This represents a significant increase from previous decades, driven largely by immigration and economic opportunities.\\', \\'The city is known for its vibrant cultural scene and historical significance.\\', \\'Many tourists visit its famous landmarks each year, contributing significantly to the local economy.\\', \\'The Brandenburg Gate stands as its most iconic symbol.\\']\\n\\n### Evaluations!\\n\\nWe can re-use what we did earlier along with the sample queries we were looking at to run an evaluation.\\n\\nRemember this eval is to illustrate the kinds of questions late chunking can help solve. A lot more work needs to go into evaluating 2 different approaches like this, so read the paper and blog posts by Jina AI if you are interested in that. For this post, it is purely illustrative to build intuition over the kinds of queries late chunking can help with.\\n\\nSo let\\'s get started. Let\\'s refresh our memory on the document we are querying.\\n\\n[code]\\n\\n    print(document)\\n[/code]\\n\\n[code]\\n\\n    Berlin is the capital and largest city of Germany. The city has a rich history dating back centuries. It was founded in the 13th century and has been a significant cultural and political center throughout European history. \\n    \\n    The metropolis experienced dramatic changes during the 20th century, including two world wars and a period of division. After reunification, it underwent extensive reconstruction and modernization efforts. \\n    \\n    Its population reached 3.85 million inhabitants in 2023, making it the most populous urban area in the country. This represents a significant increase from previous decades, driven largely by immigration and economic opportunities. \\n    \\n    The city is known for its vibrant cultural scene and historical significance. Many tourists visit its famous landmarks each year, contributing significantly to the local economy. The Brandenburg Gate stands as its most iconic symbol. \\n    \\n    \\n[/code]\\n\\nFor convenience let\\'s concatenate the single chunk queries (everything needed to answer the query is contained in 1 chunk) and cross chunk queries (everything needed to answer the query is spread aross more than 1 chunk)\\n\\n[code]\\n\\n    queries_with_answers = cross_chunk_queries + single_chunk_queries\\n    cross_chunk_queries, single_chunk_queries\\n[/code]\\n\\n([(\"What is Berlin\\'s population?\", \\'3.85 million inhabitants\\'), (\\'How many people live in the German capital?\\', \\'3.85 million inhabitants\\'), (\\'What famous landmark is in Berlin?\\', \\'Brandenburg Gate\\'), (\\'What is the city known for?\\', \\'vibrant cultural scene\\'), (\\'When was it founded?\\', \\'13th century\\')], [(\\'When was Berlin founded?\\', \\'13th century\\'), (\\'What happened to Berlin during the 20th century?\\', \\'two world wars\\'), (\"What\\'s the most iconic symbol\", \\'Brandenburg Gate\\'), (\\'What happened in the 20th century?\\', \\'two world wars\\')])\\n\\nWe can re-use the all the stuff we covered earlier in the blog post to check if the answer to the question is in the chunk that is returned\\n\\n[code]\\n\\n    def evaluate_traditional_chunking(queries_with_answers, document):\\n        # Create traditional chunks and embeddings\\n        chunks, embeddings = traditional_chunking(document)\\n        \\n        results = []\\n        for query, answer_text in queries_with_answers:\\n            # Find which chunks contain the answer\\n            answer_chunks = [i for i, chunk in enumerate(chunks) if answer_text in chunk]\\n            \\n            # Get the best matching chunk using simple_retrieval\\n            best_chunk, similarity = simple_retrieval(query, chunks, embeddings)\\n            best_chunk_idx = chunks.index(best_chunk)\\n            \\n            # Check if the best chunk contains the answer\\n            contains_answer = best_chunk_idx in answer_chunks\\n            \\n            results.append({\\n                \\'query\\': query,\\n                \\'answer\\': answer_text,\\n                \\'traditional_correct\\': contains_answer,\\n            })\\n        \\n        return results\\n    \\n    # pd.DataFrame(evaluate_traditional_chunking(queries_with_answers, document))\\n[/code]\\n\\nLet\\'s do the same for the late chunking approach so we can compare the two.\\n\\n[code]\\n\\n    def evaluate_late_chunking(queries_with_answers, document):\\n        # Get chunks and embeddings using late chunking\\n        late_chunks, late_embeddings = late_chunking(document, jina_model, jina_tokenizer)\\n        \\n        results = []\\n        for query, answer_text in queries_with_answers:\\n            # Find which chunks contain the answer\\n            answer_chunks = [i for i, chunk in enumerate(late_chunks) if answer_text in chunk]\\n    \\n            # Embed the query\\n            query_embedding = jina_model.encode(query)\\n            \\n            # Find most similar chunk\\n            similarities = [np.dot(query_embedding, chunk_emb) / \\n                           (np.linalg.norm(query_embedding) * np.linalg.norm(chunk_emb)) \\n                           for chunk_emb in late_embeddings]\\n            \\n            best_chunk_idx = np.argmax(similarities)\\n            \\n            # Check if the best chunk contains the answer\\n            contains_answer = best_chunk_idx in answer_chunks\\n            \\n            results.append({\\n                \\'query\\': query,\\n                \\'answer\\': answer_text,\\n                \\'late_chunking_correct\\': contains_answer,\\n            })\\n        \\n        return results\\n    \\n    # pd.DataFrame(evaluate_late_chunking(queries_with_answers, document))\\n[/code]\\n\\nLet\\'s put both these evaluations into a single dataframe so we can compare and see if our late chunking did better at the cross chunk queries.\\n\\n[code]\\n\\n    # Helper function to merge results\\n    def create_comparison_table(queries_with_answers, document):\\n        # Get results for both methods\\n        trad_results = evaluate_traditional_chunking(queries_with_answers, document)\\n        late_results = evaluate_late_chunking(queries_with_answers, document)\\n        \\n        # Combine results\\n        combined_results = []\\n        for trad, late in zip(trad_results, late_results):\\n            combined_results.append({\\n                \\'query\\': trad[\\'query\\'],\\n                \\'answer\\': trad[\\'answer\\'],\\n                \\'traditional_correct\\': trad[\\'traditional_correct\\'],\\n                \\'late_chunking_correct\\': late[\\'late_chunking_correct\\'],\\n                \\'query_type\\': \\'cross_chunk\\' if trad[\\'query\\'] in [q for q, _ in cross_chunk_queries] else \\'single_chunk\\'\\n            })\\n        \\n        # Create DataFrame\\n        return pd.DataFrame(combined_results)\\n[/code]\\n\\n[code]\\n\\n    # Evaluate both query types\\n    comparison_df = create_comparison_table(queries_with_answers, document)\\n    comparison_df\\n[/code]\\n\\n| query | answer | traditional_correct | late_chunking_correct | query_type  \\n---|---|---|---|---|---  \\n0 | What is Berlin\\'s population? | 3.85 million inhabitants | False | True | cross_chunk  \\n1 | How many people live in the German capital? | 3.85 million inhabitants | False | True | cross_chunk  \\n2 | What famous landmark is in Berlin? | Brandenburg Gate | False | False | cross_chunk  \\n3 | What is the city known for? | vibrant cultural scene | False | True | cross_chunk  \\n4 | When was it founded? | 13th century | False | True | cross_chunk  \\n5 | When was Berlin founded? | 13th century | True | True | single_chunk  \\n6 | What happened to Berlin during the 20th century? | two world wars | True | True | single_chunk  \\n7 | What\\'s the most iconic symbol | Brandenburg Gate | True | True | single_chunk  \\n8 | What happened in the 20th century? | two world wars | True | True | single_chunk  \\n  \\nThe results show that late chunking maintains high accuracy for single-chunk queries while improving performance on cross-chunk queries. While these are example queries and not a full proper eval, you can use this to build intuition about why late chunking works.\\n\\n### Why Late Chunking Works\\n\\nLate chunking solves the lost context problem in several important ways:\\n\\n  1. **Bidirectional context awareness** : Each token embedding is influenced by all other tokens in the document, both before and after it. This means references like \"the city\" can be properly linked to \"Berlin\" mentioned earlier.\\n\\n  2. **Consistent representation** : All chunks from the same document share the same contextual foundation, ensuring that related concepts are represented similarly regardless of which chunk they appear in.\\n\\n  3. **Preservation of long-range dependencies** : Information from the beginning of a document can influence the representation of content at the end, maintaining semantic connections across the entire text.\\n\\n  4. **Resilience to boundary selection** : Since each token\\'s embedding already contains document-wide context, the specific chunking boundaries become less critical. This means simpler chunking strategies can work just as well as complex ones.\\n\\n### The Importance of Long-Context Models\\n\\nLate chunking requires embedding models that can handle long contexts—ideally 8K tokens or more. These models aren\\'t just standard embedding models with longer input windows; they\\'re specifically designed to maintain coherent representations across thousands of tokens.\\n\\nThe key advantages of these long-context models for late chunking include:\\n\\n  1. **Attention across the entire document** : They can attend to relationships between distant parts of the text\\n  2. **Training on document-level tasks** : They\\'re often fine-tuned on tasks that require understanding document structure\\n  3. **Optimized pooling strategies** : They use pooling methods that effectively compress long sequences\\n\\nWithout these capabilities, late chunking wouldn\\'t be possible or effective.\\n\\n### Stay Updated\\n\\nGet notified about new posts on AI, web development, and tech insights.\\n\\nSubscribe\\n\\n## Contents\\n\\nThe Lost Context ProblemTraditional ChunkingWhere Traditional Chunking Falls ShortThe Reference Resolution ProblemVisualizing the Lost ContextEvaluatingWhy Overlapping Chunks Isn\\'t EnoughLate ChunkingChunkingEvaluations!Why Late Chunking WorksThe Importance of Long-Context Models\\n\\n## Let\\'s Connect\\n\\n[ GitHub ](https://github.com/isaac-flath)[ LinkedIn ](https://linkedin.com/in/isaacflath)[ Twitter ](https://x.com/isaac_flath)\\n\\n© 2025 Isaac Flath • All rights reserved\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b93f28f",
   "metadata": {
    "input_tokens": 19,
    "output_tokens": 429,
    "solveit_ai": true
   },
   "source": [
    "please ground the writing style in this example $`ex1`\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Perfect! Now I can see the writing style from Isaac Flath's late chunking post. Let me revise the outline to match this grounded, practical approach:\n",
    "\n",
    "## Blog Post Outline: \"Systematic Error Analysis for LLM Systems: Moving Beyond Random Testing\"\n",
    "\n",
    "**1. The Random Testing Problem**\n",
    "- Why ad-hoc error analysis misses critical failure modes\n",
    "- The cost of discovering issues in production\n",
    "\n",
    "**2. Systematic Test Coverage with Dimensions**\n",
    "- What dimensions are (with concrete NL-to-SQL example)\n",
    "- Building tuples for comprehensive testing\n",
    "- Why this beats random sampling\n",
    "\n",
    "**3. The Three-Stage Discovery Process**\n",
    "- Open coding: capturing what you actually observe\n",
    "- Axial coding: finding patterns in the mess\n",
    "- Taxonomy: organizing findings for action\n",
    "\n",
    "**4. Complete Workflow: NL-to-SQL Error Analysis**\n",
    "- Step-by-step implementation with real examples\n",
    "- From 100 test queries to actionable insights\n",
    "- Quantifying and prioritizing failure modes\n",
    "\n",
    "**5. Why This Approach Works**\n",
    "- Systematic vs random: coverage comparison\n",
    "- Data-driven discovery vs assumption-based fixes\n",
    "\n",
    "The style should be:\n",
    "- Direct, practical explanations with working code\n",
    "- Concrete examples throughout (NL-to-SQL focus)\n",
    "- Step-by-step implementation you can follow\n",
    "- Clear problem → solution progression\n",
    "- Minimal fluff, maximum actionable content\n",
    "\n",
    "Should I proceed with writing the full post in this style?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf6183",
   "metadata": {
    "input_tokens": 13,
    "output_tokens": 375,
    "solveit_ai": true
   },
   "source": [
    "yes let buid by section by section\n",
    "\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Great! Let's start with the first section. I'll write it in Isaac's direct, practical style with concrete examples.\n",
    "\n",
    "**Section 1: The Random Testing Problem**\n",
    "\n",
    "Your LLM system works fine in demos but fails mysteriously in production. Users ask \"Show me sales data\" and get customer records. \"Find properties under $500K\" returns luxury listings. Sound familiar?\n",
    "\n",
    "Most teams test LLM systems the same way they'd test traditional software: throw random queries at it, fix obvious bugs, ship it. This approach misses systematic failure patterns that only emerge under specific conditions.\n",
    "\n",
    "Consider a natural language to SQL system. Random testing might catch basic syntax errors but miss that your system consistently fails when:\n",
    "- Business users ask ambiguous questions about time ranges\n",
    "- Developers request complex joins with specific personas\n",
    "- Non-technical users combine filtering with aggregation\n",
    "\n",
    "The problem isn't the individual queries—it's the **combinations** of query types, user backgrounds, and request clarity that create failure modes. Random testing can't systematically explore these combinations.\n",
    "\n",
    "Traditional QA finds the obvious bugs. Systematic error analysis finds the patterns that matter for real users.\n",
    "\n",
    "---\n",
    "\n",
    "How does this opening section feel? Should I adjust the tone or add more specific examples before moving to section 2?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4ccc8d",
   "metadata": {
    "input_tokens": 13,
    "output_tokens": 376,
    "solveit_ai": true
   },
   "source": [
    "use natural pros and write and take another stab\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Let me rewrite Section 1 with more natural prose:\n",
    "\n",
    "**Section 1: The Random Testing Problem**\n",
    "\n",
    "Your natural language to SQL system works perfectly in the demo. Then you deploy it and users start complaining. \"Show me sales from last quarter\" returns nothing. \"How many customers bought products?\" gives them a syntax error. \"Find high-value transactions\" pulls up everything except what they actually wanted.\n",
    "\n",
    "What went wrong? You tested it thoroughly, right?\n",
    "\n",
    "Here's the issue: most teams test LLM systems like traditional software. They write a handful of test cases, throw some random queries at the system, fix the obvious bugs, and call it done. But LLM failures aren't random—they follow patterns.\n",
    "\n",
    "Your system might work fine for precise technical queries but consistently fail when business analysts ask vague questions about time periods. It might handle simple filters perfectly but break down when non-technical users try to combine aggregation with complex conditions. These aren't isolated bugs—they're systematic failure modes that emerge from specific combinations of user types, query complexity, and request clarity.\n",
    "\n",
    "Random testing finds the obvious problems. It doesn't find the patterns that actually matter for your users.\n",
    "\n",
    "---\n",
    "\n",
    "Better? Should I continue with section 2, or would you like me to adjust this approach?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc83cafe",
   "metadata": {
    "input_tokens": 18,
    "output_tokens": 246,
    "solveit_ai": true,
    "time_run": "5:06:31a"
   },
   "source": [
    "yeah this is better can but write a bit concise version..\n",
    "\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "**Section 1: The Random Testing Problem**\n",
    "\n",
    "Your natural language to SQL system works perfectly in demos. Then users start complaining. \"Show me sales from last quarter\" returns nothing. \"Find high-value transactions\" pulls up everything except what they wanted.\n",
    "\n",
    "Most teams test LLM systems like traditional software: write test cases, throw random queries at it, fix obvious bugs, ship it. But LLM failures aren't random—they follow patterns.\n",
    "\n",
    "Your system might handle precise technical queries but consistently fail when business analysts ask vague questions about time periods. It might process simple filters perfectly but break when non-technical users combine aggregation with complex conditions.\n",
    "\n",
    "Random testing finds obvious problems. It doesn't find the systematic failure modes that actually hurt your users.\n",
    "\n",
    "---\n",
    "\n",
    "Better length? Should I move on to section 2 about dimensions and systematic testing?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b5d11e",
   "metadata": {
    "input_tokens": 12,
    "output_tokens": 340,
    "solveit_ai": true
   },
   "source": [
    "yes, let moved to section 2\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "**Section 2: Systematic Test Coverage with Dimensions**\n",
    "\n",
    "Instead of random testing, you need systematic coverage. This means identifying the key ways your system can vary and testing combinations deliberately.\n",
    "\n",
    "**Dimensions** are the different axes along which your system behavior changes. For a natural language to SQL system, three critical dimensions are:\n",
    "\n",
    "- **Query Type**: filter, aggregation, join, complex\n",
    "- **User Persona**: business analyst, developer, non-technical user  \n",
    "- **Request Clarity**: precise, ambiguous, incomplete\n",
    "\n",
    "Each dimension captures one way things can go wrong. Query type affects SQL complexity. User persona influences how requests are phrased. Request clarity determines how much the system must infer.\n",
    "\n",
    "**Tuples** are specific combinations: (Filter, Business Analyst, Ambiguous) or (Join, Developer, Precise). Instead of random queries, you systematically generate examples for each meaningful combination.\n",
    "\n",
    "This gives you comprehensive coverage of failure modes rather than hoping random examples catch problems.\n",
    "\n",
    "---\n",
    "\n",
    "Should I continue with how to actually implement this systematic approach, or would you like me to elaborate on any part of dimensions and tuples first?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e220aa8",
   "metadata": {
    "input_tokens": 13,
    "output_tokens": 298,
    "solveit_ai": true
   },
   "source": [
    "yeah go ahead but keep but not fluff.\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "**Section 3: The Three-Stage Discovery Process**\n",
    "\n",
    "Once you have systematic test data, you need a way to find patterns in the failures. Qualitative research gives us a proven method: open coding, axial coding, and taxonomy creation.\n",
    "\n",
    "**Open Coding**: Run your test queries and label everything you notice. Stay descriptive, not interpretive. \"Missing WHERE clause for date range\" not \"poor date handling.\" Capture all observations without organizing them yet.\n",
    "\n",
    "**Axial Coding**: Group related observations. \"Missing WHERE clause,\" \"ignored time constraint,\" and \"default date range used\" all cluster into a \"Missing Filters\" pattern.\n",
    "\n",
    "**Taxonomy Creation**: Organize patterns into a hierarchy. Missing Filters becomes a subcategory under Query Logic, alongside Wrong Aggregation and Incorrect Joins.\n",
    "\n",
    "This bottom-up approach discovers actual failure patterns from your data rather than imposing assumptions about what should go wrong.\n",
    "\n",
    "---\n",
    "\n",
    "Ready for section 4 with the complete NL-to-SQL workflow example?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4b9ce9",
   "metadata": {
    "input_tokens": 3,
    "output_tokens": 408,
    "solveit_ai": true
   },
   "source": [
    "yes\n",
    "\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "**Section 4: Complete Workflow: NL-to-SQL Error Analysis**\n",
    "\n",
    "**Step 1: Define Dimensions and Generate Test Data**\n",
    "\n",
    "For our NL-to-SQL system, we use three dimensions:\n",
    "- Query Type: filter, aggregation, join, complex\n",
    "- User Persona: business analyst, developer, non-technical user\n",
    "- Request Clarity: precise, ambiguous, incomplete\n",
    "\n",
    "Generate tuples like (Filter, Business Analyst, Ambiguous) → \"Show me sales from last quarter\" or (Join, Developer, Precise) → \"Get customer names with order totals from January 2024.\"\n",
    "\n",
    "Create 100 realistic queries covering all meaningful combinations.\n",
    "\n",
    "**Step 2: Run Tests and Open Code**\n",
    "\n",
    "Execute each query through your system. For every failure, write descriptive labels:\n",
    "- \"Missing date constraint in WHERE clause\"\n",
    "- \"Used COUNT instead of SUM for totals\"\n",
    "- \"Joined wrong table for customer data\"\n",
    "- \"Generated syntactically correct but logically wrong SQL\"\n",
    "\n",
    "**Step 3: Find Patterns with Axial Coding**\n",
    "\n",
    "Group related observations:\n",
    "- Missing Filters: date constraints, status conditions ignored\n",
    "- Wrong Aggregation: COUNT/SUM/AVG confusion\n",
    "- Schema Errors: invalid columns, wrong tables\n",
    "- Logic Errors: correct syntax, wrong business meaning\n",
    "\n",
    "Would you like me to continue with the taxonomy creation and quantification steps?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01b7feb",
   "metadata": {
    "input_tokens": 1,
    "output_tokens": 324,
    "solveit_ai": true
   },
   "source": [
    "yes\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "**Step 4: Create Taxonomy and Quantify**\n",
    "\n",
    "Build a hierarchical structure from your patterns:\n",
    "\n",
    "```\n",
    "SQL Generation Failures\n",
    "├── Schema Understanding\n",
    "│   ├── Invalid Columns (18% of failures)\n",
    "│   └── Wrong Tables (12% of failures)\n",
    "├── Query Logic\n",
    "│   ├── Missing Filters (35% of failures)\n",
    "│   ├── Wrong Aggregation (22% of failures)\n",
    "│   └── Incorrect Joins (8% of failures)\n",
    "└── Syntax Issues\n",
    "    ├── Malformed SQL (3% of failures)\n",
    "    └── Type Mismatches (2% of failures)\n",
    "```\n",
    "\n",
    "**Step 5: Prioritize and Fix**\n",
    "\n",
    "Focus on highest-impact categories first. Missing Filters at 35% gets immediate attention. Schema errors at 30% combined need systematic fixes like better column name matching.\n",
    "\n",
    "This data-driven approach tells you exactly where to invest engineering effort rather than guessing at improvements.\n",
    "\n",
    "---\n",
    "\n",
    "Ready for the final section on why this systematic approach beats traditional QA methods?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d46c3d",
   "metadata": {
    "input_tokens": 1,
    "output_tokens": 334,
    "solveit_ai": true
   },
   "source": [
    "yes\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "**Section 5: Why This Systematic Approach Works**\n",
    "\n",
    "Traditional QA catches obvious bugs but misses systematic patterns. Random testing might find that your system fails on \"complex joins\" but won't tell you it specifically fails when non-technical users request complex joins with ambiguous time constraints.\n",
    "\n",
    "The systematic approach gives you:\n",
    "\n",
    "**Comprehensive Coverage**: Instead of hoping random examples catch edge cases, you systematically test all meaningful combinations of variables that affect system behavior.\n",
    "\n",
    "**Data-Driven Insights**: Patterns emerge from actual failures rather than assumptions about what might go wrong. You discover that 35% of failures are missing filters, not the SQL syntax errors you expected.\n",
    "\n",
    "**Actionable Priorities**: Quantified failure modes tell you exactly where to focus engineering effort for maximum impact.\n",
    "\n",
    "**Reproducible Process**: The methodology works across different LLM systems and domains, not just NL-to-SQL.\n",
    "\n",
    "This transforms error analysis from reactive bug hunting into proactive system improvement based on real usage patterns.\n",
    "\n",
    "---\n",
    "\n",
    "That completes the blog post! Should I put together the full post now, or would you like me to adjust any sections?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d0423b",
   "metadata": {
    "input_tokens": 16,
    "output_tokens": 550,
    "solveit_ai": true
   },
   "source": [
    "now let combine all the sections and convert into blog..\n",
    "\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "# Systematic Error Analysis for LLM Systems: Moving Beyond Random Testing\n",
    "\n",
    "Your natural language to SQL system works perfectly in demos. Then users start complaining. \"Show me sales from last quarter\" returns nothing. \"Find high-value transactions\" pulls up everything except what they wanted.\n",
    "\n",
    "Most teams test LLM systems like traditional software: write test cases, throw random queries at it, fix obvious bugs, ship it. But LLM failures aren't random—they follow patterns.\n",
    "\n",
    "Your system might handle precise technical queries but consistently fail when business analysts ask vague questions about time periods. It might process simple filters perfectly but break when non-technical users combine aggregation with complex conditions.\n",
    "\n",
    "Random testing finds obvious problems. It doesn't find the systematic failure modes that actually hurt your users.\n",
    "\n",
    "## Systematic Test Coverage with Dimensions\n",
    "\n",
    "Instead of random testing, you need systematic coverage. This means identifying the key ways your system can vary and testing combinations deliberately.\n",
    "\n",
    "**Dimensions** are the different axes along which your system behavior changes. For a natural language to SQL system, three critical dimensions are:\n",
    "\n",
    "- **Query Type**: filter, aggregation, join, complex\n",
    "- **User Persona**: business analyst, developer, non-technical user  \n",
    "- **Request Clarity**: precise, ambiguous, incomplete\n",
    "\n",
    "Each dimension captures one way things can go wrong. Query type affects SQL complexity. User persona influences how requests are phrased. Request clarity determines how much the system must infer.\n",
    "\n",
    "**Tuples** are specific combinations: (Filter, Business Analyst, Ambiguous) or (Join, Developer, Precise). Instead of random queries, you systematically generate examples for each meaningful combination.\n",
    "\n",
    "This gives you comprehensive coverage of failure modes rather than hoping random examples catch problems.\n",
    "\n",
    "Would you like me to continue with the rest of the sections, or should I pause here for your feedback?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736e71ec",
   "metadata": {
    "input_tokens": 3,
    "output_tokens": 484,
    "solveit_ai": true,
    "time_run": "5:13:00a"
   },
   "source": [
    "yes \n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "## The Three-Stage Discovery Process\n",
    "\n",
    "Once you have systematic test data, you need a way to find patterns in the failures. Qualitative research gives us a proven method: open coding, axial coding, and taxonomy creation.\n",
    "\n",
    "**Open Coding**: Run your test queries and label everything you notice. Stay descriptive, not interpretive. \"Missing WHERE clause for date range\" not \"poor date handling.\" Capture all observations without organizing them yet.\n",
    "\n",
    "**Axial Coding**: Group related observations. \"Missing WHERE clause,\" \"ignored time constraint,\" and \"default date range used\" all cluster into a \"Missing Filters\" pattern.\n",
    "\n",
    "**Taxonomy Creation**: Organize patterns into a hierarchy. Missing Filters becomes a subcategory under Query Logic, alongside Wrong Aggregation and Incorrect Joins.\n",
    "\n",
    "This bottom-up approach discovers actual failure patterns from your data rather than imposing assumptions about what should go wrong.\n",
    "\n",
    "## Complete Workflow: NL-to-SQL Error Analysis\n",
    "\n",
    "**Step 1: Define Dimensions and Generate Test Data**\n",
    "\n",
    "For our NL-to-SQL system, we use three dimensions:\n",
    "- Query Type: filter, aggregation, join, complex\n",
    "- User Persona: business analyst, developer, non-technical user\n",
    "- Request Clarity: precise, ambiguous, incomplete\n",
    "\n",
    "Generate tuples like (Filter, Business Analyst, Ambiguous) → \"Show me sales from last quarter\" or (Join, Developer, Precise) → \"Get customer names with order totals from January 2024.\"\n",
    "\n",
    "Create 100 realistic queries covering all meaningful combinations.\n",
    "\n",
    "Should I continue with the remaining steps, or would you like me to pause here?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38204440",
   "metadata": {
    "input_tokens": 1,
    "output_tokens": 412,
    "solveit_ai": true
   },
   "source": [
    "yes\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "**Step 2: Run Tests and Open Code**\n",
    "\n",
    "Execute each query through your system. For every failure, write descriptive labels:\n",
    "- \"Missing date constraint in WHERE clause\"\n",
    "- \"Used COUNT instead of SUM for totals\"\n",
    "- \"Joined wrong table for customer data\"\n",
    "- \"Generated syntactically correct but logically wrong SQL\"\n",
    "\n",
    "**Step 3: Find Patterns with Axial Coding**\n",
    "\n",
    "Group related observations:\n",
    "- Missing Filters: date constraints, status conditions ignored\n",
    "- Wrong Aggregation: COUNT/SUM/AVG confusion\n",
    "- Schema Errors: invalid columns, wrong tables\n",
    "- Logic Errors: correct syntax, wrong business meaning\n",
    "\n",
    "**Step 4: Create Taxonomy and Quantify**\n",
    "\n",
    "Build a hierarchical structure from your patterns:\n",
    "\n",
    "```\n",
    "SQL Generation Failures\n",
    "├── Schema Understanding\n",
    "│   ├── Invalid Columns (18% of failures)\n",
    "│   └── Wrong Tables (12% of failures)\n",
    "├── Query Logic\n",
    "│   ├── Missing Filters (35% of failures)\n",
    "│   ├── Wrong Aggregation (22% of failures)\n",
    "│   └── Incorrect Joins (8% of failures)\n",
    "└── Syntax Issues\n",
    "    ├── Malformed SQL (3% of failures)\n",
    "    └── Type Mismatches (2% of failures)\n",
    "```\n",
    "\n",
    "Should I continue with the final steps and conclusion?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50238d7",
   "metadata": {
    "input_tokens": 1,
    "output_tokens": 417,
    "solveit_ai": true,
    "time_run": "5:13:49a"
   },
   "source": [
    "yes\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "**Step 5: Prioritize and Fix**\n",
    "\n",
    "Focus on highest-impact categories first. Missing Filters at 35% gets immediate attention. Schema errors at 30% combined need systematic fixes like better column name matching.\n",
    "\n",
    "This data-driven approach tells you exactly where to invest engineering effort rather than guessing at improvements.\n",
    "\n",
    "## Why This Systematic Approach Works\n",
    "\n",
    "Traditional QA catches obvious bugs but misses systematic patterns. Random testing might find that your system fails on \"complex joins\" but won't tell you it specifically fails when non-technical users request complex joins with ambiguous time constraints.\n",
    "\n",
    "The systematic approach gives you:\n",
    "\n",
    "**Comprehensive Coverage**: Instead of hoping random examples catch edge cases, you systematically test all meaningful combinations of variables that affect system behavior.\n",
    "\n",
    "**Data-Driven Insights**: Patterns emerge from actual failures rather than assumptions about what might go wrong. You discover that 35% of failures are missing filters, not the SQL syntax errors you expected.\n",
    "\n",
    "**Actionable Priorities**: Quantified failure modes tell you exactly where to focus engineering effort for maximum impact.\n",
    "\n",
    "**Reproducible Process**: The methodology works across different LLM systems and domains, not just NL-to-SQL.\n",
    "\n",
    "This transforms error analysis from reactive bug hunting into proactive system improvement based on real usage patterns.\n",
    "\n",
    "---\n",
    "\n",
    "That's the complete blog post! Would you like me to make any adjustments to the content, structure, or tone?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
