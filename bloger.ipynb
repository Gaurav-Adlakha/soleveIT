{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba2683",
   "metadata": {
    "input_tokens": 9
   },
   "outputs": [],
   "source": [
    "import contextkit.read as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c29237",
   "metadata": {
    "input_tokens": 75
   },
   "outputs": [],
   "source": [
    "gist_url = 'https://gist.github.com/Gaurav-Adlakha/1350e4e6a5fdc0a869f341bce1d850a3'\n",
    "ctx =  rd.read_gist(gist_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45002b96",
   "metadata": {
    "input_tokens": 1,
    "output_tokens": 48052
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"cells\": [{\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"| Metric | Intuitive Explanation | When to Use | When to Avoid | Best for Which Datasets | Why It\\'s Better |\\\\n\", \"|--------|----------------------|-------------|--------------|------------------------|----------------|\\\\n\", \"| Accuracy | Fraction of predictions your model got right | Simple classification with balanced classes | Imbalanced datasets | Balanced datasets | Simple, intuitive, but misleading with imbalance |\\\\n\", \"| F1 Score | Harmonic mean of precision and recall | When false positives and false negatives are equally important | When costs of errors differ significantly | Imbalanced datasets | Balances precision and recall without being influenced by true negatives |\\\\n\", \"| F-beta | Weighted harmonic mean allowing control of precision vs recall importance | When one type of error is more costly | When you want a simple metric | Imbalanced datasets | Allows customizing the importance of precision vs recall (\\\\u03b2<1 favors precision, \\\\u03b2>1 favors recall) |\\\\n\", \"| Precision | \\\\\"When model predicts yes, how often is it correct?\\\\\" | When false positives are costly | When false negatives are costly | Imbalanced datasets where positive predictions must be trustworthy | Focuses only on quality of positive predictions |\\\\n\", \"| Recall | \\\\\"What fraction of actual positives did the model catch?\\\\\" | When false negatives are costly | When false positives are costly | Imbalanced datasets where finding all positives is critical | Focuses only on finding all positive cases |\\\\n\", \"| Top-k Accuracy | Fraction of examples where correct label is among top k predictions | Multi-class problems with similar classes | Binary classification | Large number of classes | Gives credit when model is \\\\\"close\\\\\" but not perfect |\\\\n\", \"| Average Precision (AP) | Average of precision values at each true positive | Ranking quality evaluation, rare event detection | When binary classification is needed | Imbalanced datasets with rare positive class | Evaluates full ranking quality without threshold, focuses on positive class |\\\\n\", \"| ROC AUC | Probability model ranks a random positive higher than a random negative | Comparing models, threshold-independent evaluation | Highly imbalanced datasets | Moderately imbalanced datasets | Independent of threshold, evaluates ranking performance |\\\\n\", \"| PR AUC | Area under precision-recall curve | Imbalanced datasets where positive class is of interest | Balanced datasets | Highly imbalanced datasets | Focuses on positive class performance without being affected by large number of negatives |\\\\n\", \"| Balanced Accuracy | Average of positive and negative recall | Imbalanced datasets | When classes have very different importance | Imbalanced datasets | Gives equal weight to each class regardless of frequency |\\\\n\", \"| Brier Score | Measures accuracy of probabilistic predictions | Calibration assessment, probabilistic forecasting | When only interested in final class, not probability | Probabilistic predictions requiring calibration | Evaluates quality of probability estimates, not just final prediction |\\\\n\", \"| Cohen\\'s Kappa | Agreement beyond chance between raters/classifiers | Measuring inter-rater reliability, accounting for chance | Highly imbalanced datasets | Moderate imbalance with multiple raters | Accounts for agreement by chance, good for comparing annotators |\\\\n\", \"| Hamming Loss | Fraction of incorrect label predictions in multi-label classification | Multi-label classification | Single-label classification | Multi-label datasets | Evaluates each label independently, appropriate when labels are independent |\\\\n\", \"| Jaccard Index | Ratio of common elements to all elements in two sets | Multi-label, set similarity, recommendations | When true negatives matter | Multi-label classification, set comparison | Focuses on overlap quality, ignores true negatives |\\\\n\", \"| Matthews Correlation (MCC) | Correlation between predicted and actual labels | Binary classification with imbalance | When one class matters more | Imbalanced datasets | Works well with imbalance, considers all four confusion matrix values |\"]}, {\"cell_type\": \"code\", \"metadata\": {}, \"source\": [\"from utils import *\"], \"outputs\": [], \"execution_count\": null}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"# Experiment with diffrent  imbalance_ratios\"]}, {\"cell_type\": \"code\", \"metadata\": {}, \"source\": [\"from sklearn.datasets import make_classification\\\\n\", \"from sklearn.model_selection import train_test_split\\\\n\", \"from sklearn.linear_model import LogisticRegression\\\\n\", \"from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score\\\\n\", \"\\\\n\", \"imbalance_ratios = [0.5, 0.3, 0.2, 0.1, 0.05]\\\\n\", \"results = []\\\\n\", \"\\\\n\", \"for ratio in imbalance_ratios:\\\\n\", \"    X, y = make_classification(n_samples=1000, n_classes=2, weights=[1-ratio, ratio], n_features=20, random_state=42)\\\\n\", \"    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\\\\n\", \"    \\\\n\", \"    model = LogisticRegression().fit(X_train, y_train)\\\\n\", \"    y_scores = model.predict_proba(X_test)[:, 1]\\\\n\", \"    \\\\n\", \"    results.append(dict(\\\\n\", \"        ratio=ratio, \\\\n\", \"        pos_pct=ratio*100,\\\\n\", \"        acc=accuracy_score(y_test, model.predict(X_test)),\\\\n\", \"        pr_auc=average_precision_score(y_test, y_scores),\\\\n\", \"        roc_auc=roc_auc_score(y_test, y_scores)\\\\n\", \"    ))\\\\n\", \"\\\\n\", \"metrics_df = pd.DataFrame(results)\\\\n\", \"print(metrics_df)\\\\n\", \"\\\\n\", \"plt.figure(figsize=(8, 5))\\\\n\", \"metrics_df.set_index(\\'pos_pct\\')[[\\'acc\\', \\'pr_auc\\', \\'roc_auc\\']].plot(marker=\\'o\\', grid=True)\\\\n\", \"plt.xlabel(\\'Positive Class Percentage\\')\\\\n\", \"plt.ylabel(\\'Metric Value\\')\\\\n\", \"plt.title(\\'Impact of Class Imbalance on Different Metrics\\')\\\\n\", \"plt.legend([\\'Accuracy\\', \\'PR AUC\\', \\'ROC AUC\\'])\\\\n\", \"plt.ylim(0.2, 1.0)\\\\n\", \"plt.show()\"], \"outputs\": [{\"name\": \"stdout\", \"output_type\": \"stream\", \"text\": [\"   ratio  pos_pct       acc    pr_auc   roc_auc\\\\n\", \"0   0.50     50.0  0.850000  0.927529  0.914171\\\\n\", \"1   0.30     30.0  0.843333  0.831013  0.906786\\\\n\", \"2   0.20     20.0  0.876667  0.735270  0.893264\\\\n\", \"3   0.10     10.0  0.910000  0.501725  0.872840\\\\n\", \"4   0.05      5.0  0.943333  0.303357  0.797095\\\\n\"]}, {\"data\": {\"text/plain\": [\"<Figure size 400x250 with 0 Axes>\"]}, \"metadata\": {}, \"output_type\": \"display_data\"}, {\"data\": {\"text/plain\": [\"<Figure size 320x240 with 1 Axes>\"], \"image/png\": \"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'",
       "\n========== TRUNCATED ==========\n",
       "'adata\": {}, \"source\": [\"## Using GridSearch to find optimal ThresholdClassifier\"]}, {\"cell_type\": \"code\", \"metadata\": {}, \"source\": [\"from sklearn.model_selection import GridSearchCV\\\\n\", \"from sklearn.metrics import make_scorer\\\\n\", \"from sklearn.base import BaseEstimator, ClassifierMixin\\\\n\", \"\\\\n\", \"class ThresholdClassifier(BaseEstimator, ClassifierMixin):\\\\n\", \"    \\\\\"Wrapper for threshold optimization\\\\\"\\\\n\", \"    def __init__(self, threshold=0.5, base_estimator=None): \\\\n\", \"        self.threshold, self.base_estimator = threshold, base_estimator\\\\n\", \"    \\\\n\", \"    def fit(self, X, y): \\\\n\", \"        self.base_estimator.fit(X, y)\\\\n\", \"        return self\\\\n\", \"    \\\\n\", \"    def predict(self, X): \\\\n\", \"        return (self.base_estimator.predict_proba(X)[:, 1] >= self.threshold).astype(int)\\\\n\", \"    \\\\n\", \"    def predict_proba(self, X): \\\\n\", \"        return self.base_estimator.predict_proba(X)\\\\n\", \"\\\\n\", \"# Create a custom F1 scorer\\\\n\", \"f1_scorer = make_scorer(f1_score)\\\\n\", \"\\\\n\", \"# Set up threshold optimization with GridSearchCV\\\\n\", \"base_model = LogisticRegression()\\\\n\", \"thresholds = np.linspace(0.1, 0.9, 9)\\\\n\", \"threshold_clf = ThresholdClassifier(base_estimator=base_model)\\\\n\", \"grid = GridSearchCV(threshold_clf, {\\'threshold\\': thresholds}, scoring=f1_scorer, cv=5)\\\\n\", \"\\\\n\", \"# Test it on our data\\\\n\", \"grid.fit(X_train, y_train)\\\\n\", \"print(f\\\\\"Best threshold: {grid.best_params_[\\'threshold\\']}\\\\\")\\\\n\", \"print(f\\\\\"Best F1 score: {grid.best_score_}\\\\\")\"], \"outputs\": [{\"name\": \"stdout\", \"output_type\": \"stream\", \"text\": [\"Best threshold: 0.2\\\\n\", \"Best F1 score: 0.5514816452896948\\\\n\"]}], \"execution_count\": null}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"Threshold optimization is about finding the best decision boundary for a classifier\\'s probability outputs. Let me walk you through it intuitively:\\\\n\", \"\\\\n\", \"When a model outputs probabilities (like 0.7 or 0.3), we typically use 0.5 as the default threshold - anything above is classified as positive, below as negative. But this default isn\\'t always optimal, especially with imbalanced data.\\\\n\", \"\\\\n\", \"Here\\'s a simple example:\\\\n\", \"\\\\n\", \"Imagine you\\'re predicting whether emails are spam (rare class) or not. Your model outputs these probabilities for 10 emails (where 1 = spam, 0 = not spam):\\\\n\", \"\\\\n\", \"```\\\\n\", \"True labels:     [0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\\\\n\", \"Probabilities:   [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.6, 0.7, 0.8]\\\\n\", \"```\\\\n\", \"\\\\n\", \"With the default 0.5 threshold:\\\\n\", \"- Predictions: [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\\\\n\", \"- We correctly identify all 3 spam emails (true positives)\\\\n\", \"- But we incorrectly flag 2 normal emails as spam (false positives)\\\\n\", \"\\\\n\", \"If we lower the threshold to 0.3:\\\\n\", \"- Predictions: [0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\\\\n\", \"- We still catch all spam emails\\\\n\", \"- But now we have 4 false positives\\\\n\", \"\\\\n\", \"If we raise the threshold to 0.7:\\\\n\", \"- Predictions: [0, 0, 0, 0, 0, 0, 1, 0, 1, 1]\\\\n\", \"- We miss 1 spam email (false negative)\\\\n\", \"- But we have 0 false positives\\\\n\", \"\\\\n\", \"The \\\\\"best\\\\\" threshold depends on what we\\'re optimizing for:\\\\n\", \"- If false positives are very costly, we\\'d choose a higher threshold\\\\n\", \"- If false negatives are very costly, we\\'d choose a lower threshold\\\\n\", \"- For a balance, we might optimize for F1 score (harmonic mean of precision and recall)\\\\n\", \"\\\\n\", \"The code we wrote automates this process by:\\\\n\", \"1. Trying different thresholds \\\\n\", \"2. Measuring performance (like F1 score) at each threshold\\\\n\", \"3. Selecting the threshold with the best performance\\\\n\", \"\\\\n\", \"This is especially important with imbalanced data, where the default threshold often performs poorly.\"]}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"## Using Fbeta metric\"]}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"```\\\\n\", \"def find_optimal_threshold_with_fbeta(y_true, y_scores, beta=1.0, thresholds=None):\\\\n\", \"    \\\\\"Find threshold that maximizes fbeta_score directly\\\\\"\\\\n\", \"    if thresholds is None: thresholds = np.linspace(0.01, 0.99, 50)\\\\n\", \"    best_score, best_threshold = -1, -1\\\\n\", \"    \\\\n\", \"    for threshold in thresholds:\\\\n\", \"        y_pred = (y_scores >= threshold).astype(int)\\\\n\", \"        score = fbeta_score(y_true, y_pred, beta=beta)\\\\n\", \"        \\\\n\", \"        if score > best_score:\\\\n\", \"            best_score, best_threshold = score, threshold\\\\n\", \"    \\\\n\", \"    return best_threshold, best_score\\\\n\", \"\\\\n\", \"# Compare different beta values using direct fbeta calculation\\\\n\", \"beta_values = [0.5, 1.0, 2.0]\\\\n\", \"direct_results = []\\\\n\", \"\\\\n\", \"for beta in beta_values:\\\\n\", \"    # Use more precise thresholds where scores tend to peak\\\\n\", \"    custom_thresholds = np.linspace(0.05, 0.7, 40)\\\\n\", \"    threshold, score = find_optimal_threshold_with_fbeta(y_test, y_scores, beta, custom_thresholds)\\\\n\", \"    y_pred = (y_scores >= threshold).astype(int)\\\\n\", \"    \\\\n\", \"    direct_results.append(dict(\\\\n\", \"        beta=beta,\\\\n\", \"        threshold=threshold,\\\\n\", \"        fbeta_score=score,\\\\n\", \"        precision=precision_score(y_test, y_pred),\\\\n\", \"        recall=recall_score(y_test, y_pred)\\\\n\", \"    ))\\\\n\", \"\\\\n\", \"# Display results\\\\n\", \"print(\\\\\"Results using direct fbeta calculation:\\\\\")\\\\n\", \"for r in direct_results: \\\\n\", \"    print(f\\\\\"Beta={r[\\'beta\\']}: threshold={r[\\'threshold\\']:.3f}, fbeta={r[\\'fbeta_score\\']:.3f}, precision={r[\\'precision\\']:.3f}, recall={r[\\'recall\\']:.3f}\\\\\")\\\\n\", \"\\\\n\", \"# Visualize optimal thresholds for different beta values\\\\n\", \"plt.figure(figsize=(10, 6))\\\\n\", \"thresholds = np.linspace(0.01, 0.99, 100)\\\\n\", \"\\\\n\", \"for beta in beta_values:\\\\n\", \"    scores = []\\\\n\", \"    for threshold in thresholds:\\\\n\", \"        y_pred = (y_scores >= threshold).astype(int)\\\\n\", \"        try:\\\\n\", \"            score = fbeta_score(y_test, y_pred, beta=beta)\\\\n\", \"        except:\\\\n\", \"            score = 0\\\\n\", \"        scores.append(score)\\\\n\", \"    \\\\n\", \"    plt.plot(thresholds, scores, label=f\\'F-{beta}\\')\\\\n\", \"    \\\\n\", \"    # Mark optimal points\\\\n\", \"    for r in direct_results:\\\\n\", \"        if r[\\'beta\\'] == beta:\\\\n\", \"            plt.plot(r[\\'threshold\\'], r[\\'fbeta_score\\'], \\'o\\', markersize=8)\\\\n\", \"\\\\n\", \"plt.xlabel(\\'Threshold\\')\\\\n\", \"plt.ylabel(\\'F-beta Score\\')\\\\n\", \"plt.title(\\'Optimal Thresholds for Different Beta Values\\')\\\\n\", \"plt.legend()\\\\n\", \"plt.grid(True)\\\\n\", \"plt.show()\\\\n\", \"```\"]}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"The F-beta score is a weighted harmonic mean of precision and recall, where beta controls the balance between these two metrics. Let me explain this intuitively:\\\\n\", \"\\\\n\", \"**What beta values mean:**\\\\n\", \"\\\\n\", \"- **Beta = 1.0**: Equal weight to precision and recall (this is the standard F1 score)\\\\n\", \"- **Beta < 1.0**: More weight to precision than recall\\\\n\", \"- **Beta > 1.0**: More weight to recall than precision\\\\n\", \"\\\\n\", \"**Why different beta values affect the optimal threshold:**\\\\n\", \"\\\\n\", \"When we optimize for different beta values, we\\'re telling our algorithm what we care about more:\\\\n\", \"\\\\n\", \"- **With beta = 0.5** (precision-focused):\\\\n\", \"  - We\\'re saying \\\\\"false positives are more costly than false negatives\\\\\"\\\\n\", \"  - The algorithm responds by being more conservative, only predicting positive when it\\'s very confident\\\\n\", \"  - This leads to higher thresholds (0.483 in our example)\\\\n\", \"  - Result: Higher precision (0.706) but lower recall (0.308)\\\\n\", \"  - Real-world example: Spam filtering where falsely marking legitimate emails as spam is worse than missing some spam\\\\n\", \"\\\\n\", \"- **With beta = 2.0** (recall-focused):\\\\n\", \"  - We\\'re saying \\\\\"false negatives are more costly than false positives\\\\\"\\\\n\", \"  - The algorithm becomes more aggressive, predicting positive even when less confident\\\\n\", \"  - This leads to lower thresholds (0.183 in our example)\\\\n\", \"  - Result: Higher recall (0.705) but lower precision (0.433)\\\\n\", \"  - Real-world example: Cancer screening where missing a cancer case is worse than a false alarm\\\\n\", \"\\\\n\", \"**The mathematical relationship:**\\\\n\", \"\\\\n\", \"The F-beta score formula is:\\\\n\", \"F-beta = (1 + \\\\u03b2\\\\u00b2) \\\\u00d7 (precision \\\\u00d7 recall) / ((\\\\u03b2\\\\u00b2 \\\\u00d7 precision) + recall)\\\\n\", \"\\\\n\", \"As beta increases:\\\\n\", \"1. The threshold for predicting positive decreases\\\\n\", \"2. More examples get classified as positive\\\\n\", \"3. We catch more true positives (higher recall)\\\\n\", \"4. But we also get more false positives (lower precision)\\\\n\", \"\\\\n\", \"This trade-off is visible in our graph, where the optimal threshold systematically shifts left (lower) as beta increases, showing how the model becomes more willing to predict the positive class.\"]}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"# top_k_accuracy metrics\"]}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"Top-k accuracy is a metric that gives your model credit if the correct answer appears anywhere among its top k predictions, rather than just the very top prediction.\\\\n\", \"\\\\n\", \"Let me explain with a simple example:\\\\n\", \"\\\\n\", \"Imagine you\\'re building an image classifier that identifies dog breeds. When shown a picture of a Golden Retriever, the model outputs these confidence scores:\\\\n\", \"\\\\n\", \"1. Labrador Retriever: 0.45\\\\n\", \"2. Golden Retriever: 0.40 (correct answer)\\\\n\", \"3. German Shepherd: 0.10\\\\n\", \"4. Beagle: 0.05\\\\n\", \"\\\\n\", \"With standard accuracy (top-1), this would count as incorrect because the highest prediction wasn\\'t right.\\\\n\", \"\\\\n\", \"But with top-2 accuracy, this counts as correct because the true label (Golden Retriever) appears within the top 2 predictions.\\\\n\", \"\\\\n\", \"This metric is especially useful when:\\\\n\", \"- Classes are very similar or easily confused\\\\n\", \"- Multiple answers might be acceptable\\\\n\", \"- You want to measure how \\\\\"close\\\\\" your model is getting\\\\n\", \"\\\\n\", \"Would you like me to explain how top-k accuracy is calculated, or would you prefer an example of when you might use it in a real application?\"]}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"Let me walk through a simple example with binary classification:\\\\n\", \"\\\\n\", \"Imagine we have a model that predicts whether an image contains a cat (class 0) or a dog (class 1). For 3 different images, our model outputs these probabilities:\\\\n\", \"\\\\n\", \"**Image 1:**\\\\n\", \"- Cat (class 0): 0.7\\\\n\", \"- Dog (class 1): 0.3\\\\n\", \"- True label: Cat (0)\\\\n\", \"\\\\n\", \"**Image 2:**\\\\n\", \"- Cat (class 0): 0.4\\\\n\", \"- Dog (class 1): 0.6\\\\n\", \"- True label: Cat (0)\\\\n\", \"\\\\n\", \"**Image 3:**\\\\n\", \"- Cat (class 0): 0.2\\\\n\", \"- Dog (class 1): 0.8\\\\n\", \"- True label: Dog (1)\\\\n\", \"\\\\n\", \"Now let\\'s calculate top-k accuracy:\\\\n\", \"\\\\n\", \"**For k=1:**\\\\n\", \"- Image 1: Highest prediction is Cat (0.7), true label is Cat \\\\u2192 Correct\\\\n\", \"- Image 2: Highest prediction is Dog (0.6), true label is Cat \\\\u2192 Wrong\\\\n\", \"- Image 3: Highest prediction is Dog (0.8), true label is Dog \\\\u2192 Correct\\\\n\", \"\\\\n\", \"So top-1 accuracy = 2/3 = 0.67 (we got 2 out of 3 correct)\\\\n\", \"\\\\n\", \"**For k=2:**\\\\n\", \"- Image 1: Top 2 predictions include Cat, true label is Cat \\\\u2192 Correct\\\\n\", \"- Image 2: Top 2 predictions include Cat, true label is Cat \\\\u2192 Correct\\\\n\", \"- Image 3: Top 2 predictions include Dog, true label is Dog \\\\u2192 Correct\\\\n\", \"\\\\n\", \"So top-2 accuracy = 3/3 = 1.0 (we got all correct)\\\\n\", \"\\\\n\", \"For binary classification with k=2, we\\'ll always get 100% because we\\'re considering both possible classes.\\\\n\", \"\\\\n\", \"Does this example help clarify the concept?\"]}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"# average precision score\"]}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"Average Precision (AP) is a metric that summarizes the precision-recall curve into a single number. Let me explain it intuitively:\\\\n\", \"\\\\n\", \"Imagine you\\'re searching for photos of dogs in a large collection. As you look through the results:\\\\n\", \"\\\\n\", \"- Precision measures: \\\\\"Of the photos I\\'ve looked at so far, what percentage are actually dogs?\\\\\"\\\\n\", \"- Recall measures: \\\\\"What percentage of all dog photos in the collection have I found so far?\\\\\"\\\\n\", \"\\\\n\", \"Average Precision essentially measures how good your ranking system is by calculating the precision at every point where you find a relevant item (a dog photo), and then averaging these precision values.\\\\n\", \"\\\\n\", \"**Intuitive explanation:**\\\\n\", \"- If all dog photos appear at the top of your search results, AP will be high (close to 1.0)\\\\n\", \"- If dog photos are mixed with non-dog photos or appear later in the results, AP will be lower\\\\n\", \"- If you miss many dog photos entirely, AP will be very low\\\\n\", \"\\\\n\", \"**When to use Average Precision:**\\\\n\", \"- When you care about the ranking of your predictions, not just whether they\\'re right/wrong\\\\n\", \"- In information retrieval tasks (search engines, recommendation systems)\\\\n\", \"- In object detection systems (finding objects in images)\\\\n\", \"- When working with imbalanced datasets where the positive class is rare\\\\n\", \"\\\\n\", \"**It\\'s particularly useful when:**\\\\n\", \"1. You have a confidence score for each prediction\\\\n\", \"2. The order of predictions matters\\\\n\", \"3. You\\'re dealing with imbalanced data\\\\n\", \"\\\\n\", \"Would you like me to explain how it compares to other metrics like ROC AUC, or would you prefer an example calculation?\"]}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"Let\\'s dive deeper into Average Precision with a concrete example in a recommendation system:\\\\n\", \"\\\\n\", \"Imagine you\\'re building a movie recommendation system. A user loves science fiction movies, and your system recommends 10 movies in a ranked order:\\\\n\", \"\\\\n\", \"1. Star Wars (sci-fi) \\\\u2713\\\\n\", \"2. The Matrix (sci-fi) \\\\u2713\\\\n\", \"3. Inception (sci-fi) \\\\u2713\\\\n\", \"4. Titanic (romance) \\\\u2717\\\\n\", \"5. Interstellar (sci-fi) \\\\u2713\\\\n\", \"6. The Notebook (romance) \\\\u2717\\\\n\", \"7. Alien (sci-fi) \\\\u2713\\\\n\", \"8. E.T. (sci-fi) \\\\u2713\\\\n\", \"9. Casablanca (drama) \\\\u2717\\\\n\", \"10. Avatar (sci-fi) \\\\u2713\\\\n\", \"\\\\n\", \"Here, \\\\u2713 marks relevant recommendations (sci-fi) and \\\\u2717 marks irrelevant ones.\\\\n\", \"\\\\n\", \"**How AP works in this context:**\\\\n\", \"- We calculate precision at each position where we find a relevant item\\\\n\", \"- Then we average these precision values\\\\n\", \"\\\\n\", \"Let\\'s calculate:\\\\n\", \"- Position 1: Precision = 1/1 = 1.0 (found 1 sci-fi out of 1 movie)\\\\n\", \"- Position 2: Precision = 2/2 = 1.0 (found 2 sci-fi out of 2 movies)\\\\n\", \"- Position 3: Precision = 3/3 = 1.0 (found 3 sci-fi out of 3 movies)\\\\n\", \"- Position 5: Precision = 4/5 = 0.8 (found 4 sci-fi out of 5 movies)\\\\n\", \"- Position 7: Precision = 5/7 = 0.71 (found 5 sci-fi out of 7 movies)\\\\n\", \"- Position 8: Precision = 6/8 = 0.75 (found 6 sci-fi out of 8 movies)\\\\n\", \"- Position 10: Precision = 7/10 = 0.7 (found 7 sci-fi out of 10 movies)\\\\n\", \"\\\\n\", \"Average Precision = (1.0 + 1.0 + 1.0 + 0.8 + 0.71 + 0.75 + 0.7) / 7 = 0.85\\\\n\", \"\\\\n\", \"**What AP scores signify:**\\\\n\", \"\\\\n\", \"- **High AP (close to 1.0)**: The system is excellent at ranking relevant items at the top\\\\n\", \"- **Medium AP (around 0.5-0.7)**: The system is placing some relevant items high, but also mixing in irrelevant items or placing relevant items too low\\\\n\", \"- **Low AP (below 0.5)**: The system is doing a poor job of prioritizing relevant items\\\\n\", \"\\\\n\", \"Does this example help clarify how AP relates to ranking? Would you like me to explain how the meaning changes in different contexts?\"]}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"Great question about imbalanced datasets! Average Precision is particularly valuable when dealing with class imbalance. Let me explain with a simple example:\\\\n\", \"\\\\n\", \"Imagine you\\'re detecting fraudulent transactions, where only 3% of transactions are fraudulent (positive class) and 97% are legitimate (negative class).\\\\n\", \"\\\\n\", \"Let\\'s say we have 100 transactions to evaluate, so there are only 3 fraudulent ones. Our model gives each transaction a probability score of being fraudulent, and we rank them from highest to lowest:\\\\n\", \"\\\\n\", \"| Rank | Transaction | True Label | Probability |\\\\n\", \"|------|-------------|------------|-------------|\\\\n\", \"| 1    | A           | Fraud \\\\u2713    | 0.92        |\\\\n\", \"| 2    | B           | Legitimate \\\\u2717 | 0.85      |\\\\n\", \"| 3    | C           | Legitimate \\\\u2717 | 0.79      |\\\\n\", \"| 4    | D           | Fraud \\\\u2713    | 0.75        |\\\\n\", \"| 5    | E           | Legitimate \\\\u2717 | 0.70      |\\\\n\", \"| ...  | ...         | ...        | ...         |\\\\n\", \"| 20   | T           | Fraud \\\\u2713    | 0.40        |\\\\n\", \"| ...  | ...         | ...        | ...         |\\\\n\", \"\\\\n\", \"**Why AP works well with imbalance:**\\\\n\", \"\\\\n\", \"1. **It focuses on the minority class**: AP only calculates precision at positions where we find positive examples (fraud), so it\\'s not skewed by the abundant negative class\\\\n\", \"\\\\n\", \"2. **It rewards good ranking**: With imbalanced data, a model could achieve 97% accuracy by simply predicting \\\\\"legitimate\\\\\" for everything. AP forces the model to rank the rare positive cases higher than negatives\\\\n\", \"\\\\n\", \"3. **It\\'s threshold-independent**: You don\\'t need to pick a probability threshold, which is often tricky with imbalanced data\\\\n\", \"\\\\n\", \"**Calculating AP in this example:**\\\\n\", \"- Position 1: Precision = 1/1 = 1.0 (1 fraud out of 1 transaction)\\\\n\", \"- Position 4: Precision = 2/4 = 0.5 (2 frauds out of 4 transactions)\\\\n\", \"- Position 20: Precision = 3/20 = 0.15 (3 frauds out of 20 transactions)\\\\n\", \"\\\\n\", \"AP = (1.0 + 0.5 + 0.15) / 3 = 0.55\\\\n\", \"\\\\n\", \"**In contrast:**\\\\n\", \"- Accuracy would be high (97%) even if we missed all fraud cases\\\\n\", \"- F1-score would require setting a threshold\\\\n\", \"- ROC AUC might be overly optimistic due to the many true negatives\\\\n\", \"\\\\n\", \"Does this help explain why AP is valuable for imbalanced datasets? Would you like me to compare it with another metric like ROC AUC in this context?\"]}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"# Balanced accuracy\"]}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"Balanced accuracy is the average of sensitivity (recall of the positive class) and specificity (recall of the negative class). In simple terms, it\\'s the average of \\\\\"how well you detect positive cases\\\\\" and \\\\\"how well you detect negative cases.\\\\\"\\\\n\", \"\\\\n\", \"**Intuitive explanation:**\\\\n\", \"Imagine you\\'re diagnosing a rare disease. Standard accuracy might look good even if you miss every sick patient (by just saying \\\\\"healthy\\\\\" for everyone). Balanced accuracy prevents this by equally weighting your ability to identify both sick and healthy patients.\\\\n\", \"\\\\n\", \"**When balanced accuracy makes sense:**\\\\n\", \"- When your dataset is imbalanced (one class is much more common)\\\\n\", \"- When both classes are equally important to detect correctly\\\\n\", \"- When misclassification costs are similar for both classes\\\\n\", \"- When you want a single, easy-to-understand metric\\\\n\", \"\\\\n\", \"**When it doesn\\'t make sense:**\\\\n\", \"- When one class is much more important than the other\\\\n\", \"- When misclassification costs are very different\\\\n\", \"- When you need more detailed performance insights\\\\n\", \"- When your data is already well-balanced (regular accuracy works fine)\\\\n\", \"\\\\n\", \"**Behavior with different datasets:**\\\\n\", \"- **Unbalanced datasets**: Balanced accuracy remains stable and meaningful, while regular accuracy can be misleadingly high\\\\n\", \"- **Balanced datasets**: Balanced accuracy equals regular accuracy\\\\n\", \"\\\\n\", \"**Example scenario:**\\\\n\", \"In fraud detection where frauds are rare (1%), a model that predicts \\\\\"no fraud\\\\\" for everything would get 99% accuracy but 50% balanced accuracy (100% on non-fraud, 0% on fraud). This reveals the model\\'s true weakness.\\\\n\", \"\\\\n\", \"Would you like me to walk through a numerical example to show how balanced accuracy is calculated?\"]}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"# BrierScore\"]}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"The Brier Score measures how accurate your probability predictions are. Think of it as the \\\\\"weather forecaster score\\\\\" - it tells you how good you are at predicting probabilities.\\\\n\", \"\\\\n\", \"**Intuitive explanation:**\\\\n\", \"Imagine you\\'re predicting rain for 5 days. Each day, you assign a probability of rain:\\\\n\", \"\\\\n\", \"| Day | Your Prediction | Actual Outcome |\\\\n\", \"|-----|----------------|---------------|\\\\n\", \"| 1   | 90% chance     | It rained \\\\u2713    |\\\\n\", \"| 2   | 80% chance     | No rain \\\\u2717      |\\\\n\", \"| 3   | 20% chance     | No rain \\\\u2713      |\\\\n\", \"| 4   | 30% chance     | It rained \\\\u2717    |\\\\n\", \"| 5   | 60% chance     | It rained \\\\u2713    |\\\\n\", \"\\\\n\", \"The Brier Score calculates the \\\\\"squared error\\\\\" between your predictions and what actually happened:\\\\n\", \"\\\\n\", \"Brier Score = (1/5) \\\\u00d7 [(0.9-1)\\\\u00b2 + (0.8-0)\\\\u00b2 + (0.2-0)\\\\u00b2 + (0.3-1)\\\\u00b2 + (0.6-1)\\\\u00b2]\\\\n\", \"            = (1/5) \\\\u00d7 [0.01 + 0.64 + 0.04 + 0.49 + 0.16]\\\\n\", \"            = 0.268\\\\n\", \"\\\\n\", \"**Lower is better:** A perfect score is 0, worst is 1.\\\\n\", \"\\\\n\", \"**When to use Brier Score:**\\\\n\", \"- When you care about the quality of probability estimates, not just binary predictions\\\\n\", \"- For calibrating models (ensuring 70% confidence actually means 70% likelihood)\\\\n\", \"- In risk assessment where probability accuracy matters\\\\n\", \"- For comparing probabilistic forecasts (weather, sports, finance)\\\\n\", \"\\\\n\", \"**When to avoid it:**\\\\n\", \"- When you only care about final classifications, not probabilities\\\\n\", \"- For multiclass problems with many classes (use proper scoring rules instead)\\\\n\", \"- When different types of errors have very different costs\\\\n\", \"- When your primary concern is ranking predictions rather than absolute probabilities\\\\n\", \"\\\\n\", \"Would you like me to compare it to other metrics or explain how to interpret different Brier Score values?\"]}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"# CohenKappa\"]}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"Cohen\\'s Kappa measures agreement between two raters (or a model and ground truth) while accounting for agreement that might occur by chance. It\\'s like asking: \\\\\"How much better is our agreement than if we were just guessing?\\\\\"\\\\n\", \"\\\\n\", \"**Intuitive explanation:**\\\\n\", \"\\\\n\", \"Imagine two doctors diagnosing 100 patients as either \\\\\"sick\\\\\" or \\\\\"healthy\\\\\":\\\\n\", \"\\\\n\", \"| | Doctor B: \\\\\"Sick\\\\\" | Doctor B: \\\\\"Healthy\\\\\" |\\\\n\", \"|-|-----------------|---------------------|\\\\n\", \"| Doctor A: \\\\\"Sick\\\\\" | 25 | 15 |\\\\n\", \"| Doctor A: \\\\\"Healthy\\\\\" | 10 | 50 |\\\\n\", \"\\\\n\", \"- They agree on 75 patients (25 sick + 50 healthy)\\\\n\", \"- Simple agreement = 75/100 = 75%\\\\n\", \"\\\\n\", \"But what if they were just guessing? With 40% of diagnoses being \\\\\"sick\\\\\", we\\'d expect them to randomly agree on:\\\\n\", \"- Both say \\\\\"sick\\\\\": 0.4 \\\\u00d7 0.35 = 14% of cases\\\\n\", \"- Both say \\\\\"healthy\\\\\": 0.6 \\\\u00d7 0.65 = 39% of cases\\\\n\", \"- Total expected random agreement = 53%\\\\n\", \"\\\\n\", \"Cohen\\'s Kappa adjusts for this:\\\\n\", \"Kappa = (observed agreement - expected agreement) / (1 - expected agreement)\\\\n\", \"      = (0.75 - 0.53) / (1 - 0.53)\\\\n\", \"      = 0.47\\\\n\", \"\\\\n\", \"This means they agree 47% better than random chance.\\\\n\", \"\\\\n\", \"**When to use Cohen\\'s Kappa:**\\\\n\", \"- Evaluating inter-rater reliability (e.g., multiple doctors\\' diagnoses)\\\\n\", \"- When class distributions are imbalanced\\\\n\", \"- Comparing model performance against human experts\\\\n\", \"- In fields like medicine, psychology, and content analysis\\\\n\", \"\\\\n\", \"**When to avoid it:**\\\\n\", \"- With highly imbalanced classes (can produce counterintuitive results)\\\\n\", \"- When more than two raters are involved (use Fleiss\\' Kappa instead)\\\\n\", \"- When the cost of different disagreements varies (it treats all disagreements equally)\\\\n\", \"\\\\n\", \"**Specific scenarios for using Kappa:**\\\\n\", \"- Medical diagnosis agreement between clinicians\\\\n\", \"- Document classification by multiple annotators\\\\n\", \"- Quality control for manual data labeling\\\\n\", \"- Psychiatric assessment reliability\\\\n\", \"\\\\n\", \"Would you like me to show how to calculate and interpret Kappa with code, or explain how to interpret different Kappa values?\"]}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"# Hammingloss metric\"]}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"Hamming Loss measures the fraction of labels that are incorrectly predicted in multi-label classification. Think of it as the \\\\\"error rate per label.\\\\\"\\\\n\", \"\\\\n\", \"**Intuitive explanation:**\\\\n\", \"Imagine each example can have multiple tags attached to it. Hamming Loss counts how often you get each tag right or wrong, then averages these errors.\\\\n\", \"\\\\n\", \"**Simple example:**\\\\n\", \"Let\\'s say we\\'re classifying movies by genre, where each movie can belong to multiple genres:\\\\n\", \"\\\\n\", \"Movie: \\\\\"The Avengers\\\\\"\\\\n\", \"- True genres: [Action, Adventure, Sci-Fi]\\\\n\", \"- Predicted genres: [Action, Comedy, Sci-Fi]\\\\n\", \"\\\\n\", \"For this example:\\\\n\", \"- We correctly predicted: Action, Sci-Fi\\\\n\", \"- We missed: Adventure\\\\n\", \"- We incorrectly added: Comedy\\\\n\", \"\\\\n\", \"So we made 2 errors out of 5 possible genres:\\\\n\", \"Hamming Loss = 2/5 = 0.4 (lower is better)\\\\n\", \"\\\\n\", \"**When to use Hamming Loss:**\\\\n\", \"- When examples can have multiple labels simultaneously\\\\n\", \"- When all label mistakes are equally important\\\\n\", \"- When you want to evaluate each label prediction independently\\\\n\", \"- In text categorization, image tagging, or gene function prediction\\\\n\", \"\\\\n\", \"**When to avoid it:**\\\\n\", \"- For single-label classification problems\\\\n\", \"- When some labels are more important than others\\\\n\", \"- When label combinations matter (Hamming Loss doesn\\'t capture label correlations)\\\\n\", \"- When you care more about completely correct predictions (use Subset Accuracy instead)\\\\n\", \"\\\\n\", \"**Real-world scenario:**\\\\n\", \"Document tagging systems where each document can have multiple tags and missing or incorrectly adding any tag is equally problematic.\\\\n\", \"\\\\n\", \"Would you like me to show how to calculate Hamming Loss with code, or explain how it compares to other multi-label metrics?\\\\n\", \"\\\\n\", \"\\\\n\", \"I apologize for the confusion. Let me clarify with a more detailed example:\\\\n\", \"\\\\n\", \"In multi-label classification, we need to make a yes/no decision for each possible label. Let\\'s use movie genres with a clearer set of all possible genres.\\\\n\", \"\\\\n\", \"**All possible genres in our system:** [Action, Adventure, Comedy, Drama, Sci-Fi]\\\\n\", \"\\\\n\", \"Now, for the movie \\\\\"The Avengers\\\\\":\\\\n\", \"\\\\n\", \"- **True genres:** [Action, Adventure, Sci-Fi]\\\\n\", \"- **Predicted genres:** [Action, Comedy, Sci-Fi]\\\\n\", \"\\\\n\", \"For each of the 5 possible genres, let\\'s check if our prediction was correct:\\\\n\", \"\\\\n\", \"1. **Action**: True label: Yes, Predicted: Yes \\\\u2192 Correct \\\\u2713\\\\n\", \"2. **Adventure**: True label: Yes, Predicted: No \\\\u2192 Error \\\\u2717\\\\n\", \"3. **Comedy**: True label: No, Predicted: Yes \\\\u2192 Error \\\\u2717\\\\n\", \"4. **Drama**: True label: No, Predicted: No \\\\u2192 Correct \\\\u2713\\\\n\", \"5. **Sci-Fi**: True label: Yes, Predicted: Yes \\\\u2192 Correct \\\\u2713\\\\n\", \"\\\\n\", \"We made 2 errors out of 5 label decisions, so:\\\\n\", \"Hamming Loss = 2/5 = 0.4\\\\n\", \"\\\\n\", \"Let\\'s look at another example:\\\\n\", \"\\\\n\", \"Movie: \\\\\"The Shawshank Redemption\\\\\"\\\\n\", \"- **True genres:** [Drama]\\\\n\", \"- **Predicted genres:** [Drama, Crime]\\\\n\", \"\\\\n\", \"For each possible genre:\\\\n\", \"1. **Action**: True: No, Predicted: No \\\\u2192 Correct \\\\u2713\\\\n\", \"2. **Adventure**: True: No, Predicted: No \\\\u2192 Correct \\\\u2713\\\\n\", \"3. **Comedy**: True: No, Predicted: No \\\\u2192 Correct \\\\u2713\\\\n\", \"4. **Crime**: True: No, Predicted: Yes \\\\u2192 Error \\\\u2717\\\\n\", \"5. **Drama**: True: Yes, Predicted: Yes \\\\u2192 Correct \\\\u2713\\\\n\", \"\\\\n\", \"We made 1 error out of 5 label decisions:\\\\n\", \"Hamming Loss = 1/5 = 0.2\\\\n\", \"\\\\n\", \"Does this explanation make more sense now?\"]}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"# Jaccard metric\"]}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"The Jaccard Index (or Jaccard Similarity) measures the similarity between two sets by comparing what they have in common versus everything present in either set. It\\'s essentially:\\\\n\", \"\\\\n\", \"Jaccard = (Size of Intersection) / (Size of Union)\\\\n\", \"\\\\n\", \"**Intuitive explanation:**\\\\n\", \"Imagine you and a friend each have a collection of books. The Jaccard Index tells you: \\\\\"Out of all the unique books that either of you own, what fraction do you both have?\\\\\"\\\\n\", \"\\\\n\", \"**Simple example:**\\\\n\", \"- Your books: {Harry Potter, Lord of the Rings, Dune, Foundation}\\\\n\", \"- Friend\\'s books: {Harry Potter, Dune, Game of Thrones, Neuromancer}\\\\n\", \"\\\\n\", \"Intersection (books you both have): {Harry Potter, Dune} = 2 books\\\\n\", \"Union (all unique books between you): {Harry Potter, Lord of the Rings, Dune, Foundation, Game of Thrones, Neuromancer} = 6 books\\\\n\", \"\\\\n\", \"Jaccard Index = 2/6 = 0.33 (or 33% similarity)\\\\n\", \"\\\\n\", \"**When to use Jaccard:**\\\\n\", \"- Comparing sets of items (document topics, product recommendations, species in ecosystems)\\\\n\", \"- Multi-label classification evaluation\\\\n\", \"- Measuring similarity between customer segments\\\\n\", \"- Image segmentation evaluation\\\\n\", \"- When you care equally about false positives and false negatives\\\\n\", \"\\\\n\", \"**When NOT to use Jaccard:**\\\\n\", \"- When the total size of the universe matters (Jaccard ignores true negatives)\\\\n\", \"- When false positives and false negatives have different costs\\\\n\", \"- For single-label classification (better metrics exist)\\\\n\", \"- When working with continuous values rather than binary presence/absence\\\\n\", \"\\\\n\", \"**Perfect scenario:**\\\\n\", \"Recommender systems evaluation - \\\\\"How many of the recommended items were relevant, out of all items that were either recommended or relevant?\\\\\"\\\\n\", \"\\\\n\", \"Would you like me to show how Jaccard compares to other similarity metrics, or provide a code example?\"]}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"# Matthews Correlation Coefficient (MCC)\"]}, {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"Matthews Correlation Coefficient (MCC) measures the quality of binary classifications while accounting for dataset imbalance. It\\'s essentially a correlation coefficient between the predicted and actual classifications.\\\\n\", \"\\\\n\", \"**Intuitive explanation:**\\\\n\", \"MCC tells you how well your predictions correlate with reality, ranging from -1 (completely wrong) through 0 (random guessing) to +1 (perfect predictions). It works like a \\\\\"balanced accuracy\\\\\" that\\'s reliable even with imbalanced data.\\\\n\", \"\\\\n\", \"**Simple example:**\\\\n\", \"Imagine we\\'re predicting whether 100 emails are spam or not:\\\\n\", \"- 90 are legitimate, 10 are spam (imbalanced dataset)\\\\n\", \"- Our model correctly identifies 85 legitimate emails and 7 spam emails\\\\n\", \"- It misclassifies 5 legitimate emails as spam and 3 spam as legitimate\\\\n\", \"\\\\n\", \"Let\\'s calculate the components:\\\\n\", \"- True Positives (TP) = 7 (correctly identified spam)\\\\n\", \"- True Negatives (TN) = 85 (correctly identified legitimate)\\\\n\", \"- False Positives (FP) = 5 (legitimate wrongly marked as spam)\\\\n\", \"- False Negatives (FN) = 3 (spam wrongly marked as legitimate)\\\\n\", \"\\\\n\", \"MCC = (TP\\\\u00d7TN - FP\\\\u00d7FN) / \\\\u221a[(TP+FP)(TP+FN)(TN+FP)(TN+FN)]\\\\n\", \"MCC = (7\\\\u00d785 - 5\\\\u00d73) / \\\\u221a[(7+5)(7+3)(85+5)(85+3)]\\\\n\", \"MCC = 0.54\\\\n\", \"\\\\n\", \"**When to use MCC:**\\\\n\", \"- When your dataset is imbalanced\\\\n\", \"- When both false positives and false negatives matter\\\\n\", \"- When you need a single, reliable metric that works across different class distributions\\\\n\", \"- In bioinformatics, medical testing, or quality control\\\\n\", \"\\\\n\", \"**When NOT to use MCC:**\\\\n\", \"- When you specifically care more about one class than the other\\\\n\", \"- When you need an easily interpretable metric for non-technical stakeholders\\\\n\", \"- When working with multi-class problems (though extensions exist)\\\\n\", \"- When you need probability calibration information\\\\n\", \"\\\\n\", \"**Perfect scenario:**\\\\n\", \"Disease screening tests where the disease is rare, but both false positives (unnecessary treatments) and false negatives (missed cases) are important to minimize.\\\\n\", \"\\\\n\", \"Would you like me to compare MCC with other metrics like F1 or accuracy?\"]}], \"metadata\": {\"kernelspec\": {\"display_name\": \"python3\", \"language\": \"python\", \"name\": \"python3\"}}, \"nbformat\": 4, \"nbformat_minor\": 4}'",
       "\n========== MIDDLE OF OUTPUT TRUNCATED ==========\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f6d210",
   "metadata": {
    "input_tokens": 10
   },
   "outputs": [],
   "source": [
    "from toolslm.download import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a72c8f",
   "metadata": {
    "input_tokens": 123
   },
   "source": [
    "#BLOG\n",
    "It should be a reference style guide, but should focus on the most practical and important parts that are actually needed for work. \n",
    "\n",
    "Write, in the form of a fast.ai style post (i.e. detailed, technical, for a sophisticated audience but without assuming any prior knowledge), a blog post about the classfication metrics  $txt. This is only the first part context.  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c3eac8",
   "metadata": {
    "input_tokens": 175,
    "output_tokens": 399
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Experiment with diffrent  imbalance_ratios',\n",
       " '# Find optimal threshold',\n",
       " '## Using GridSearch to find optimal ThresholdClassifier',\n",
       " '## Using Fbeta metric',\n",
       " '# Compare different beta values using direct fbeta calculation',\n",
       " '# Use more precise thresholds where scores tend to peak',\n",
       " '# Display results',\n",
       " '# Visualize optimal thresholds for different beta values',\n",
       " '# Mark optimal points',\n",
       " '# top_k_accuracy metrics',\n",
       " '# average precision score',\n",
       " '# Balanced accuracy',\n",
       " '# BrierScore',\n",
       " '# CohenKappa',\n",
       " '# Hammingloss metric',\n",
       " '# Jaccard metric',\n",
       " '# Matthews Correlation Coefficient (MCC)']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Parse the JSON content\n",
    "data = json.loads(ctx)\n",
    "\n",
    "# Function to extract headings from markdown\n",
    "def extract_headings(cells):\n",
    "    headings = []\n",
    "    for cell in cells:\n",
    "        if cell[\"cell_type\"] == \"markdown\":\n",
    "            content = cell[\"source\"]\n",
    "            if isinstance(content, list): content = \"\".join(content)\n",
    "            for line in content.split(\"\\n\"):\n",
    "                if line.strip().startswith(\"#\"):\n",
    "                    headings.append(line.strip())\n",
    "    return headings\n",
    "\n",
    "# Extract and display the headings\n",
    "headings = extract_headings(data[\"cells\"])\n",
    "headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559493e7",
   "metadata": {
    "input_tokens": 910
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import display, Image\n",
    "\n",
    "def extract_sections(data, headings):\n",
    "    \"Extract content for specified notebook headings with code, outputs and images\"\n",
    "    results, current, buf, imgs = {}, None, [], []\n",
    "    \n",
    "    for cell in data[\"cells\"]:\n",
    "        if cell[\"cell_type\"] == \"markdown\":\n",
    "            md = \"\".join(cell[\"source\"]) if isinstance(cell[\"source\"], list) else cell[\"source\"]\n",
    "            \n",
    "            for line in md.split(\"\\n\"):\n",
    "                if line.strip().startswith(\"#\"):\n",
    "                    if current in headings and buf:\n",
    "                        results[current] = {\"text\": \"\\n\".join(buf), \"images\": imgs.copy()}\n",
    "                        buf, imgs = [], []\n",
    "                    \n",
    "                    current = line.strip()\n",
    "                    if current in headings: buf, imgs = [], []\n",
    "                    else: current = None\n",
    "                elif current in headings: buf.append(line)\n",
    "        \n",
    "        elif cell[\"cell_type\"] == \"code\" and current in headings:\n",
    "            code = \"\".join(cell[\"source\"]) if isinstance(cell[\"source\"], list) else cell[\"source\"]\n",
    "            buf.append(f\"```python\\n{code}\\n```\")\n",
    "            \n",
    "            if \"outputs\" in cell and cell[\"outputs\"]:\n",
    "                for out in cell[\"outputs\"]:\n",
    "                    if \"data\" in out:\n",
    "                        if \"text/plain\" in out[\"data\"]:\n",
    "                            txt = \"\".join(out[\"data\"][\"text/plain\"]) if isinstance(out[\"data\"][\"text/plain\"], list) else out[\"data\"][\"text/plain\"]\n",
    "                            buf.append(f\"Output:\\n```\\n{txt}\\n```\")\n",
    "                        \n",
    "                        if \"image/png\" in out[\"data\"]: \n",
    "                            imgs.append((\"png\", out[\"data\"][\"image/png\"]))\n",
    "                            buf.append(\"[IMAGE: PNG image output]\")\n",
    "                        if \"image/jpeg\" in out[\"data\"]: \n",
    "                            imgs.append((\"jpeg\", out[\"data\"][\"image/jpeg\"]))\n",
    "                            buf.append(\"[IMAGE: JPEG image output]\")\n",
    "                    \n",
    "                    if \"text\" in out:\n",
    "                        txt = \"\".join(out[\"text\"]) if isinstance(out[\"text\"], list) else out[\"text\"]\n",
    "                        buf.append(f\"Output:\\n```\\n{txt}\\n```\")\n",
    "    \n",
    "    if current in headings and buf:\n",
    "        results[current] = {\"text\": \"\\n\".join(buf), \"images\": imgs.copy()}\n",
    "    \n",
    "    return results\n",
    "\n",
    "def show_sections(sections):\n",
    "    \"Display extracted notebook sections with text and images\"\n",
    "    for heading, content in sections.items():\n",
    "        # print(f\"\\n{heading}\\n{'='*len(heading)}\")\n",
    "        # print(f\"Text content:\\n{content['text']}\\n\")\n",
    "        \n",
    "        if content['images']:\n",
    "            # print(f\"Number of images: {len(content['images'])}\")\n",
    "            for i, (fmt, img_data) in enumerate(content['images']):\n",
    "                # print(f\"Image {i+1} ({fmt}):\")\n",
    "                display(Image(data=base64.b64decode(img_data)))\n",
    "                # print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de3b2d0",
   "metadata": {
    "input_tokens": 66
   },
   "outputs": [],
   "source": [
    "def get_sections(data,target_headings):\n",
    "\n",
    "    \"Extract and display notebook sections\"\n",
    "    sections = extract_sections(data, target_headings)\n",
    "    show_sections(sections)\n",
    "    return sections\n",
    "\n",
    "\n",
    "# sections = get_sections(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9d70c7",
   "metadata": {
    "input_tokens": 57
   },
   "outputs": [],
   "source": [
    "   target_headings = [\n",
    "    '# Experiment with diffrent  imbalance_ratios',\n",
    "    '# Find optimal threshold',\n",
    "    '## Using GridSearch to find optimal ThresholdClassifier'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f558ebcd",
   "metadata": {
    "input_tokens": 16
   },
   "outputs": [],
   "source": [
    "sec_1= extract_sections(data,target_headings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526f458b",
   "metadata": {
    "input_tokens": 117,
    "output_tokens": 37,
    "solveit_ai": true
   },
   "source": [
    "Write, in the form of a fast.ai style post (i.e. detailed, technical, for a sophisticated audience but without assuming any prior knowledge), a blog post about the classfication metrics $txt. This is only the first part Sharing the first part of an experiment I've done. based the blog on that contetx and use that context... $`sec_1`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Reply<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Please post on Discord that you got this error:\n",
    "Expecting value: line 2 column 1 (char 1)"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "concise",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
