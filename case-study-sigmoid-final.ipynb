{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09bac8e",
   "metadata": {
    "input_tokens": 43,
    "time_run": "8:35:53a"
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "from fastcore.utils import *\n",
    "\n",
    "path = Path('/app/data/static/case study-2024')\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4afa387",
   "metadata": {
    "input_tokens": 18,
    "time_run": "8:35:55a"
   },
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(path.ls()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48650fa2",
   "metadata": {
    "input_tokens": 6,
    "output_tokens": 7390,
    "time_run": "8:35:55a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var1</th>\n",
       "      <th>Var2</th>\n",
       "      <th>Var3</th>\n",
       "      <th>Var4</th>\n",
       "      <th>Var5</th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var8</th>\n",
       "      <th>Var9</th>\n",
       "      <th>Var10</th>\n",
       "      <th>Var11</th>\n",
       "      <th>Var12</th>\n",
       "      <th>Var13</th>\n",
       "      <th>Var14</th>\n",
       "      <th>Var15</th>\n",
       "      <th>Var16</th>\n",
       "      <th>Var17</th>\n",
       "      <th>Var18</th>\n",
       "      <th>Var19</th>\n",
       "      <th>Var20</th>\n",
       "      <th>Var21</th>\n",
       "      <th>Var22</th>\n",
       "      <th>Var23</th>\n",
       "      <th>Var24</th>\n",
       "      <th>Var25</th>\n",
       "      <th>Var26</th>\n",
       "      <th>Var27</th>\n",
       "      <th>Var28</th>\n",
       "      <th>Var29</th>\n",
       "      <th>Var30</th>\n",
       "      <th>Var31</th>\n",
       "      <th>Var32</th>\n",
       "      <th>Var33</th>\n",
       "      <th>Var34</th>\n",
       "      <th>Var35</th>\n",
       "      <th>Var36</th>\n",
       "      <th>Var37</th>\n",
       "      <th>Var38</th>\n",
       "      <th>Var39</th>\n",
       "      <th>Var40</th>\n",
       "      <th>...</th>\n",
       "      <th>Var191</th>\n",
       "      <th>Var192</th>\n",
       "      <th>Var193</th>\n",
       "      <th>Var194</th>\n",
       "      <th>Var195</th>\n",
       "      <th>Var196</th>\n",
       "      <th>Var197</th>\n",
       "      <th>Var198</th>\n",
       "      <th>Var199</th>\n",
       "      <th>Var200</th>\n",
       "      <th>Var201</th>\n",
       "      <th>Var202</th>\n",
       "      <th>Var203</th>\n",
       "      <th>Var204</th>\n",
       "      <th>Var205</th>\n",
       "      <th>Var206</th>\n",
       "      <th>Var207</th>\n",
       "      <th>Var208</th>\n",
       "      <th>Var209</th>\n",
       "      <th>Var210</th>\n",
       "      <th>Var211</th>\n",
       "      <th>Var212</th>\n",
       "      <th>Var213</th>\n",
       "      <th>Var214</th>\n",
       "      <th>Var215</th>\n",
       "      <th>Var216</th>\n",
       "      <th>Var217</th>\n",
       "      <th>Var218</th>\n",
       "      <th>Var219</th>\n",
       "      <th>Var220</th>\n",
       "      <th>Var221</th>\n",
       "      <th>Var222</th>\n",
       "      <th>Var223</th>\n",
       "      <th>Var224</th>\n",
       "      <th>Var225</th>\n",
       "      <th>Var226</th>\n",
       "      <th>Var227</th>\n",
       "      <th>Var228</th>\n",
       "      <th>Var229</th>\n",
       "      <th>Var230</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1526.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>184.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>464.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>166.56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3570.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bZkvyxLkBI</td>\n",
       "      <td>RO12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>taul</td>\n",
       "      <td>1K8T</td>\n",
       "      <td>lK27</td>\n",
       "      <td>ka_ns41</td>\n",
       "      <td>nQUveAzAF7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dXGu</td>\n",
       "      <td>9_Y1</td>\n",
       "      <td>FbIm</td>\n",
       "      <td>VpdQ</td>\n",
       "      <td>haYg</td>\n",
       "      <td>me75fM6ugJ</td>\n",
       "      <td>kIsH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uKAI</td>\n",
       "      <td>L84s</td>\n",
       "      <td>XfqtO3UdzaXh_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XTbPUYD</td>\n",
       "      <td>sH5Z</td>\n",
       "      <td>cJvF</td>\n",
       "      <td>FzaX</td>\n",
       "      <td>1YVfGrO</td>\n",
       "      <td>oslk</td>\n",
       "      <td>fXVEsaq</td>\n",
       "      <td>jySVZNlOJy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xb3V</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>525.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>353.52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4764966.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CEat0G8rTN</td>\n",
       "      <td>RO12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>taul</td>\n",
       "      <td>1K8T</td>\n",
       "      <td>2Ix5</td>\n",
       "      <td>qEdASpP</td>\n",
       "      <td>y2LIM01bE1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lg1t</td>\n",
       "      <td>9_Y1</td>\n",
       "      <td>k13i</td>\n",
       "      <td>sJzTlal</td>\n",
       "      <td>zm5i</td>\n",
       "      <td>me75fM6ugJ</td>\n",
       "      <td>kIsH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uKAI</td>\n",
       "      <td>L84s</td>\n",
       "      <td>NhsEn4L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kZJyVg2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FzaX</td>\n",
       "      <td>0AJo2f2</td>\n",
       "      <td>oslk</td>\n",
       "      <td>2Kb5FSF</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fKCe</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5236.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>904.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1212.0</td>\n",
       "      <td>1515.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.0</td>\n",
       "      <td>816.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>220.08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5883894.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eOQt0GoOh3</td>\n",
       "      <td>AERks4l</td>\n",
       "      <td>SEuy</td>\n",
       "      <td>taul</td>\n",
       "      <td>1K8T</td>\n",
       "      <td>ffXs</td>\n",
       "      <td>NldASpP</td>\n",
       "      <td>y4g9XoZ</td>\n",
       "      <td>vynJTq9</td>\n",
       "      <td>smXZ</td>\n",
       "      <td>4bTR</td>\n",
       "      <td>9_Y1</td>\n",
       "      <td>MGOA</td>\n",
       "      <td>VpdQ</td>\n",
       "      <td>haYg</td>\n",
       "      <td>DHn_WUyBhW_whjA88g9bvA64_</td>\n",
       "      <td>kIsH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uKAI</td>\n",
       "      <td>L84s</td>\n",
       "      <td>UbxQ8lZ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTGHfSv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pMWAe2U</td>\n",
       "      <td>bHR7</td>\n",
       "      <td>UYBR</td>\n",
       "      <td>FzaX</td>\n",
       "      <td>JFM1BiF</td>\n",
       "      <td>Al6ZaUT</td>\n",
       "      <td>NKv4yOc</td>\n",
       "      <td>jySVZNlOJy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kG3k</td>\n",
       "      <td>Qu4f</td>\n",
       "      <td>02N6s8f</td>\n",
       "      <td>ib5G6X1eUxUn6</td>\n",
       "      <td>am7c</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jg69tYsGvO</td>\n",
       "      <td>RO12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>taul</td>\n",
       "      <td>1K8T</td>\n",
       "      <td>ssAy</td>\n",
       "      <td>_ybO0dd</td>\n",
       "      <td>4hMlgkf58mhwh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W8mQ</td>\n",
       "      <td>9_Y1</td>\n",
       "      <td>YULl</td>\n",
       "      <td>VpdQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>me75fM6ugJ</td>\n",
       "      <td>kIsH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uKAI</td>\n",
       "      <td>Mtgm</td>\n",
       "      <td>NhsEn4L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kq0dQfu</td>\n",
       "      <td>eKej</td>\n",
       "      <td>UYBR</td>\n",
       "      <td>FzaX</td>\n",
       "      <td>L91KIiz</td>\n",
       "      <td>oslk</td>\n",
       "      <td>CE7uk3u</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FSa2</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1029.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3216.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IXSgUHShse</td>\n",
       "      <td>RO12</td>\n",
       "      <td>SEuy</td>\n",
       "      <td>taul</td>\n",
       "      <td>1K8T</td>\n",
       "      <td>uNkU</td>\n",
       "      <td>EKR938I</td>\n",
       "      <td>ThrHXVS</td>\n",
       "      <td>0v21jmy</td>\n",
       "      <td>smXZ</td>\n",
       "      <td>xklU</td>\n",
       "      <td>9_Y1</td>\n",
       "      <td>RVjC</td>\n",
       "      <td>sJzTlal</td>\n",
       "      <td>6JmL</td>\n",
       "      <td>me75fM6ugJ</td>\n",
       "      <td>kIsH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uKAI</td>\n",
       "      <td>L84s</td>\n",
       "      <td>XfqtO3UdzaXh_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SJs3duv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11p4mKe</td>\n",
       "      <td>H3p7</td>\n",
       "      <td>UYBR</td>\n",
       "      <td>FzaX</td>\n",
       "      <td>OrnLfvc</td>\n",
       "      <td>oslk</td>\n",
       "      <td>1J2cvxe</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kG3k</td>\n",
       "      <td>FSa2</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>mj86</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 230 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Var1  Var2  Var3  Var4  Var5  ...  Var226   Var227         Var228  Var229  Var230\n",
       "0   NaN   NaN   NaN   NaN   NaN  ...    xb3V     RAYp  F2FyR07IdsN7I     NaN     NaN\n",
       "1   NaN   NaN   NaN   NaN   NaN  ...    fKCe     RAYp  F2FyR07IdsN7I     NaN     NaN\n",
       "2   NaN   NaN   NaN   NaN   NaN  ...    Qu4f  02N6s8f  ib5G6X1eUxUn6    am7c     NaN\n",
       "3   NaN   NaN   NaN   NaN   NaN  ...    FSa2     RAYp  F2FyR07IdsN7I     NaN     NaN\n",
       "4   NaN   NaN   NaN   NaN   NaN  ...    FSa2     RAYp  F2FyR07IdsN7I    mj86     NaN\n",
       "\n",
       "[5 rows x 230 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7c9cf",
   "metadata": {
    "input_tokens": 6,
    "output_tokens": 12526,
    "time_run": "8:35:56a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var1</th>\n",
       "      <th>Var2</th>\n",
       "      <th>Var3</th>\n",
       "      <th>Var4</th>\n",
       "      <th>Var5</th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var8</th>\n",
       "      <th>Var9</th>\n",
       "      <th>Var10</th>\n",
       "      <th>Var11</th>\n",
       "      <th>Var12</th>\n",
       "      <th>Var13</th>\n",
       "      <th>Var14</th>\n",
       "      <th>Var15</th>\n",
       "      <th>Var16</th>\n",
       "      <th>Var17</th>\n",
       "      <th>Var18</th>\n",
       "      <th>Var19</th>\n",
       "      <th>Var20</th>\n",
       "      <th>Var21</th>\n",
       "      <th>Var22</th>\n",
       "      <th>Var23</th>\n",
       "      <th>Var24</th>\n",
       "      <th>Var25</th>\n",
       "      <th>Var26</th>\n",
       "      <th>Var27</th>\n",
       "      <th>Var28</th>\n",
       "      <th>Var29</th>\n",
       "      <th>Var30</th>\n",
       "      <th>Var31</th>\n",
       "      <th>Var32</th>\n",
       "      <th>Var33</th>\n",
       "      <th>Var34</th>\n",
       "      <th>Var35</th>\n",
       "      <th>Var36</th>\n",
       "      <th>Var37</th>\n",
       "      <th>Var38</th>\n",
       "      <th>Var39</th>\n",
       "      <th>Var40</th>\n",
       "      <th>...</th>\n",
       "      <th>Var153</th>\n",
       "      <th>Var154</th>\n",
       "      <th>Var155</th>\n",
       "      <th>Var156</th>\n",
       "      <th>Var157</th>\n",
       "      <th>Var158</th>\n",
       "      <th>Var159</th>\n",
       "      <th>Var160</th>\n",
       "      <th>Var161</th>\n",
       "      <th>Var162</th>\n",
       "      <th>Var163</th>\n",
       "      <th>Var164</th>\n",
       "      <th>Var165</th>\n",
       "      <th>Var166</th>\n",
       "      <th>Var167</th>\n",
       "      <th>Var168</th>\n",
       "      <th>Var169</th>\n",
       "      <th>Var170</th>\n",
       "      <th>Var171</th>\n",
       "      <th>Var172</th>\n",
       "      <th>Var173</th>\n",
       "      <th>Var174</th>\n",
       "      <th>Var175</th>\n",
       "      <th>Var176</th>\n",
       "      <th>Var177</th>\n",
       "      <th>Var178</th>\n",
       "      <th>Var179</th>\n",
       "      <th>Var180</th>\n",
       "      <th>Var181</th>\n",
       "      <th>Var182</th>\n",
       "      <th>Var183</th>\n",
       "      <th>Var184</th>\n",
       "      <th>Var185</th>\n",
       "      <th>Var186</th>\n",
       "      <th>Var187</th>\n",
       "      <th>Var188</th>\n",
       "      <th>Var189</th>\n",
       "      <th>Var190</th>\n",
       "      <th>Var209</th>\n",
       "      <th>Var230</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>702.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1240.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>1.487000e+03</td>\n",
       "      <td>44471.000000</td>\n",
       "      <td>44461.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>1.487000e+03</td>\n",
       "      <td>1240.000000</td>\n",
       "      <td>558.000000</td>\n",
       "      <td>44461.000000</td>\n",
       "      <td>1240.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44471.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>42770.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>44989.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.470000e+02</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>1.241000e+03</td>\n",
       "      <td>1.579000e+03</td>\n",
       "      <td>4.499100e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.499100e+04</td>\n",
       "      <td>7.020000e+02</td>\n",
       "      <td>1579.00000</td>\n",
       "      <td>694.000000</td>\n",
       "      <td>1129.000000</td>\n",
       "      <td>873.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>1.241000e+03</td>\n",
       "      <td>4.499100e+04</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>8.730000e+02</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1.083000e+03</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1240.000000</td>\n",
       "      <td>1.241000e+03</td>\n",
       "      <td>646.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>7.020000e+02</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>1.579000e+03</td>\n",
       "      <td>1.241000e+03</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>702.00000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>21022.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.487179</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>425.298387</td>\n",
       "      <td>0.125396</td>\n",
       "      <td>2.387933e+05</td>\n",
       "      <td>1326.437116</td>\n",
       "      <td>6.809496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.145299</td>\n",
       "      <td>3.926057e+05</td>\n",
       "      <td>8.625806</td>\n",
       "      <td>16.071685</td>\n",
       "      <td>1249.688401</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.239275</td>\n",
       "      <td>11.393287</td>\n",
       "      <td>7.215959</td>\n",
       "      <td>0.245092</td>\n",
       "      <td>NaN</td>\n",
       "      <td>234.518225</td>\n",
       "      <td>290.245382</td>\n",
       "      <td>7.535306</td>\n",
       "      <td>4.507926</td>\n",
       "      <td>96.827010</td>\n",
       "      <td>0.070612</td>\n",
       "      <td>0.028245</td>\n",
       "      <td>224.507669</td>\n",
       "      <td>0.022792</td>\n",
       "      <td>7.435897</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.270050e+05</td>\n",
       "      <td>1.173247</td>\n",
       "      <td>0.716810</td>\n",
       "      <td>1.595539e+05</td>\n",
       "      <td>6.485221e+05</td>\n",
       "      <td>2.579107e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.962933</td>\n",
       "      <td>...</td>\n",
       "      <td>6.181967e+06</td>\n",
       "      <td>1.538221e+06</td>\n",
       "      <td>0.80114</td>\n",
       "      <td>169.472622</td>\n",
       "      <td>33.073516</td>\n",
       "      <td>1.896907</td>\n",
       "      <td>4.713940</td>\n",
       "      <td>38.803005</td>\n",
       "      <td>3.374288</td>\n",
       "      <td>3.360168e+05</td>\n",
       "      <td>4.860780e+05</td>\n",
       "      <td>1.753642</td>\n",
       "      <td>2.884236e+04</td>\n",
       "      <td>22.553463</td>\n",
       "      <td>NaN</td>\n",
       "      <td>332.938575</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.041096</td>\n",
       "      <td>3.674517e+05</td>\n",
       "      <td>9.744452</td>\n",
       "      <td>0.006846</td>\n",
       "      <td>7.044965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.716129</td>\n",
       "      <td>6.188884e+05</td>\n",
       "      <td>16.687307</td>\n",
       "      <td>3.138062</td>\n",
       "      <td>3.776755e+06</td>\n",
       "      <td>0.611456</td>\n",
       "      <td>1.416638e+06</td>\n",
       "      <td>7.777380e+04</td>\n",
       "      <td>8.460919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.299145</td>\n",
       "      <td>16.54416</td>\n",
       "      <td>167.368477</td>\n",
       "      <td>270.142137</td>\n",
       "      <td>22007.045192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>40.709951</td>\n",
       "      <td>0.141933</td>\n",
       "      <td>4270.193518</td>\n",
       "      <td>1.275481</td>\n",
       "      <td>6.441259e+05</td>\n",
       "      <td>2685.693668</td>\n",
       "      <td>6.326053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154.777855</td>\n",
       "      <td>9.280896e+05</td>\n",
       "      <td>2.869558</td>\n",
       "      <td>64.185508</td>\n",
       "      <td>2794.954874</td>\n",
       "      <td>3.714316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.386254</td>\n",
       "      <td>49.493856</td>\n",
       "      <td>34.415427</td>\n",
       "      <td>1.781497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>565.560147</td>\n",
       "      <td>704.489990</td>\n",
       "      <td>49.449472</td>\n",
       "      <td>9.928819</td>\n",
       "      <td>214.318283</td>\n",
       "      <td>0.528836</td>\n",
       "      <td>0.247210</td>\n",
       "      <td>98.520240</td>\n",
       "      <td>0.212436</td>\n",
       "      <td>8.852461</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.957921e+05</td>\n",
       "      <td>4.377707</td>\n",
       "      <td>2.996007</td>\n",
       "      <td>3.277151e+05</td>\n",
       "      <td>1.382225e+06</td>\n",
       "      <td>3.010076e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81.410005</td>\n",
       "      <td>...</td>\n",
       "      <td>4.348926e+06</td>\n",
       "      <td>2.280224e+06</td>\n",
       "      <td>3.04295</td>\n",
       "      <td>544.686538</td>\n",
       "      <td>187.525494</td>\n",
       "      <td>6.325709</td>\n",
       "      <td>11.027292</td>\n",
       "      <td>99.497149</td>\n",
       "      <td>8.579672</td>\n",
       "      <td>9.731987e+05</td>\n",
       "      <td>8.488638e+05</td>\n",
       "      <td>8.104872</td>\n",
       "      <td>1.157869e+05</td>\n",
       "      <td>96.211261</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102.879214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.921735</td>\n",
       "      <td>6.042377e+05</td>\n",
       "      <td>10.397319</td>\n",
       "      <td>0.132503</td>\n",
       "      <td>36.186111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.002253</td>\n",
       "      <td>1.306030e+06</td>\n",
       "      <td>59.016629</td>\n",
       "      <td>28.206035</td>\n",
       "      <td>3.785696e+06</td>\n",
       "      <td>2.495681</td>\n",
       "      <td>2.279786e+06</td>\n",
       "      <td>2.016188e+05</td>\n",
       "      <td>46.973777</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.781967</td>\n",
       "      <td>60.22303</td>\n",
       "      <td>113.980072</td>\n",
       "      <td>86.707692</td>\n",
       "      <td>29085.146490</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-66.880000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-6.420000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>518.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.920000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>166.560000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.794000e+03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.232346e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>283.360000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.917352e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>19.380000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>2732.670000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>861.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>232.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>220.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.290246e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.131560e+06</td>\n",
       "      <td>3.415240e+05</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.346220e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>331.120000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.654884e+05</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.431310e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.167780e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>197.640000</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>12668.940000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.187425e+05</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>2.628630e+05</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1604.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>170.080000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>285.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>266.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.297200e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.587940e+05</td>\n",
       "      <td>6.832260e+05</td>\n",
       "      <td>4.570944e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.037338e+07</td>\n",
       "      <td>2.267270e+06</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.825560e+05</td>\n",
       "      <td>6.159000e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.760000e+02</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>383.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.270468e+05</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.544140e+05</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.471827e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.844952e+06</td>\n",
       "      <td>4.881000e+04</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>14.00000</td>\n",
       "      <td>252.960000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>29396.340000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>680.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>130668.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>6.048550e+06</td>\n",
       "      <td>131761.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2300.000000</td>\n",
       "      <td>1.232559e+07</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>197872.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>434.920000</td>\n",
       "      <td>1220.000000</td>\n",
       "      <td>948.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36272.000000</td>\n",
       "      <td>45340.000000</td>\n",
       "      <td>1555.000000</td>\n",
       "      <td>494.000000</td>\n",
       "      <td>13168.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5158.560000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.088640e+07</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>2.419600e+06</td>\n",
       "      <td>1.163502e+07</td>\n",
       "      <td>1.884690e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2648.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.390780e+07</td>\n",
       "      <td>1.504856e+07</td>\n",
       "      <td>35.00000</td>\n",
       "      <td>8050.000000</td>\n",
       "      <td>5440.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>4862.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.088640e+07</td>\n",
       "      <td>1.451520e+07</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>1.209600e+06</td>\n",
       "      <td>2261.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1270.480000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>957.000000</td>\n",
       "      <td>5.443200e+06</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1156.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>892.000000</td>\n",
       "      <td>8.554350e+06</td>\n",
       "      <td>1345.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>1.428483e+07</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>1.199478e+07</td>\n",
       "      <td>3.048400e+06</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>910.00000</td>\n",
       "      <td>628.620000</td>\n",
       "      <td>642.000000</td>\n",
       "      <td>230427.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 192 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Var1         Var2           Var3  ...         Var190  Var209  Var230\n",
       "count  702.000000  1241.000000    1240.000000  ...     333.000000     0.0     0.0\n",
       "mean    11.487179     0.004029     425.298387  ...   22007.045192     NaN     NaN\n",
       "std     40.709951     0.141933    4270.193518  ...   29085.146490     NaN     NaN\n",
       "min      0.000000     0.000000       0.000000  ...       0.000000     NaN     NaN\n",
       "25%      0.000000     0.000000       0.000000  ...    2732.670000     NaN     NaN\n",
       "50%      0.000000     0.000000       0.000000  ...   12668.940000     NaN     NaN\n",
       "75%     16.000000     0.000000       0.000000  ...   29396.340000     NaN     NaN\n",
       "max    680.000000     5.000000  130668.000000  ...  230427.000000     NaN     NaN\n",
       "\n",
       "[8 rows x 192 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6dc05c",
   "metadata": {
    "input_tokens": 6,
    "output_tokens": 126,
    "time_run": "8:35:56a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Columns: 230 entries, Var1 to Var230\n",
      "dtypes: float64(191), int64(1), object(38)\n",
      "memory usage: 87.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8269d26",
   "metadata": {
    "input_tokens": 4,
    "output_tokens": 22,
    "time_run": "8:35:56a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 230)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa9f747",
   "metadata": {
    "input_tokens": 6,
    "output_tokens": 354,
    "time_run": "8:35:56a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Var1', 'Var2', 'Var3', 'Var4', 'Var5', 'Var6', 'Var7', 'Var8', 'Var9',\n",
       "       'Var10',\n",
       "       ...\n",
       "       'Var221', 'Var222', 'Var223', 'Var224', 'Var225', 'Var226', 'Var227',\n",
       "       'Var228', 'Var229', 'Var230'],\n",
       "      dtype='object', length=230)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32607933",
   "metadata": {
    "input_tokens": 9,
    "output_tokens": 51,
    "time_run": "8:35:56a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/app/data/static/case study-2024/train_churn_labels.csv')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccad9a1c",
   "metadata": {
    "input_tokens": 18,
    "time_run": "8:35:56a"
   },
   "outputs": [],
   "source": [
    "df_label = pd.read_csv(path.ls()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90be0b3b",
   "metadata": {
    "input_tokens": 6,
    "output_tokens": 360,
    "time_run": "8:35:56a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label\n",
       "0     -1\n",
       "1      1\n",
       "2     -1\n",
       "3     -1\n",
       "4     -1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a5e845",
   "metadata": {
    "input_tokens": 232
   },
   "source": [
    "# Task & Expectations:\n",
    "- Attached is a training dataset and corresponding label file for a classification problem.\n",
    "- Your task is to propose a solution approach, documenting the steps, ideas, observations, data\n",
    "analysis, and feature engineering (if any). There’s no need to code the solution, but feel free to\n",
    "provide as much detail as you'd like.\n",
    "- Please include the success criteria and performance metrics you would use for model\n",
    "evaluation, along with the reasoning behind your choices. All decisions should be supported by\n",
    "data observations or sound reasoning.\n",
    "- We will assess the quality of your response based on the submitted document (Word or\n",
    "PowerPoint). Feel free to share other support documents like notebooks, excels etc. which can\n",
    "give us more confidence around the work done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b86487b",
   "metadata": {
    "input_tokens": 283
   },
   "source": [
    "## Plan\n",
    "\n",
    "1. **Data Understanding**\n",
    "   - Check basic statistics of features\n",
    "   - Analyze missing values\n",
    "   - Check distribution of churn labels (-1 and 1)\n",
    "\n",
    "2. **Data Preparation**\n",
    "   - Handle missing values (simple imputation)\n",
    "   - Convert categorical variables to numeric\n",
    "   - Create a clean dataset ready for modeling\n",
    "\n",
    "3. **Feature Selection**\n",
    "   - Identify the most important features\n",
    "   - Remove redundant or irrelevant features\n",
    "   - Focus on features with strongest relationship to churn\n",
    "\n",
    "4. **Modeling**\n",
    "   - Start with a simple model (like Random Forest)\n",
    "   - Train and evaluate the model\n",
    "   - Tune hyperparameters if needed\n",
    "\n",
    "5. **Evaluation**\n",
    "   - Use appropriate metrics (accuracy, precision, recall)\n",
    "   - Validate results with cross-validation\n",
    "   - Interpret model predictions\n",
    "\n",
    "6. **Business Insights**\n",
    "   - Identify key factors that predict churn\n",
    "   - Suggest actionable recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39921a85",
   "metadata": {
    "input_tokens": 72,
    "time_run": "8:35:56a"
   },
   "outputs": [],
   "source": [
    "# Categorize columns by missing percentage:\n",
    "\n",
    "## High (>50%): Consider dropping\n",
    "## Medium (10-50%): Impute with care or create \"missing\" flags\n",
    "## Low (<10%): Standard imputation is reasonable\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018f18e3",
   "metadata": {
    "input_tokens": 72,
    "time_run": "8:35:56a"
   },
   "outputs": [],
   "source": [
    "def get_missing_cols(df, threshold=0.99):\n",
    "    missing_pct = df.isna().sum() / len(df)\n",
    "    mis_col = missing_pct[missing_pct > threshold].index.tolist()\n",
    "    return len(mis_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6671dfbd",
   "metadata": {
    "input_tokens": 49,
    "output_tokens": 30,
    "time_run": "8:35:56a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[161, 154, 154, 24]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds = [0.50,0.80,0.90,0.99]\n",
    "[get_missing_cols(df_raw, t) for t in thresholds]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fa8b8c",
   "metadata": {
    "input_tokens": 76
   },
   "source": [
    "Remove columns with >99% missing values as they provide minimal signal. For columns with 90-99% missing, evaluate whether the pattern of missingness itself is informative before deciding, and consider creating binary flags to capture this information rather than dropping completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7acf3d",
   "metadata": {
    "input_tokens": 28,
    "output_tokens": 106,
    "time_run": "8:35:56a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m cols_to_drop_90,_ = get_missing_cols(df_raw,threshold=\u001b[32m0.90\u001b[39m)\n",
      "\n",
      "\u001b[31mTypeError\u001b[39m: cannot unpack non-iterable int object\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-4c54d1e37cad>\", line 1, in <module>\n    cols_to_drop_90,_ = get_missing_cols(df_raw,threshold=0.90)\n    ^^^^^^^^^^^^^^^^^\n",
      "TypeError: cannot unpack non-iterable int object\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop_90,_ = get_missing_cols(df_raw,threshold=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2fd710",
   "metadata": {
    "input_tokens": 15,
    "output_tokens": 106,
    "time_run": "8:35:56a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcols_to_drop_90\u001b[49m[\u001b[32m0\u001b[39m:\u001b[32m5\u001b[39m]\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'cols_to_drop_90' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cols_to_drop_90' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-1c0d36b5a48b>\", line 1, in <module>\n    cols_to_drop_90[0:5]\n    ^^^^^^^^^^^^^^^\n",
      "NameError: name 'cols_to_drop_90' is not defined\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop_90[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f24b0ce",
   "metadata": {
    "input_tokens": 22,
    "output_tokens": 114,
    "time_run": "8:35:56a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_clean =  df_raw.drop(columns=\u001b[43mcols_to_drop_90\u001b[49m)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'cols_to_drop_90' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cols_to_drop_90' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-7a734196ebd5>\", line 1, in <module>\n    df_clean =  df_raw.drop(columns=cols_to_drop_90)\n                                    ^^^^^^^^^^^^^^^\n",
      "NameError: name 'cols_to_drop_90' is not defined\n"
     ]
    }
   ],
   "source": [
    "df_clean =  df_raw.drop(columns=cols_to_drop_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08384da",
   "metadata": {
    "input_tokens": 4,
    "output_tokens": 93,
    "time_run": "8:35:56a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_clean\u001b[49m.shape\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-1348133b9261>\", line 1, in <module>\n    df_clean.shape\n    ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610cee55",
   "metadata": {
    "input_tokens": 4,
    "output_tokens": 22,
    "time_run": "8:35:56a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 230)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03838202",
   "metadata": {
    "input_tokens": 12,
    "time_run": "8:35:58a"
   },
   "outputs": [],
   "source": [
    "from fastai.tabular.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81a4ff8",
   "metadata": {
    "input_tokens": 4,
    "output_tokens": 93,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_clean\u001b[49m.shape\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-1348133b9261>\", line 1, in <module>\n    df_clean.shape\n    ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0f3abf",
   "metadata": {
    "input_tokens": 4,
    "output_tokens": 93,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_clean\u001b[49m.columns\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-8cd9b42ecf3d>\", line 1, in <module>\n    df_clean.columns\n    ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553a3278",
   "metadata": {
    "input_tokens": 18,
    "time_run": "8:35:59a"
   },
   "outputs": [],
   "source": [
    "df_label = pd.read_csv(path.ls()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6beb97",
   "metadata": {
    "input_tokens": 12,
    "output_tokens": 57,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "-1       0.92656\n",
       " 1       0.07344\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5515c5b",
   "metadata": {
    "input_tokens": 34,
    "output_tokens": 132,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m cont_names, cat_names  = cont_cat_split(\u001b[43mdf_clean\u001b[49m, max_card =\u001b[32m20\u001b[39m ,dep_var=\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-eec70de782e4>\", line 1, in <module>\n    cont_names, cat_names  = cont_cat_split(df_clean, max_card =20 ,dep_var='label')\n                                            ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "cont_names, cat_names  = cont_cat_split(df_clean, max_card =20 ,dep_var='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e16ef08",
   "metadata": {
    "input_tokens": 12,
    "output_tokens": 99,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mcont_names\u001b[49m),\u001b[38;5;28mlen\u001b[39m(cat_names)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'cont_names' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cont_names' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-c9bb336179aa>\", line 1, in <module>\n    len(cont_names),len(cat_names)\n        ^^^^^^^^^^\n",
      "NameError: name 'cont_names' is not defined\n"
     ]
    }
   ],
   "source": [
    "len(cont_names),len(cat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd7877f",
   "metadata": {
    "input_tokens": 13,
    "output_tokens": 100,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_clean\u001b[49m[cat_names].nunique()\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-a3f88f1a8f95>\", line 1, in <module>\n    df_clean[cat_names].nunique()\n    ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "df_clean[cat_names].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a5334d",
   "metadata": {
    "input_tokens": 9,
    "output_tokens": 108,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_clean\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mVar192\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-89ecc7815586>\", line 1, in <module>\n    df_clean['Var192']\n    ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "df_clean['Var192']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb37dd0",
   "metadata": {
    "input_tokens": 798
   },
   "source": [
    "For high cardinality categorical columns in fastai, you have several options:\n",
    "\n",
    "**Simple approaches first:**\n",
    "\n",
    "```python\n",
    "# 1. Set a higher cardinality threshold when splitting columns\n",
    "cont_names, cat_names = cont_cat_split(df_clean, max_card=100, dep_var='label')\n",
    "\n",
    "# 2. Move specific high-cardinality columns to continuous\n",
    "high_card_cols = ['Var198', 'Var199', 'Var200', 'Var202', 'Var214', 'Var216', 'Var217', 'Var220', 'Var222']\n",
    "for col in high_card_cols:\n",
    "    if col in cat_names: \n",
    "        cat_names.remove(col)\n",
    "        cont_names.append(col)\n",
    "```\n",
    "\n",
    "**More advanced approaches:**\n",
    "\n",
    "1. **Target encoding** - Replace categories with their target mean:\n",
    "   ```python\n",
    "   from sklearn.preprocessing import TargetEncoder\n",
    "   \n",
    "   def apply_target_encoding(df, cols, target='label'):\n",
    "       df_new = df.copy()\n",
    "       for col in cols:\n",
    "           encoder = TargetEncoder()\n",
    "           df_new[f'{col}_encoded'] = encoder.fit_transform(df[col].fillna('missing'), df[target])\n",
    "       return df_new\n",
    "   ```\n",
    "\n",
    "2. **Clustering similar values** - Group rare categories:\n",
    "   ```python\n",
    "   def cluster_rare_categories(df, col, min_freq=100):\n",
    "       counts = df[col].value_counts()\n",
    "       rare_cats = counts[counts < min_freq].index\n",
    "       df_new = df.copy()\n",
    "       df_new[col] = df_new[col].apply(lambda x: 'rare_category' if x in rare_cats else x)\n",
    "       return df_new\n",
    "   ```\n",
    "\n",
    "3. **Text embeddings** - For text-like categorical columns:\n",
    "   ```python\n",
    "   # For columns that look like text, consider embeddings\n",
    "   from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "   from sklearn.decomposition import TruncatedSVD\n",
    "   \n",
    "   def embed_text_columns(df, text_cols, n_components=5):\n",
    "       df_new = df.copy()\n",
    "       for col in text_cols:\n",
    "           tfidf = TfidfVectorizer(min_df=5)\n",
    "           text_data = df[col].fillna('').astype(str)\n",
    "           tfidf_matrix = tfidf.fit_transform(text_data)\n",
    "           svd = TruncatedSVD(n_components=n_components)\n",
    "           text_features = svd.fit_transform(tfidf_matrix)\n",
    "           for i in range(n_components):\n",
    "               df_new[f'{col}_emb_{i}'] = text_features[:, i]\n",
    "       return df_new\n",
    "   ```\n",
    "\n",
    "Start with the simpler approaches and evaluate model performance before moving to more complex techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7409ef7d",
   "metadata": {
    "input_tokens": 4,
    "output_tokens": 93,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_clean\u001b[49m.shape\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-1348133b9261>\", line 1, in <module>\n    df_clean.shape\n    ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ac4135",
   "metadata": {
    "input_tokens": 33,
    "output_tokens": 130,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m cont_names, cat_names = cont_cat_split(\u001b[43mdf_clean\u001b[49m, max_card=\u001b[32m100\u001b[39m, dep_var=\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-4d6411dde098>\", line 1, in <module>\n    cont_names, cat_names = cont_cat_split(df_clean, max_card=100, dep_var='label')\n                                           ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "cont_names, cat_names = cont_cat_split(df_clean, max_card=100, dep_var='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea22c12",
   "metadata": {
    "input_tokens": 12,
    "output_tokens": 99,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mcont_names\u001b[49m),\u001b[38;5;28mlen\u001b[39m(cat_names)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'cont_names' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cont_names' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-c9bb336179aa>\", line 1, in <module>\n    len(cont_names),len(cat_names)\n        ^^^^^^^^^^\n",
      "NameError: name 'cont_names' is not defined\n"
     ]
    }
   ],
   "source": [
    "len(cont_names),len(cat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f3e1d8",
   "metadata": {
    "input_tokens": 3,
    "output_tokens": 91,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcat_names\u001b[49m\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'cat_names' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cat_names' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-4ce89cb36905>\", line 1, in <module>\n    cat_names\n",
      "NameError: name 'cat_names' is not defined\n"
     ]
    }
   ],
   "source": [
    "cat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b4646",
   "metadata": {
    "input_tokens": 6,
    "output_tokens": 93,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_clean\u001b[49m.info()\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-85e1696ee713>\", line 1, in <module>\n    df_clean.info()\n    ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6bb764",
   "metadata": {
    "input_tokens": 12,
    "output_tokens": 109,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_clean\u001b[49m.select_dtypes(\u001b[33m'\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-1c803fe39d2a>\", line 1, in <module>\n    df_clean.select_dtypes('object')\n    ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "df_clean.select_dtypes('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91031cf",
   "metadata": {
    "input_tokens": 25,
    "output_tokens": 130,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m more_than_100 = \u001b[43mdf_clean\u001b[49m.select_dtypes(\u001b[33m'\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m'\u001b[39m).nunique()>\u001b[32m100\u001b[39m\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-ea332d23519e>\", line 1, in <module>\n    more_than_100 = df_clean.select_dtypes('object').nunique()>100\n                    ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "more_than_100 = df_clean.select_dtypes('object').nunique()>100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e66a1df",
   "metadata": {
    "input_tokens": 24,
    "output_tokens": 123,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_clean\u001b[49m.select_dtypes(\u001b[33m'\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m'\u001b[39m).nunique()[more_than_100]\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-56962bfbce8a>\", line 1, in <module>\n    df_clean.select_dtypes('object').nunique()[more_than_100]\n    ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "df_clean.select_dtypes('object').nunique()[more_than_100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8ecf4c",
   "metadata": {
    "input_tokens": 34,
    "output_tokens": 133,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m high_card_cols = \u001b[43mdf_clean\u001b[49m.select_dtypes(\u001b[33m'\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m'\u001b[39m).nunique()[more_than_100].index.tolist()\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-c6f1ce43b19a>\", line 1, in <module>\n    high_card_cols = df_clean.select_dtypes('object').nunique()[more_than_100].index.tolist()\n                     ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "high_card_cols = df_clean.select_dtypes('object').nunique()[more_than_100].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7238ac",
   "metadata": {
    "input_tokens": 4,
    "output_tokens": 94,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mhigh_card_cols\u001b[49m\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'high_card_cols' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'high_card_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-ea0c17a9e232>\", line 1, in <module>\n    high_card_cols\n",
      "NameError: name 'high_card_cols' is not defined\n"
     ]
    }
   ],
   "source": [
    "high_card_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbd4e70",
   "metadata": {
    "input_tokens": 9,
    "output_tokens": 108,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_clean\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mVar192\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-89ecc7815586>\", line 1, in <module>\n    df_clean['Var192']\n    ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "df_clean['Var192']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a0667c",
   "metadata": {
    "input_tokens": 18,
    "output_tokens": 130,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_clean\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mVar192\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[33m'\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m'\u001b[39m).cat.codes\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-973fae3f876f>\", line 1, in <module>\n    df_clean['Var192'].astype('category').cat.codes\n    ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "df_clean['Var192'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d071d4",
   "metadata": {
    "input_tokens": 4,
    "output_tokens": 93,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_clean\u001b[49m.shape\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-1348133b9261>\", line 1, in <module>\n    df_clean.shape\n    ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eaf1df",
   "metadata": {
    "input_tokens": 33,
    "output_tokens": 130,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m cont_names, cat_names = cont_cat_split(\u001b[43mdf_clean\u001b[49m, max_card=\u001b[32m100\u001b[39m, dep_var=\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-4d6411dde098>\", line 1, in <module>\n    cont_names, cat_names = cont_cat_split(df_clean, max_card=100, dep_var='label')\n                                           ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "cont_names, cat_names = cont_cat_split(df_clean, max_card=100, dep_var='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f45b24",
   "metadata": {
    "input_tokens": 12,
    "output_tokens": 99,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mcont_names\u001b[49m),\u001b[38;5;28mlen\u001b[39m(cat_names)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'cont_names' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cont_names' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-c9bb336179aa>\", line 1, in <module>\n    len(cont_names),len(cat_names)\n        ^^^^^^^^^^\n",
      "NameError: name 'cont_names' is not defined\n"
     ]
    }
   ],
   "source": [
    "len(cont_names),len(cat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a8743",
   "metadata": {
    "input_tokens": 76,
    "output_tokens": 174,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[43mhigh_card_cols\u001b[49m:\n",
      "\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cat_names:\n",
      "\u001b[32m      3\u001b[39m         df_clean[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_encoded\u001b[39m\u001b[33m'\u001b[39m] = df_clean[col].astype(\u001b[33m'\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m'\u001b[39m).cat.codes\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'high_card_cols' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'high_card_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-eae625684a9f>\", line 1, in <module>\n    for col in high_card_cols:\n               ^^^^^^^^^^^^^^\n",
      "NameError: name 'high_card_cols' is not defined\n"
     ]
    }
   ],
   "source": [
    "for col in high_card_cols:\n",
    "    if col in cat_names:\n",
    "        df_clean[f'{col}_encoded'] = df_clean[col].astype('category').cat.codes\n",
    "        \n",
    "        cat_names.remove(col)\n",
    "        cont_names.append(f'{col}_encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d49168",
   "metadata": {
    "input_tokens": 4,
    "output_tokens": 93,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_clean\u001b[49m.shape\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-1348133b9261>\", line 1, in <module>\n    df_clean.shape\n    ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392cd563",
   "metadata": {
    "input_tokens": 12,
    "output_tokens": 109,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_clean\u001b[49m.select_dtypes(\u001b[33m'\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-1c803fe39d2a>\", line 1, in <module>\n    df_clean.select_dtypes('object')\n    ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "df_clean.select_dtypes('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7d44a7",
   "metadata": {
    "input_tokens": 12,
    "output_tokens": 99,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mcont_names\u001b[49m),\u001b[38;5;28mlen\u001b[39m(cat_names)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'cont_names' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cont_names' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-c9bb336179aa>\", line 1, in <module>\n    len(cont_names),len(cat_names)\n        ^^^^^^^^^^\n",
      "NameError: name 'cont_names' is not defined\n"
     ]
    }
   ],
   "source": [
    "len(cont_names),len(cat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a14cb65",
   "metadata": {
    "input_tokens": 7,
    "output_tokens": 96,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mhigh_card_cols\u001b[49m)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'high_card_cols' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'high_card_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-ab3e3b553033>\", line 1, in <module>\n    len(high_card_cols)\n        ^^^^^^^^^^^^^^\n",
      "NameError: name 'high_card_cols' is not defined\n"
     ]
    }
   ],
   "source": [
    "len(high_card_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03960e29",
   "metadata": {
    "input_tokens": 6,
    "output_tokens": 16,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "42 + 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36dae7f",
   "metadata": {
    "input_tokens": 6,
    "output_tokens": 16,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "34 -11 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baca2fa",
   "metadata": {
    "input_tokens": 9,
    "output_tokens": 96,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_clean\u001b[49m[cont_names]\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-c38642bbf98e>\", line 1, in <module>\n    df_clean[cont_names]\n    ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "df_clean[cont_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873761df",
   "metadata": {
    "input_tokens": 13,
    "output_tokens": 100,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_clean\u001b[49m[cont_names[-\u001b[32m11\u001b[39m:]]\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-731e2fdabbd3>\", line 1, in <module>\n    df_clean[cont_names[-11:]]\n    ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "df_clean[cont_names[-11:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c925c5",
   "metadata": {
    "input_tokens": 18,
    "time_run": "8:35:59a"
   },
   "outputs": [],
   "source": [
    "procs = [FillMissing, Categorify, Normalize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6747a7",
   "metadata": {
    "input_tokens": 10,
    "output_tokens": 97,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_clean = \u001b[43mdf_clean\u001b[49m.copy()\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-9982b602a75c>\", line 1, in <module>\n    df_clean = df_clean.copy()\n               ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "df_clean = df_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdda9b44",
   "metadata": {
    "input_tokens": 25,
    "time_run": "8:35:59a"
   },
   "outputs": [],
   "source": [
    "splits = RandomSplitter(valid_pct=0.2)(range_of(df_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c5cd91",
   "metadata": {
    "input_tokens": 73,
    "output_tokens": 180,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m to = TabularPandas(\u001b[43mdf_clean\u001b[49m, procs=procs, cat_names=cat_names, cont_names=cont_names,\n",
      "\u001b[32m      2\u001b[39m                   y_names=\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m, y_block =CategoryBlock ,splits=splits)\n",
      "\u001b[32m      4\u001b[39m to.show(\u001b[32m3\u001b[39m)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-36eb55331fa8>\", line 1, in <module>\n    to = TabularPandas(df_clean, procs=procs, cat_names=cat_names, cont_names=cont_names,\n                       ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "to = TabularPandas(df_clean, procs=procs, cat_names=cat_names, cont_names=cont_names,\n",
    "                  y_names='label', y_block =CategoryBlock ,splits=splits)\n",
    "\n",
    "to.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d23bd16",
   "metadata": {
    "input_tokens": 12,
    "output_tokens": 304,
    "solveit_ai": true
   },
   "source": [
    "please help me fix the above err\n",
    "\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "It looks like you're getting a `KeyError: 'label'` because the 'label' column doesn't exist in your dataframe. You need to add the label data from `df_label` to your `df_clean` dataframe before creating the TabularPandas object.\n",
    "\n",
    "Let's fix this:\n",
    "\n",
    "```python\n",
    "# Combine the cleaned data with the labels\n",
    "df_clean['label'] = df_label['Label'].values\n",
    "\n",
    "# Now create the TabularPandas object\n",
    "procs = [FillMissing, Categorify, Normalize]\n",
    "splits = RandomSplitter(valid_pct=0.2)(range_of(df_raw))\n",
    "\n",
    "to = TabularPandas(df_clean, procs=procs, cat_names=cat_names, cont_names=cont_names,\n",
    "                  y_names='label', y_block=CategoryBlock, splits=splits)\n",
    "\n",
    "# Now try showing the data\n",
    "to.show(3)\n",
    "```\n",
    "\n",
    "This should resolve the error by adding the 'label' column to your dataframe before creating the TabularPandas object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a0ae59",
   "metadata": {
    "input_tokens": 210,
    "output_tokens": 190,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Combine the cleaned data with the labels\u001b[39;00m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdf_clean\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m] = df_label[\u001b[33m'\u001b[39m\u001b[33mLabel\u001b[39m\u001b[33m'\u001b[39m].values\n",
      "\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Now create the TabularPandas object\u001b[39;00m\n",
      "\u001b[32m      5\u001b[39m procs = [FillMissing, Categorify, Normalize]\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-4d34bb49ff2a>\", line 2, in <module>\n    df_clean['label'] = df_label['Label'].values\n    ^^^^^^^^\n",
      "NameError: name 'df_clean' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Combine the cleaned data with the labels\n",
    "df_clean['label'] = df_label['Label'].values\n",
    "\n",
    "# Now create the TabularPandas object\n",
    "procs = [FillMissing, Categorify, Normalize]\n",
    "splits = RandomSplitter(valid_pct=0.2)(range_of(df_raw))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Then run your TabularPandas code\n",
    "to = TabularPandas(df_clean, procs=procs, cat_names=cat_names, cont_names=cont_names,\n",
    "                  y_names='label', y_block=CategoryBlock, splits=splits)\n",
    "\n",
    "\n",
    "# Now try showing the data\n",
    "to.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc19d95",
   "metadata": {
    "input_tokens": 7,
    "output_tokens": 93,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mto\u001b[49m.show(\u001b[32m20\u001b[39m)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'to' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'to' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-62fe5aa7d322>\", line 1, in <module>\n    to.show(20)\n    ^^\n",
      "NameError: name 'to' is not defined\n"
     ]
    }
   ],
   "source": [
    "to.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff817353",
   "metadata": {
    "input_tokens": 18,
    "output_tokens": 103,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dls = \u001b[43mto\u001b[49m.dataloaders(bs=\u001b[32m64\u001b[39m)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'to' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'to' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-6349848fd9b9>\", line 1, in <module>\n    dls = to.dataloaders(bs=64)\n          ^^\n",
      "NameError: name 'to' is not defined\n"
     ]
    }
   ],
   "source": [
    "dls = to.dataloaders(bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1307820f",
   "metadata": {
    "input_tokens": 13,
    "output_tokens": 100,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdls\u001b[49m.show_batch(max_n=\u001b[32m3\u001b[39m)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'dls' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dls' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-be124e3ccf48>\", line 1, in <module>\n    dls.show_batch(max_n=3)\n    ^^^\n",
      "NameError: name 'dls' is not defined\n"
     ]
    }
   ],
   "source": [
    "dls.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49409a14",
   "metadata": {
    "input_tokens": 73,
    "output_tokens": 165,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfastai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy, F1Score, RocAuc,Precision,RocAucBinary\n",
      "\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m learn = tabular_learner(\u001b[43mdls\u001b[49m, metrics=[accuracy, F1Score(),Precision(),RocAucBinary()])\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'dls' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dls' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-21eefb4e378c>\", line 3, in <module>\n    learn = tabular_learner(dls, metrics=[accuracy, F1Score(),Precision(),RocAucBinary()])\n                            ^^^\n",
      "NameError: name 'dls' is not defined\n"
     ]
    }
   ],
   "source": [
    "from fastai.metrics import accuracy, F1Score, RocAuc,Precision,RocAucBinary\n",
    "\n",
    "learn = tabular_learner(dls, metrics=[accuracy, F1Score(),Precision(),RocAucBinary()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec75af4",
   "metadata": {
    "input_tokens": 19,
    "output_tokens": 105,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mlearn\u001b[49m.fit_one_cycle(\u001b[32m1\u001b[39m, \u001b[32m1e-2\u001b[39m)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'learn' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'learn' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-3ea49add0339>\", line 1, in <module>\n    learn.fit_one_cycle(1, 1e-2)\n    ^^^^^\n",
      "NameError: name 'learn' is not defined\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f773d8f4",
   "metadata": {
    "input_tokens": 27,
    "output_tokens": 511,
    "solveit_ai": true
   },
   "source": [
    "Can you analyze the metric and give me the result? I think we're doing very badly.\n",
    "\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Looking at the metrics from your model training, you're absolutely right - the model is performing poorly, especially considering this appears to be an imbalanced classification problem.\n",
    "\n",
    "Here's the analysis of your metrics:\n",
    "\n",
    "1. **Accuracy (0.929)**: This looks good at first glance, but it's misleading. With imbalanced data (your label distribution showed approximately 93% negative class and 7% positive class), a model can achieve high accuracy by simply predicting the majority class for everything.\n",
    "\n",
    "2. **F1 Score (0.011)**: This is extremely low. F1 score is the harmonic mean of precision and recall, and a value this close to zero indicates the model is failing to identify the positive class effectively.\n",
    "\n",
    "3. **Precision (0.095)**: Very poor. This means that when your model predicts the positive class, it's correct only about 9.5% of the time - most predictions are false positives.\n",
    "\n",
    "4. **ROC AUC (0.707)**: This is mediocre. While better than random (0.5), it's not strong enough for a reliable model, especially with such imbalanced data.\n",
    "\n",
    "The model is likely predicting almost everything as the majority class (-1), which gives high accuracy but fails at actually detecting the minority class (1). \n",
    "\n",
    "To improve this, you should consider:\n",
    "\n",
    "1. Using class weights or oversampling techniques to address the imbalance\n",
    "2. Using metrics better suited for imbalanced data (like precision-recall AUC)\n",
    "3. Trying different learning rates and training for more epochs\n",
    "4. Adding regularization to prevent overfitting to the majority class\n",
    "\n",
    "Would you like me to suggest specific code changes to implement any of these improvements?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a36a82",
   "metadata": {
    "input_tokens": 55,
    "output_tokens": 282,
    "solveit_ai": true
   },
   "source": [
    "\n",
    "\n",
    "let try with tthee two first\n",
    "\n",
    "Using class weights or oversampling techniques to address the imbalance\n",
    "Using metrics better suited for imbalanced data (like precision-recall AUC\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Let's implement these two improvements. First, let's use class weights to address the imbalance, and then add precision-recall AUC as a metric:\n",
    "\n",
    "```python\n",
    "# Calculate class weights based on the label distribution\n",
    "weights = torch.FloatTensor([1, 12]) # Approximate weights based on ~93% vs ~7% distribution\n",
    "\n",
    "# Create a new learner with class weights and PR-AUC metric\n",
    "from fastai.metrics import PrecisionRecallScore\n",
    "\n",
    "learn = tabular_learner(dls, metrics=[accuracy, F1Score(), Precision(), RocAucBinary(), PrecisionRecallScore()], \n",
    "                        loss_func=CrossEntropyLossFlat(weight=weights))\n",
    "\n",
    "# Train the model with the same parameters\n",
    "learn.fit_one_cycle(1, 1e-2)\n",
    "```\n",
    "\n",
    "This should help the model focus more on the minority class and give us a better evaluation metric (PR-AUC) that's more suitable for imbalanced classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c322c8c",
   "metadata": {
    "input_tokens": 169,
    "output_tokens": 306,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n",
      "\u001b[32m      1\u001b[39m weights = torch.FloatTensor([\u001b[32m1\u001b[39m, \u001b[32m12\u001b[39m]) \u001b[38;5;66;03m# Approximate weights based on ~93% vs ~7% distribution\u001b[39;00m\n",
      "\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Create a new learner with class weights and PR-AUC metric\u001b[39;00m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfastai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PrecisionRecallScore\n",
      "\u001b[32m      6\u001b[39m learn = tabular_learner(dls, metrics=[accuracy, F1Score(), Precision(), RocAucBinary(), PrecisionRecallScore()], \n",
      "\u001b[32m      7\u001b[39m                         loss_func=CrossEntropyLossFlat(weight=weights))\n",
      "\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Train the model with the same parameters\u001b[39;00m\n",
      "\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'PrecisionRecallScore' from 'fastai.metrics' (/app/data/.local/lib/python3.11/site-packages/fastai/metrics.py)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PrecisionRecallScore' from 'fastai.metrics' (/app/data/.local/lib/python3.11/site-packages/fastai/metrics.py)",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-0ce97ca43348>\", line 4, in <module>\n    from fastai.metrics import PrecisionRecallScore\n",
      "ImportError: cannot import name 'PrecisionRecallScore' from 'fastai.metrics' (/app/data/.local/lib/python3.11/site-packages/fastai/metrics.py)\n"
     ]
    }
   ],
   "source": [
    "weights = torch.FloatTensor([1, 12]) # Approximate weights based on ~93% vs ~7% distribution\n",
    "\n",
    "# Create a new learner with class weights and PR-AUC metric\n",
    "from fastai.metrics import PrecisionRecallScore\n",
    "\n",
    "learn = tabular_learner(dls, metrics=[accuracy, F1Score(), Precision(), RocAucBinary(), PrecisionRecallScore()], \n",
    "                        loss_func=CrossEntropyLossFlat(weight=weights))\n",
    "\n",
    "# Train the model with the same parameters\n",
    "learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfbc501",
   "metadata": {
    "input_tokens": 15,
    "output_tokens": 448,
    "solveit_ai": true
   },
   "source": [
    "pleas ehelp em wuth anove err\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "It seems PrecisionRecallScore isn't directly available in fastai's metrics. Let's create our own precision-recall AUC metric and try again with the class weights:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from fastai.metrics import Metric\n",
    "\n",
    "class PRAUCScore(Metric):\n",
    "    \"Precision-Recall AUC\"\n",
    "    def __init__(self): self.name = \"prauc\"\n",
    "    \n",
    "    def reset(self): self.preds, self.targs = [], []\n",
    "    \n",
    "    def accumulate(self, learn):\n",
    "        pred = learn.pred[:,1]\n",
    "        self.preds.append(pred)\n",
    "        self.targs.append(learn.y)\n",
    "    \n",
    "    def value(self):\n",
    "        preds = torch.cat(self.preds).cpu().numpy()\n",
    "        targs = torch.cat(self.targs).cpu().numpy()\n",
    "        precision, recall, _ = precision_recall_curve(targs, preds)\n",
    "        return auc(recall, precision)\n",
    "\n",
    "# Now let's create the learner with class weights and our custom PR-AUC metric\n",
    "weights = torch.FloatTensor([1, 12])\n",
    "learn = tabular_learner(dls, metrics=[accuracy, F1Score(), Precision(), RocAucBinary(), PRAUCScore()], \n",
    "                      loss_func=CrossEntropyLossFlat(weight=weights))\n",
    "\n",
    "# Train the model with the same parameters\n",
    "learn.fit_one_cycle(1, 1e-2)\n",
    "```\n",
    "\n",
    "This should help address the class imbalance with weights and give us a better metric for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3c18c1",
   "metadata": {
    "input_tokens": 366,
    "output_tokens": 253,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 23\u001b[39m\n",
      "\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Now let's create the learner with class weights and our custom PR-AUC metric\u001b[39;00m\n",
      "\u001b[32m     22\u001b[39m weights = torch.FloatTensor([\u001b[32m1\u001b[39m, \u001b[32m12\u001b[39m])\n",
      "\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m learn = tabular_learner(\u001b[43mdls\u001b[49m, metrics=[accuracy, F1Score(), Precision(), RocAucBinary(), PRAUCScore()], \n",
      "\u001b[32m     24\u001b[39m                       loss_func=CrossEntropyLossFlat(weight=weights))\n",
      "\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Train the model with the same parameters\u001b[39;00m\n",
      "\u001b[32m     27\u001b[39m learn.fit_one_cycle(\u001b[32m1\u001b[39m, \u001b[32m1e-2\u001b[39m)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'dls' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dls' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-2f81d4e17793>\", line 23, in <module>\n    learn = tabular_learner(dls, metrics=[accuracy, F1Score(), Precision(), RocAucBinary(), PRAUCScore()],\n                           ^^^\n",
      "NameError: name 'dls' is not defined\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from fastai.metrics import Metric\n",
    "\n",
    "class PRAUCScore(Metric):\n",
    "    \"Precision-Recall AUC\"\n",
    "    def __init__(self): self.name = \"prauc\"\n",
    "    \n",
    "    def reset(self): self.preds, self.targs = [], []\n",
    "    \n",
    "    def accumulate(self, learn):\n",
    "        pred = learn.pred[:,1]\n",
    "        self.preds.append(pred)\n",
    "        self.targs.append(learn.y)\n",
    "    \n",
    "    def value(self):\n",
    "        preds = torch.cat(self.preds).cpu().numpy()\n",
    "        targs = torch.cat(self.targs).cpu().numpy()\n",
    "        precision, recall, _ = precision_recall_curve(targs, preds)\n",
    "        return auc(recall, precision)\n",
    "\n",
    "# Now let's create the learner with class weights and our custom PR-AUC metric\n",
    "weights = torch.FloatTensor([1, 12])\n",
    "learn = tabular_learner(dls, metrics=[accuracy, F1Score(), Precision(), RocAucBinary(), PRAUCScore()], \n",
    "                      loss_func=CrossEntropyLossFlat(weight=weights))\n",
    "\n",
    "# Train the model with the same parameters\n",
    "learn.fit_one_cycle(1, 1e-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08b9ab5",
   "metadata": {
    "input_tokens": 12,
    "output_tokens": 451,
    "solveit_ai": true
   },
   "source": [
    "please hlp me with above err..\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "It looks like there's an issue with the `name` attribute in our custom metric. In fastai, `name` is a property, not a regular attribute. Let's fix that and try again:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from fastai.metrics import Metric\n",
    "\n",
    "class PRAUCScore(Metric):\n",
    "    \"Precision-Recall AUC\"\n",
    "    @property\n",
    "    def name(self): return \"prauc\"\n",
    "    \n",
    "    def reset(self): self.preds, self.targs = [], []\n",
    "    \n",
    "    def accumulate(self, learn):\n",
    "        pred = learn.pred[:,1]\n",
    "        self.preds.append(pred)\n",
    "        self.targs.append(learn.y)\n",
    "    \n",
    "    def value(self):\n",
    "        preds = torch.cat(self.preds).cpu().numpy()\n",
    "        targs = torch.cat(self.targs).cpu().numpy()\n",
    "        precision, recall, _ = precision_recall_curve(targs, preds)\n",
    "        return auc(recall, precision)\n",
    "\n",
    "# Create the learner with class weights and our fixed PR-AUC metric\n",
    "weights = torch.FloatTensor([1, 12])\n",
    "learn = tabular_learner(dls, metrics=[accuracy, F1Score(), Precision(), RocAucBinary(), PRAUCScore()], \n",
    "                      loss_func=CrossEntropyLossFlat(weight=weights))\n",
    "\n",
    "# Train the model\n",
    "learn.fit_one_cycle(1, 1e-2)\n",
    "```\n",
    "\n",
    "This should properly define the custom metric and apply the class weights to help with the imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a030e58",
   "metadata": {
    "input_tokens": 357,
    "output_tokens": 238,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 24\u001b[39m\n",
      "\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Create the learner with class weights and our fixed PR-AUC metric\u001b[39;00m\n",
      "\u001b[32m     23\u001b[39m weights = torch.FloatTensor([\u001b[32m1\u001b[39m, \u001b[32m12\u001b[39m])\n",
      "\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m learn = tabular_learner(\u001b[43mdls\u001b[49m, metrics=[accuracy, F1Score(), Precision(), RocAucBinary(), PRAUCScore()], \n",
      "\u001b[32m     25\u001b[39m                       loss_func=CrossEntropyLossFlat(weight=weights))\n",
      "\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n",
      "\u001b[32m     28\u001b[39m learn.fit_one_cycle(\u001b[32m1\u001b[39m, \u001b[32m1e-2\u001b[39m)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'dls' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dls' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-be48a0fbb294>\", line 24, in <module>\n    learn = tabular_learner(dls, metrics=[accuracy, F1Score(), Precision(), RocAucBinary(), PRAUCScore()],\n                           ^^^\n",
      "NameError: name 'dls' is not defined\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from fastai.metrics import Metric\n",
    "\n",
    "class PRAUCScore(Metric):\n",
    "    \"Precision-Recall AUC\"\n",
    "    @property\n",
    "    def name(self): return \"prauc\"\n",
    "    \n",
    "    def reset(self): self.preds, self.targs = [], []\n",
    "    \n",
    "    def accumulate(self, learn):\n",
    "        pred = learn.pred[:,1]\n",
    "        self.preds.append(pred)\n",
    "        self.targs.append(learn.y)\n",
    "    \n",
    "    def value(self):\n",
    "        preds = torch.cat(self.preds).cpu().numpy()\n",
    "        targs = torch.cat(self.targs).cpu().numpy()\n",
    "        precision, recall, _ = precision_recall_curve(targs, preds)\n",
    "        return auc(recall, precision)\n",
    "\n",
    "# Create the learner with class weights and our fixed PR-AUC metric\n",
    "weights = torch.FloatTensor([1, 12])\n",
    "learn = tabular_learner(dls, metrics=[accuracy, F1Score(), Precision(), RocAucBinary(), PRAUCScore()], \n",
    "                      loss_func=CrossEntropyLossFlat(weight=weights))\n",
    "\n",
    "# Train the model\n",
    "learn.fit_one_cycle(1, 1e-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3bea8f",
   "metadata": {
    "input_tokens": 42,
    "output_tokens": 912,
    "solveit_ai": true
   },
   "source": [
    "Could you analyze our metrics and give me your commentary? Is it good or bad? If it's bad, what can we do about it?\n",
    "\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Looking at your metrics after applying class weights, we've seen some improvement but the model is still struggling with the imbalanced classification task:\n",
    "\n",
    "1. **Accuracy: 0.740** - This has decreased from the previous 0.929, which is actually a good sign! The model is no longer just predicting the majority class for everything.\n",
    "\n",
    "2. **F1 Score: 0.214** - This has improved significantly from 0.011, showing better balance between precision and recall, but it's still quite low.\n",
    "\n",
    "3. **Precision: 0.134** - Slightly improved from 0.095, but still poor. Only about 13% of positive predictions are correct.\n",
    "\n",
    "4. **ROC AUC: 0.705** - Essentially unchanged, which is still mediocre.\n",
    "\n",
    "5. **PR-AUC**: Unfortunately there's an issue with the display of this metric, but it would have given us a better view of performance on the minority class.\n",
    "\n",
    "**Assessment**: The model is still performing poorly, but the class weights have helped somewhat by forcing the model to pay more attention to the minority class.\n",
    "\n",
    "**What we can do next:**\n",
    "\n",
    "1. **Train for more epochs** - One epoch isn't enough for the model to learn properly:\n",
    "   ```python\n",
    "   learn.fit_one_cycle(5, 1e-2)\n",
    "   ```\n",
    "\n",
    "2. **Try different weight ratios** - Our current 1:12 ratio might need adjustment:\n",
    "   ```python\n",
    "   weights = torch.FloatTensor([1, 15])  # Try increasing minority class weight\n",
    "   learn = tabular_learner(dls, metrics=[accuracy, F1Score(), Precision(), RocAucBinary()], \n",
    "                         loss_func=CrossEntropyLossFlat(weight=weights))\n",
    "   ```\n",
    "\n",
    "3. **Use oversampling** - We can create a new dataloader with oversampling:\n",
    "   ```python\n",
    "   from fastai.callback.data import BalancedSampler\n",
    "   \n",
    "   dls_balanced = dls.new(after_batch=[BalancedSampler()])\n",
    "   learn = tabular_learner(dls_balanced, metrics=[accuracy, F1Score(), Precision(), RocAucBinary()])\n",
    "   ```\n",
    "\n",
    "4. **Fix the PR-AUC metric** to properly display:\n",
    "   ```python\n",
    "   class PRAUCScore(Metric):\n",
    "       @property\n",
    "       def name(self): return \"prauc\"\n",
    "       \n",
    "       def reset(self): self.preds, self.targs = [], []\n",
    "       \n",
    "       def accumulate(self, learn):\n",
    "           pred = learn.pred[:,1]\n",
    "           self.preds.append(pred)\n",
    "           self.targs.append(learn.y)\n",
    "       \n",
    "       def value(self):\n",
    "           preds = torch.cat(self.preds).cpu().numpy()\n",
    "           targs = torch.cat(self.targs).cpu().numpy()\n",
    "           precision, recall, _ = precision_recall_curve(targs, preds)\n",
    "           return torch.tensor(auc(recall, precision))\n",
    "   ```\n",
    "\n",
    "I'd recommend trying the oversampling approach with more epochs as the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8752bdec",
   "metadata": {
    "input_tokens": 189,
    "time_run": "8:35:59a"
   },
   "outputs": [],
   "source": [
    "class PRAUCScore(Metric):\n",
    "    @property\n",
    "    def name(self): return \"prauc\"\n",
    "\n",
    "    def reset(self): self.preds, self.targs = [], []\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        pred = learn.pred[:,1]\n",
    "        self.preds.append(pred)\n",
    "        self.targs.append(learn.y)\n",
    "\n",
    "    def value(self):\n",
    "        preds = torch.cat(self.preds).cpu().numpy()\n",
    "        targs = torch.cat(self.targs).cpu().numpy()\n",
    "        precision, recall, _ = precision_recall_curve(targs, preds)\n",
    "        return torch.tensor(auc(recall, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3af11f",
   "metadata": {
    "input_tokens": 93,
    "output_tokens": 189,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n",
      "\u001b[32m      1\u001b[39m weights = torch.FloatTensor([\u001b[32m1\u001b[39m, \u001b[32m15\u001b[39m])  \u001b[38;5;66;03m# Try increasing minority class weight\u001b[39;00m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m learn = tabular_learner(\u001b[43mdls\u001b[49m, metrics=[accuracy, F1Score(), Precision(), RocAucBinary(),PRAUCScore()], \n",
      "\u001b[32m      3\u001b[39m                       loss_func=CrossEntropyLossFlat(weight=weights))\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'dls' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dls' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-6e90260ad7f2>\", line 2, in <module>\n    learn = tabular_learner(dls, metrics=[accuracy, F1Score(), Precision(), RocAucBinary(),PRAUCScore()],\n                           ^^^\n",
      "NameError: name 'dls' is not defined\n"
     ]
    }
   ],
   "source": [
    "weights = torch.FloatTensor([1, 15])  # Try increasing minority class weight\n",
    "learn = tabular_learner(dls, metrics=[accuracy, F1Score(), Precision(), RocAucBinary(),PRAUCScore()], \n",
    "                      loss_func=CrossEntropyLossFlat(weight=weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607c9e4e",
   "metadata": {
    "input_tokens": 19,
    "output_tokens": 105,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mlearn\u001b[49m.fit_one_cycle(\u001b[32m5\u001b[39m, \u001b[32m1e-2\u001b[39m)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'learn' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'learn' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-24cb127eeafb>\", line 1, in <module>\n    learn.fit_one_cycle(5, 1e-2)\n    ^^^^^\n",
      "NameError: name 'learn' is not defined\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(5, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f71e549",
   "metadata": {
    "input_tokens": 46,
    "output_tokens": 933,
    "solveit_ai": true
   },
   "source": [
    "Can you analyze these metrics and give me your observations? Am I still doing better? I think there's something still wrong with this precision-recall curve metric.\n",
    "\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Looking at the metrics from your 5-epoch training with increased class weights (1:15), there's definitely improvement, but some issues remain:\n",
    "\n",
    "### Metric Analysis\n",
    "\n",
    "1. **Accuracy**: Improved from 0.506 (epoch 1) to 0.658 (epoch 5). This is more balanced than the initial 0.93, suggesting the model is no longer just predicting the majority class.\n",
    "\n",
    "2. **F1 Score**: Improved from 0.128 to 0.203. Still low, but better than our initial 0.011.\n",
    "\n",
    "3. **Precision**: Increased from 0.072 to 0.120. Still low - only about 12% of positive predictions are correct.\n",
    "\n",
    "4. **ROC AUC**: Improved from 0.551 to 0.715. This is a moderate improvement and suggests better discrimination ability.\n",
    "\n",
    "5. **PR-AUC**: Unfortunately, this metric is still showing as a method reference rather than a value. The issue is that we need to return a tensor rather than a numpy value.\n",
    "\n",
    "### Observations\n",
    "\n",
    "1. **Training Progress**: The metrics improved over epochs, suggesting the model is learning, but we're still seeing relatively low precision and F1 scores.\n",
    "\n",
    "2. **Class Imbalance**: The increased class weights (1:15) have helped, but the model still struggles with the minority class.\n",
    "\n",
    "3. **Best Epoch**: Epoch 3 had the highest accuracy (0.72) but lower F1 and precision than epoch 5, showing the trade-off between overall accuracy and minority class detection.\n",
    "\n",
    "4. **PR-AUC Issue**: The PR-AUC metric is still not displaying properly. We need to fix the return value to be a tensor.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Let's fix the PR-AUC metric:\n",
    "\n",
    "```python\n",
    "class PRAUCScore(Metric):\n",
    "    @property\n",
    "    def name(self): return \"prauc\"\n",
    "    \n",
    "    def reset(self): self.preds, self.targs = [], []\n",
    "    \n",
    "    def accumulate(self, learn):\n",
    "        pred = learn.pred[:,1]\n",
    "        self.preds.append(pred)\n",
    "        self.targs.append(learn.y)\n",
    "    \n",
    "    def value(self):\n",
    "        preds = torch.cat(self.preds).cpu().numpy()\n",
    "        targs = torch.cat(self.targs).cpu().numpy()\n",
    "        precision, recall, _ = precision_recall_curve(targs, preds)\n",
    "        return torch.tensor(auc(recall, precision), dtype=torch.float32)\n",
    "```\n",
    "\n",
    "2. Consider trying balanced sampling instead of class weights:\n",
    "\n",
    "```python\n",
    "from fastai.callback.data import BalancedSampler\n",
    "\n",
    "dls_balanced = dls.new(after_batch=[BalancedSampler()])\n",
    "learn = tabular_learner(dls_balanced, metrics=[accuracy, F1Score(), Precision(), RocAucBinary(), PRAUCScore()])\n",
    "```\n",
    "\n",
    "The model is doing better than before, but precision and F1 scores are still quite low, indicating that while we've improved from the initial state, the model still struggles with correctly identifying the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c1b89",
   "metadata": {
    "input_tokens": 82,
    "output_tokens": 226,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfastai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallback\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BalancedSampler\n",
      "\u001b[32m      3\u001b[39m dls_balanced = dls.new(after_batch=[BalancedSampler()])\n",
      "\u001b[32m      4\u001b[39m learn = tabular_learner(dls_balanced, metrics=[accuracy, F1Score(), Precision(), RocAucBinary(), PRAUCScore()])\n",
      "\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'BalancedSampler' from 'fastai.callback.data' (/app/data/.local/lib/python3.11/site-packages/fastai/callback/data.py)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BalancedSampler' from 'fastai.callback.data' (/app/data/.local/lib/python3.11/site-packages/fastai/callback/data.py)",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-56b4c84c6746>\", line 1, in <module>\n    from fastai.callback.data import BalancedSampler\n",
      "ImportError: cannot import name 'BalancedSampler' from 'fastai.callback.data' (/app/data/.local/lib/python3.11/site-packages/fastai/callback/data.py)\n"
     ]
    }
   ],
   "source": [
    "from fastai.callback.data import BalancedSampler\n",
    "\n",
    "dls_balanced = dls.new(after_batch=[BalancedSampler()])\n",
    "learn = tabular_learner(dls_balanced, metrics=[accuracy, F1Score(), Precision(), RocAucBinary(), PRAUCScore()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6887d6",
   "metadata": {
    "input_tokens": 7,
    "output_tokens": 816,
    "solveit_ai": true
   },
   "source": [
    "help me fix the err\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "It seems the `BalancedSampler` isn't available in your version of fastai. Let's implement our own balanced sampler to oversample the minority class:\n",
    "\n",
    "```python\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "# Fix the PR-AUC metric first\n",
    "class PRAUCScore(Metric):\n",
    "    @property\n",
    "    def name(self): return \"prauc\"\n",
    "    \n",
    "    def reset(self): self.preds, self.targs = [], []\n",
    "    \n",
    "    def accumulate(self, learn):\n",
    "        pred = learn.pred[:,1]\n",
    "        self.preds.append(pred)\n",
    "        self.targs.append(learn.y)\n",
    "    \n",
    "    def value(self):\n",
    "        preds = torch.cat(self.preds).cpu().numpy()\n",
    "        targs = torch.cat(self.targs).cpu().numpy()\n",
    "        precision, recall, _ = precision_recall_curve(targs, preds)\n",
    "        return torch.tensor(auc(recall, precision), dtype=torch.float32)\n",
    "\n",
    "# Create a balanced dataloader\n",
    "def create_balanced_dataloader(dls):\n",
    "    # Get training indices\n",
    "    train_idx = dls.train_ds.indices\n",
    "    \n",
    "    # Get labels for training data\n",
    "    train_labels = [dls.train_ds.dataset.y[i] for i in train_idx]\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    \n",
    "    # Calculate class weights (inverse of class frequency)\n",
    "    class_counts = torch.bincount(train_labels)\n",
    "    class_weights = 1. / class_counts.float()\n",
    "    \n",
    "    # Assign weight to each sample\n",
    "    weights = class_weights[train_labels]\n",
    "    \n",
    "    # Create sampler\n",
    "    sampler = WeightedRandomSampler(weights, len(weights))\n",
    "    \n",
    "    # Create new dataloader with sampler\n",
    "    new_dls = dls.new(after_item=dls.after_item, \n",
    "                      after_batch=dls.after_batch, \n",
    "                      train_bs=dls.train_bs,\n",
    "                      valid_bs=dls.valid_bs)\n",
    "    \n",
    "    # Replace the sampler in the training dataloader\n",
    "    new_dls.train.dl.sampler = sampler\n",
    "    \n",
    "    return new_dls\n",
    "\n",
    "# Try a simpler approach using class weights again but with a better metric\n",
    "weights = torch.FloatTensor([1, 20])  # Try an even higher weight for minority class\n",
    "learn = tabular_learner(dls, metrics=[accuracy, F1Score(), Precision(), RocAucBinary(), PRAUCScore()], \n",
    "                      loss_func=CrossEntropyLossFlat(weight=weights))\n",
    "\n",
    "# Train with the new weights\n",
    "learn.fit_one_cycle(5, 1e-2)\n",
    "```\n",
    "\n",
    "This will fix the PR-AUC metric and use an even higher class weight (1:20) to see if we can improve the model's performance on the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fa291a",
   "metadata": {
    "input_tokens": 712,
    "output_tokens": 258,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 53\u001b[39m\n",
      "\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Try a simpler approach using class weights again but with a better metric\u001b[39;00m\n",
      "\u001b[32m     52\u001b[39m weights = torch.FloatTensor([\u001b[32m1\u001b[39m, \u001b[32m20\u001b[39m])  \u001b[38;5;66;03m# Try an even higher weight for minority class\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m learn = tabular_learner(\u001b[43mdls\u001b[49m, metrics=[accuracy, F1Score(), Precision(), RocAucBinary(), PRAUCScore()], \n",
      "\u001b[32m     54\u001b[39m                       loss_func=CrossEntropyLossFlat(weight=weights))\n",
      "\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Train with the new weights\u001b[39;00m\n",
      "\u001b[32m     57\u001b[39m learn.fit_one_cycle(\u001b[32m5\u001b[39m, \u001b[32m1e-2\u001b[39m)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'dls' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dls' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-4cca68d25025>\", line 53, in <module>\n    learn = tabular_learner(dls, metrics=[accuracy, F1Score(), Precision(), RocAucBinary(), PRAUCScore()],\n                           ^^^\n",
      "NameError: name 'dls' is not defined\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "# Fix the PR-AUC metric first\n",
    "class PRAUCScore(Metric):\n",
    "    @property\n",
    "    def name(self): return \"prauc\"\n",
    "    \n",
    "    def reset(self): self.preds, self.targs = [], []\n",
    "    \n",
    "    def accumulate(self, learn):\n",
    "        pred = learn.pred[:,1]\n",
    "        self.preds.append(pred)\n",
    "        self.targs.append(learn.y)\n",
    "    \n",
    "    def value(self):\n",
    "        preds = torch.cat(self.preds).cpu().numpy()\n",
    "        targs = torch.cat(self.targs).cpu().numpy()\n",
    "        precision, recall, _ = precision_recall_curve(targs, preds)\n",
    "        return torch.tensor(auc(recall, precision), dtype=torch.float32)\n",
    "\n",
    "# Create a balanced dataloader\n",
    "def create_balanced_dataloader(dls):\n",
    "    # Get training indices\n",
    "    train_idx = dls.train_ds.indices\n",
    "    \n",
    "    # Get labels for training data\n",
    "    train_labels = [dls.train_ds.dataset.y[i] for i in train_idx]\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    \n",
    "    # Calculate class weights (inverse of class frequency)\n",
    "    class_counts = torch.bincount(train_labels)\n",
    "    class_weights = 1. / class_counts.float()\n",
    "    \n",
    "    # Assign weight to each sample\n",
    "    weights = class_weights[train_labels]\n",
    "    \n",
    "    # Create sampler\n",
    "    sampler = WeightedRandomSampler(weights, len(weights))\n",
    "    \n",
    "    # Create new dataloader with sampler\n",
    "    new_dls = dls.new(after_item=dls.after_item, \n",
    "                      after_batch=dls.after_batch, \n",
    "                      train_bs=dls.train_bs,\n",
    "                      valid_bs=dls.valid_bs)\n",
    "    \n",
    "    # Replace the sampler in the training dataloader\n",
    "    new_dls.train.dl.sampler = sampler\n",
    "    \n",
    "    return new_dls\n",
    "\n",
    "# Try a simpler approach using class weights again but with a better metric\n",
    "weights = torch.FloatTensor([1, 20])  # Try an even higher weight for minority class\n",
    "learn = tabular_learner(dls, metrics=[accuracy, F1Score(), Precision(), RocAucBinary(), PRAUCScore()], \n",
    "                      loss_func=CrossEntropyLossFlat(weight=weights))\n",
    "\n",
    "# Train with the new weights\n",
    "learn.fit_one_cycle(5, 1e-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae1657d",
   "metadata": {
    "input_tokens": 57,
    "output_tokens": 999,
    "solveit_ai": true
   },
   "source": [
    "Please analyze the matrix and give me a recommendation. For the precision-recall curve, I think we should use the native SKlearn, not the customized fastAI version. Please implement that.\n",
    "\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Looking at the metrics from your 5-epoch training with the higher class weights (1:20), here's my analysis:\n",
    "\n",
    "### Metrics Analysis\n",
    "\n",
    "1. **Accuracy**: Stabilized around 0.55, which is more balanced than the initial 0.93.\n",
    "2. **F1 Score**: Improved to ~0.186, better than our initial 0.011 but still low.\n",
    "3. **Precision**: Around 0.106, indicating only ~10.6% of positive predictions are correct.\n",
    "4. **ROC AUC**: Consistently around 0.70, which is moderate discrimination ability.\n",
    "5. **PR-AUC**: Still showing as a method reference rather than a value.\n",
    "\n",
    "### Observations\n",
    "\n",
    "The model is still struggling with the imbalanced classification task. Despite using higher class weights (1:20), the precision and F1 scores remain low, suggesting the model has difficulty correctly identifying the minority class.\n",
    "\n",
    "### Recommendation\n",
    "\n",
    "Let's implement a proper evaluation using scikit-learn's precision-recall curve and AUC calculation directly, rather than trying to integrate it into fastai's metrics system:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_recall_curve, auc, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train the model with the existing setup\n",
    "weights = torch.FloatTensor([1, 20])\n",
    "learn = tabular_learner(dls, metrics=[accuracy, F1Score(), Precision(), RocAucBinary()], \n",
    "                      loss_func=CrossEntropyLossFlat(weight=weights))\n",
    "learn.fit_one_cycle(5, 1e-2)\n",
    "\n",
    "# Get predictions on validation set\n",
    "preds, targets = learn.get_preds()\n",
    "probs = preds[:, 1].numpy()  # Probability of positive class\n",
    "actual = targets.numpy()\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(actual, probs)\n",
    "pr_auc = auc(recall, precision)\n",
    "ap = average_precision_score(actual, probs)\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.3f}, AP = {ap:.3f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
    "print(f\"Average Precision: {ap:.4f}\")\n",
    "\n",
    "# Get confusion matrix at default threshold (0.5)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "y_pred = (probs >= 0.5).astype(int)\n",
    "cm = confusion_matrix(actual, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(actual, y_pred))\n",
    "```\n",
    "\n",
    "This will give you a more comprehensive evaluation of the model's performance on the imbalanced dataset, including the precision-recall curve, average precision, confusion matrix, and a detailed classification report.\n",
    "\n",
    "For further improvement, you might want to try:\n",
    "1. Using SMOTE or other advanced resampling techniques\n",
    "2. Trying different architectures or preprocessing\n",
    "3. Using a different threshold (not necessarily 0.5) for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2b80f",
   "metadata": {
    "input_tokens": 517,
    "output_tokens": 226,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n",
      "\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Train the model with the existing setup\u001b[39;00m\n",
      "\u001b[32m      5\u001b[39m weights = torch.FloatTensor([\u001b[32m1\u001b[39m, \u001b[32m20\u001b[39m])\n",
      "\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m learn = tabular_learner(\u001b[43mdls\u001b[49m, metrics=[accuracy, F1Score(), Precision(), RocAucBinary()], \n",
      "\u001b[32m      7\u001b[39m                       loss_func=CrossEntropyLossFlat(weight=weights))\n",
      "\u001b[32m      8\u001b[39m learn.fit_one_cycle(\u001b[32m5\u001b[39m, \u001b[32m1e-2\u001b[39m)\n",
      "\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Get predictions on validation set\u001b[39;00m\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'dls' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dls' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-55f4f1d48a67>\", line 6, in <module>\n    learn = tabular_learner(dls, metrics=[accuracy, F1Score(), Precision(), RocAucBinary()],\n                           ^^^\n",
      "NameError: name 'dls' is not defined\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train the model with the existing setup\n",
    "weights = torch.FloatTensor([1, 20])\n",
    "learn = tabular_learner(dls, metrics=[accuracy, F1Score(), Precision(), RocAucBinary()], \n",
    "                      loss_func=CrossEntropyLossFlat(weight=weights))\n",
    "learn.fit_one_cycle(5, 1e-2)\n",
    "\n",
    "# Get predictions on validation set\n",
    "preds, targets = learn.get_preds()\n",
    "probs = preds[:, 1].numpy()  # Probability of positive class\n",
    "actual = targets.numpy()\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(actual, probs)\n",
    "pr_auc = auc(recall, precision)\n",
    "ap = average_precision_score(actual, probs)\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.3f}, AP = {ap:.3f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
    "print(f\"Average Precision: {ap:.4f}\")\n",
    "\n",
    "# Get confusion matrix at default threshold (0.5)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "y_pred = (probs >= 0.5).astype(int)\n",
    "cm = confusion_matrix(actual, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(actual, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe51129",
   "metadata": {
    "input_tokens": 40,
    "output_tokens": 877,
    "solveit_ai": true
   },
   "source": [
    "Let's try to make sense of this matrix and compare it to the one we're pulling from sklean too. Add your observation as well.\n",
    "\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Looking at both the precision-recall curve and the confusion matrix, I can provide a comprehensive analysis of your model's performance:\n",
    "\n",
    "### Precision-Recall Curve Analysis\n",
    "\n",
    "The PR curve shows a PR-AUC of 0.168, which is quite low. In an imbalanced classification problem like yours, PR-AUC is a more informative metric than ROC-AUC because it focuses on the positive class performance.\n",
    "\n",
    "The curve drops steeply at the beginning and then gradually declines, indicating that:\n",
    "1. The model struggles to maintain precision as recall increases\n",
    "2. There's a significant trade-off between precision and recall\n",
    "3. The model has difficulty consistently identifying the positive class\n",
    "\n",
    "### Confusion Matrix Analysis\n",
    "\n",
    "```\n",
    "Confusion Matrix:\n",
    "[[5358 3968]\n",
    " [ 180  494]]\n",
    "```\n",
    "\n",
    "Breaking this down:\n",
    "- True Negatives: 5,358 (correctly identified negative cases)\n",
    "- False Positives: 3,968 (negative cases incorrectly classified as positive)\n",
    "- False Negatives: 180 (positive cases incorrectly classified as negative)\n",
    "- True Positives: 494 (correctly identified positive cases)\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **High False Positive Rate**: The model is generating a large number of false positives (3,968), which explains the low precision (0.11).\n",
    "\n",
    "2. **Good Recall but Poor Precision**: The model achieves a reasonable recall of 0.73 for the positive class, but at the cost of extremely low precision (0.11). This means it's finding most of the positive cases but also incorrectly labeling many negative cases as positive.\n",
    "\n",
    "3. **Class Imbalance Impact**: Despite using class weights (1:20), the model still struggles with the fundamental imbalance challenge.\n",
    "\n",
    "4. **Overall Accuracy is Misleading**: The 0.59 accuracy doesn't tell the full story given the imbalance.\n",
    "\n",
    "### Comparison with Previous Metrics\n",
    "\n",
    "The scikit-learn evaluation provides a clearer picture than the fastai metrics we were looking at before. The fastai metrics showed improving F1 scores (up to ~0.19) and precision (~0.11), which align with what we're seeing in the sklearn report.\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "1. **Adjust the Classification Threshold**: Instead of using the default 0.5 threshold, you could try different thresholds to better balance precision and recall.\n",
    "\n",
    "2. **Try Different Sampling Approaches**: Consider SMOTE or other advanced resampling techniques to better handle the imbalance.\n",
    "\n",
    "3. **Feature Engineering**: The high false positive rate suggests the model might benefit from more discriminative features.\n",
    "\n",
    "4. **Ensemble Methods**: Consider using ensemble methods specifically designed for imbalanced data, such as balanced random forests.\n",
    "\n",
    "5. **Different Evaluation Metric**: Focus on F1-score or PR-AUC as your primary optimization metric rather than accuracy.\n",
    "\n",
    "Would you like me to implement any of these recommendations next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1ffdbb",
   "metadata": {
    "input_tokens": 49,
    "output_tokens": 858,
    "solveit_ai": true
   },
   "source": [
    "let timpalement this Adjust the Classification Threshold: Instead of using the default 0.5 threshold, you could try different thresholds to better balance precision and recall.\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Let's implement threshold adjustment to find the optimal balance between precision and recall:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions on validation set again (if needed)\n",
    "preds, targets = learn.get_preds()\n",
    "probs = preds[:, 1].numpy()\n",
    "actual = targets.numpy()\n",
    "\n",
    "# Calculate precision, recall and thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(actual, probs)\n",
    "\n",
    "# Calculate F1 score for each threshold\n",
    "f1_scores = []\n",
    "for p, r in zip(precision[:-1], recall[:-1]):  # Last element has no threshold\n",
    "    f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Find threshold with best F1 score\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "best_f1 = f1_scores[best_idx]\n",
    "best_precision = precision[best_idx]\n",
    "best_recall = recall[best_idx]\n",
    "\n",
    "# Plot precision, recall, and F1 score vs threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, precision[:-1], 'b-', label='Precision')\n",
    "plt.plot(thresholds, recall[:-1], 'g-', label='Recall')\n",
    "plt.plot(thresholds, f1_scores, 'r-', label='F1 Score')\n",
    "plt.axvline(x=best_threshold, color='k', linestyle='--', \n",
    "           label=f'Best Threshold = {best_threshold:.3f}')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision, Recall, and F1 Score vs Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Apply the best threshold and calculate new confusion matrix\n",
    "y_pred_best = (probs >= best_threshold).astype(int)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm_best = confusion_matrix(actual, y_pred_best)\n",
    "print(f\"Best Threshold: {best_threshold:.4f}\")\n",
    "print(f\"Best F1 Score: {best_f1:.4f}\")\n",
    "print(f\"Precision at best threshold: {best_precision:.4f}\")\n",
    "print(f\"Recall at best threshold: {best_recall:.4f}\")\n",
    "print(\"\\nConfusion Matrix with Best Threshold:\")\n",
    "print(cm_best)\n",
    "print(\"\\nClassification Report with Best Threshold:\")\n",
    "print(classification_report(actual, y_pred_best))\n",
    "```\n",
    "\n",
    "This code will:\n",
    "1. Calculate precision, recall, and F1 scores across different thresholds\n",
    "2. Find the threshold that maximizes the F1 score\n",
    "3. Plot how precision, recall, and F1 score change with different thresholds\n",
    "4. Apply the optimal threshold and show the new confusion matrix and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3040dacb",
   "metadata": {
    "input_tokens": 733,
    "output_tokens": 160,
    "time_run": "8:35:59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n",
      "\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Get predictions on validation set again (if needed)\u001b[39;00m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m preds, targets = \u001b[43mlearn\u001b[49m.get_preds()\n",
      "\u001b[32m      7\u001b[39m probs = preds[:, \u001b[32m1\u001b[39m].numpy()\n",
      "\u001b[32m      8\u001b[39m actual = targets.numpy()\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'learn' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'learn' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-524115c51879>\", line 6, in <module>\n    preds, targets = learn.get_preds()\n                     ^^^^^\n",
      "NameError: name 'learn' is not defined\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions on validation set again (if needed)\n",
    "preds, targets = learn.get_preds()\n",
    "probs = preds[:, 1].numpy()\n",
    "actual = targets.numpy()\n",
    "\n",
    "# Calculate precision, recall and thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(actual, probs)\n",
    "\n",
    "# Calculate F1 score for each threshold\n",
    "f1_scores = []\n",
    "for p, r in zip(precision[:-1], recall[:-1]):  # Last element has no threshold\n",
    "    f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Find threshold with best F1 score\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "best_f1 = f1_scores[best_idx]\n",
    "best_precision = precision[best_idx]\n",
    "best_recall = recall[best_idx]\n",
    "\n",
    "# Plot precision, recall, and F1 score vs threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, precision[:-1], 'b-', label='Precision')\n",
    "plt.plot(thresholds, recall[:-1], 'g-', label='Recall')\n",
    "plt.plot(thresholds, f1_scores, 'r-', label='F1 Score')\n",
    "plt.axvline(x=best_threshold, color='k', linestyle='--', \n",
    "           label=f'Best Threshold = {best_threshold:.3f}')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision, Recall, and F1 Score vs Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Apply the best threshold and calculate new confusion matrix\n",
    "y_pred_best = (probs >= best_threshold).astype(int)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm_best = confusion_matrix(actual, y_pred_best)\n",
    "print(f\"Best Threshold: {best_threshold:.4f}\")\n",
    "print(f\"Best F1 Score: {best_f1:.4f}\")\n",
    "print(f\"Precision at best threshold: {best_precision:.4f}\")\n",
    "print(f\"Recall at best threshold: {best_recall:.4f}\")\n",
    "print(\"\\nConfusion Matrix with Best Threshold:\")\n",
    "print(cm_best)\n",
    "print(\"\\nClassification Report with Best Threshold:\")\n",
    "print(classification_report(actual, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceeb28c",
   "metadata": {
    "input_tokens": 16,
    "output_tokens": 910,
    "solveit_ai": true
   },
   "source": [
    "Can you analyze the matrix and give me your observations?\n",
    "\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Looking at the threshold optimization results and the new confusion matrix, I can provide the following analysis:\n",
    "\n",
    "### Threshold Optimization Analysis\n",
    "\n",
    "The plot shows how precision, recall, and F1 score vary with different thresholds. The optimal threshold was found to be 0.721, which maximizes the F1 score at 0.243.\n",
    "\n",
    "### Confusion Matrix Comparison\n",
    "\n",
    "**Original Confusion Matrix (0.5 threshold):**\n",
    "```\n",
    "[[5358 3968]\n",
    " [ 180  494]]\n",
    "```\n",
    "\n",
    "**New Confusion Matrix (0.721 threshold):**\n",
    "```\n",
    "[[8318 1008]\n",
    " [ 441  233]]\n",
    "```\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Significantly Reduced False Positives**: False positives decreased dramatically from 3,968 to 1,008. This is a 75% reduction, which means much fewer negative cases are being incorrectly classified as positive.\n",
    "\n",
    "2. **Higher Precision**: Precision improved from 0.11 to 0.19 (about 73% improvement). This means that when the model predicts the positive class, it's more likely to be correct.\n",
    "\n",
    "3. **Lower Recall**: Recall decreased from 0.73 to 0.35 (a 52% reduction). The model is now detecting fewer of the actual positive cases.\n",
    "\n",
    "4. **Better Overall Accuracy**: Accuracy improved from 0.59 to 0.86, which is a substantial improvement.\n",
    "\n",
    "5. **F1 Score Improvement**: The F1 score improved from 0.19 to 0.24, which represents a better balance between precision and recall.\n",
    "\n",
    "6. **Better Class Balance Handling**: The model is now more conservative in predicting the positive class, which is appropriate given the high imbalance.\n",
    "\n",
    "### Business Implications\n",
    "\n",
    "1. **Trade-off Analysis**: By increasing the threshold, we've made a conscious decision to trade recall for precision. This makes sense in scenarios where false positives are more costly than false negatives.\n",
    "\n",
    "2. **Higher Confidence Predictions**: The higher threshold means we're only predicting the positive class when the model is more confident, which could be valuable in decision-making contexts.\n",
    "\n",
    "3. **Improved Resource Allocation**: With fewer false positives, resources can be allocated more efficiently to investigate true positive cases.\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "1. **Consider the Business Context**: The optimal threshold depends on the specific business cost of false positives versus false negatives. If missing positive cases is very costly, you might want a lower threshold.\n",
    "\n",
    "2. **Explore Other Thresholds**: Different points on the precision-recall curve might be more appropriate depending on your specific needs.\n",
    "\n",
    "3. **Combine with Other Approaches**: While threshold optimization helps, combining it with better sampling techniques or model architectures could yield further improvements.\n",
    "\n",
    "4. **Feature Engineering**: The model still struggles with distinguishing between classes, suggesting that better features might help.\n",
    "\n",
    "Would you like to try another approach to further improve the model, such as implementing a sampling technique like SMOTE?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca9472d",
   "metadata": {
    "input_tokens": 79,
    "output_tokens": 463,
    "solveit_ai": true
   },
   "source": [
    "Given the experiments we've run, and the task expectations outlined earlier, can you summarize our progress with simple, non-technical commentary? I don't need a full doc format yet, just concise and straightforward observations that make sense in light of what we've done. Avoid jargon.\n",
    "\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "## Progress Summary\n",
    "\n",
    "We started with a model that was highly imbalanced - it was good at predicting the majority class but terrible at identifying the minority class. This is like having a security system that's great at letting authorized people in but also lets in many unauthorized people.\n",
    "\n",
    "Here's what we've learned:\n",
    "\n",
    "1. Our initial model was misleading - it showed high accuracy (93%) but was actually performing poorly at finding the important minority cases.\n",
    "\n",
    "2. We tried two main approaches to fix this:\n",
    "   - Giving more weight to the minority class during training\n",
    "   - Finding a better decision threshold instead of using the default 0.5\n",
    "\n",
    "3. The results improved significantly:\n",
    "   - We can now correctly identify 89% of the majority class\n",
    "   - We can identify 35% of the minority class (up from basically none)\n",
    "   - False alarms dropped dramatically - we're much less likely to misclassify majority cases\n",
    "\n",
    "4. The trade-off we made:\n",
    "   - We're more selective about what we call a \"positive\" case\n",
    "   - This means we miss more positive cases but have fewer false alarms\n",
    "   - In many real-world situations, this is preferable - quality over quantity\n",
    "\n",
    "5. Next steps could include:\n",
    "   - Better data balancing techniques\n",
    "   - Exploring different models\n",
    "   - Adding more informative features\n",
    "\n",
    "Overall, we've turned a misleading model into one that's much more useful and trustworthy, even though there's still room for improvement. The key insight was that a high accuracy number can hide poor performance on the cases we care most about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0388c1",
   "metadata": {
    "input_tokens": 463
   },
   "source": [
    "## Progress Summary\n",
    "\n",
    "We started with a model that was highly imbalanced - it was good at predicting the majority class but terrible at identifying the minority class. This is like having a security system that's great at letting authorized people in but also lets in many unauthorized people.\n",
    "\n",
    "Here's what we've learned:\n",
    "\n",
    "1. Our initial model was misleading - it showed high accuracy (93%) but was actually performing poorly at finding the important minority cases.\n",
    "\n",
    "2. We tried two main approaches to fix this:\n",
    "   - Giving more weight to the minority class during training\n",
    "   - Finding a better decision threshold instead of using the default 0.5\n",
    "\n",
    "3. The results improved significantly:\n",
    "   - We can now correctly identify 89% of the majority class\n",
    "   - We can identify 35% of the minority class (up from basically none)\n",
    "   - False alarms dropped dramatically - we're much less likely to misclassify majority cases\n",
    "\n",
    "4. The trade-off we made:\n",
    "   - We're more selective about what we call a \"positive\" case\n",
    "   - This means we miss more positive cases but have fewer false alarms\n",
    "   - In many real-world situations, this is preferable - quality over quantity\n",
    "\n",
    "5. Next steps could include:\n",
    "   - Better data balancing techniques\n",
    "   - Exploring different models\n",
    "   - Adding more informative features\n",
    "\n",
    "Overall, we've turned a misleading model into one that's much more useful and trustworthy, even though there's still room for improvement. The key insight was that a high accuracy number can hide poor performance on the cases we care most about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c82a0a",
   "metadata": {
    "input_tokens": 42,
    "output_tokens": 564,
    "solveit_ai": true
   },
   "source": [
    "Can you add some commentary on the metrics we've used? Keep it simple, no jargon, and be precise. Back it up with data.\n",
    "\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "## Metrics Commentary\n",
    "\n",
    "When evaluating our model, we used several different measurements. Here's what they told us:\n",
    "\n",
    "**Accuracy (86%)**: This tells us the overall percentage of correct predictions. While it improved from 59% to 86%, this number alone doesn't tell the full story with imbalanced data.\n",
    "\n",
    "**Precision (19%)**: This measures \"when we predict something is positive, how often are we right?\" Initially, only 11% of our positive predictions were correct. We improved this to 19%, meaning fewer false alarms.\n",
    "\n",
    "**Recall (35%)**: This shows \"what percentage of actual positive cases do we catch?\" Our original model caught 73% but with many false alarms. Our improved model catches 35% but with much higher confidence.\n",
    "\n",
    "**F1 Score (24%)**: This combines precision and recall into one number. It improved from 19% to 24%, showing better overall performance despite the recall trade-off.\n",
    "\n",
    "**Confusion Matrix Data**:\n",
    "- False positives dropped from 3,968 to 1,008 (75% reduction)\n",
    "- True negatives increased from 5,358 to 8,318 (55% increase)\n",
    "- True positives decreased from 494 to 233 (53% decrease)\n",
    "- False negatives increased from 180 to 441 (145% increase)\n",
    "\n",
    "These numbers confirm our trade-off: we're missing more positive cases but drastically reducing false alarms.\n",
    "\n",
    "**Threshold (0.72)**: By raising our decision threshold from 0.5 to 0.72, we're only calling something \"positive\" when the model is more confident. This is like raising the bar for evidence before making an important decision.\n",
    "\n",
    "The key takeaway: Different metrics tell different parts of the story. By looking at all of them together, we get a complete picture of how our model actually performs in real-world conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26601cb8",
   "metadata": {
    "input_tokens": 564
   },
   "source": [
    "## Metrics Commentary\n",
    "\n",
    "When evaluating our model, we used several different measurements. Here's what they told us:\n",
    "\n",
    "**Accuracy (86%)**: This tells us the overall percentage of correct predictions. While it improved from 59% to 86%, this number alone doesn't tell the full story with imbalanced data.\n",
    "\n",
    "**Precision (19%)**: This measures \"when we predict something is positive, how often are we right?\" Initially, only 11% of our positive predictions were correct. We improved this to 19%, meaning fewer false alarms.\n",
    "\n",
    "**Recall (35%)**: This shows \"what percentage of actual positive cases do we catch?\" Our original model caught 73% but with many false alarms. Our improved model catches 35% but with much higher confidence.\n",
    "\n",
    "**F1 Score (24%)**: This combines precision and recall into one number. It improved from 19% to 24%, showing better overall performance despite the recall trade-off.\n",
    "\n",
    "**Confusion Matrix Data**:\n",
    "- False positives dropped from 3,968 to 1,008 (75% reduction)\n",
    "- True negatives increased from 5,358 to 8,318 (55% increase)\n",
    "- True positives decreased from 494 to 233 (53% decrease)\n",
    "- False negatives increased from 180 to 441 (145% increase)\n",
    "\n",
    "These numbers confirm our trade-off: we're missing more positive cases but drastically reducing false alarms.\n",
    "\n",
    "**Threshold (0.72)**: By raising our decision threshold from 0.5 to 0.72, we're only calling something \"positive\" when the model is more confident. This is like raising the bar for evidence before making an important decision.\n",
    "\n",
    "The key takeaway: Different metrics tell different parts of the story. By looking at all of them together, we get a complete picture of how our model actually performs in real-world conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdd8c87",
   "metadata": {
    "input_tokens": 276,
    "output_tokens": 939,
    "solveit_ai": true
   },
   "source": [
    "Based on the Task & Expectations outlined, ptoved  the following comments provides a clear, data-driven assessment, addressing each question without overstating the conclusions.\n",
    "\n",
    "\n",
    "Task & Expectations:\n",
    "```\n",
    "Attached is a training dataset and corresponding label file for a classification problem.\n",
    "Your task is to propose a solution approach, documenting the steps, ideas, observations, data\n",
    "analysis, and feature engineering (if any). There’s no need to code the solution, but feel free to\n",
    "provide as much detail as you'd like.\n",
    "Please include the success criteria and performance metrics you would use for model\n",
    "evaluation, along with the reasoning behind your choices. All decisions should be supported by\n",
    "data observations or sound reasoning.\n",
    "We will assess the quality of your response based on the submitted document (Word or\n",
    "PowerPoint). Feel free to share other support documents like notebooks, excels etc. which can\n",
    "give us more confidence around the work done.\n",
    "```\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "# Classification Problem Solution Approach\n",
    "\n",
    "## 1. Data Understanding and Analysis\n",
    "\n",
    "Based on our exploratory analysis, we observed a significant class imbalance in the dataset, with approximately 93% majority class and 7% minority class samples. This imbalance presented our primary challenge, as standard modeling approaches tend to favor the majority class.\n",
    "\n",
    "Key observations:\n",
    "- The dataset contains a mix of categorical and numerical features\n",
    "- Several features contained missing values denoted as \"#na#\"\n",
    "- Features showed varying distributions across the target classes\n",
    "\n",
    "## 2. Solution Approach\n",
    "\n",
    "Our approach followed these steps:\n",
    "\n",
    "1. **Data Preprocessing**\n",
    "   - Handled missing values by creating indicator variables\n",
    "   - Encoded categorical variables appropriately\n",
    "   - Normalized numerical features to ensure consistent scale\n",
    "\n",
    "2. **Model Development**\n",
    "   - Used a tabular model with appropriate class weights to address imbalance\n",
    "   - Trained for multiple epochs to allow the model to learn complex patterns\n",
    "   - Experimented with different class weights to find optimal balance\n",
    "\n",
    "3. **Threshold Optimization**\n",
    "   - Identified that the default classification threshold (0.5) was suboptimal\n",
    "   - Used precision-recall analysis to determine the optimal threshold (0.72)\n",
    "   - Applied this threshold to maximize F1 score\n",
    "\n",
    "## 3. Performance Metrics and Evaluation\n",
    "\n",
    "We selected metrics specifically suited for imbalanced classification:\n",
    "\n",
    "**Primary Metrics:**\n",
    "- **F1 Score**: Improved from 0.19 to 0.24 with threshold optimization\n",
    "- **Precision**: Increased from 0.11 to 0.19, reducing false positives by 75%\n",
    "- **Recall**: Adjusted from 0.73 to 0.35, focusing on higher confidence predictions\n",
    "\n",
    "**Supporting Metrics:**\n",
    "- **Confusion Matrix**: Provided detailed breakdown of prediction types\n",
    "- **Precision-Recall Curve**: Helped identify optimal operating point (threshold)\n",
    "- **ROC-AUC**: Remained around 0.71, indicating reasonable discriminative ability\n",
    "\n",
    "## 4. Results and Trade-offs\n",
    "\n",
    "Our solution demonstrates a clear trade-off between precision and recall:\n",
    "- Initial model: High recall (0.73) but very low precision (0.11)\n",
    "- Optimized model: Moderate recall (0.35) with improved precision (0.19)\n",
    "\n",
    "This trade-off represents a business decision: we've chosen to reduce false alarms at the cost of missing some positive cases.\n",
    "\n",
    "## 5. Recommendations for Further Improvement\n",
    "\n",
    "Based on our findings, we recommend:\n",
    "\n",
    "1. **Advanced Sampling Techniques**: Implement SMOTE or similar approaches to create more balanced training data\n",
    "2. **Feature Engineering**: Develop more discriminative features based on domain knowledge\n",
    "3. **Model Exploration**: Test different model architectures beyond the current approach\n",
    "4. **Ensemble Methods**: Combine multiple models to improve overall performance\n",
    "5. **Cost-sensitive Learning**: Adjust the model based on the specific business costs of false positives vs. false negatives\n",
    "\n",
    "The success of this approach ultimately depends on the specific business context and the relative importance of precision versus recall in the application domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f1da0c",
   "metadata": {
    "input_tokens": 39,
    "output_tokens": 1015,
    "solveit_ai": true
   },
   "source": [
    "whats is Precision-Recall Curve help me understand it intuitively with simple example first and the explain the above metric score too...\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "# Understanding Precision-Recall Curves\n",
    "\n",
    "## Simple Intuitive Example\n",
    "\n",
    "Imagine you're a doctor screening patients for a rare disease:\n",
    "\n",
    "- **Precision** is: \"When I tell someone they have the disease, how often am I right?\"\n",
    "- **Recall** is: \"Out of all the people who actually have the disease, how many did I correctly identify?\"\n",
    "\n",
    "Let's say you have 100 patients:\n",
    "- 10 actually have the disease\n",
    "- 90 are healthy\n",
    "\n",
    "### Scenario 1: Very Strict Test (High Threshold)\n",
    "You only diagnose someone if you're very confident.\n",
    "- You identify 3 people as having the disease, and all 3 actually have it\n",
    "- You miss 7 people who have the disease\n",
    "- **Precision**: 3/3 = 100% (all your positive diagnoses were correct)\n",
    "- **Recall**: 3/10 = 30% (you only found 30% of the disease cases)\n",
    "\n",
    "### Scenario 2: Very Lenient Test (Low Threshold)\n",
    "You diagnose someone even with minimal suspicion.\n",
    "- You identify 20 people as having the disease\n",
    "- 8 of them actually have the disease, 12 are false alarms\n",
    "- **Precision**: 8/20 = 40% (only 40% of your positive diagnoses were correct)\n",
    "- **Recall**: 8/10 = 80% (you found 80% of the disease cases)\n",
    "\n",
    "## What is a Precision-Recall Curve?\n",
    "\n",
    "A precision-recall curve shows how precision and recall change as you adjust your threshold (how confident you need to be to make a positive prediction).\n",
    "\n",
    "- **Left side of curve**: High precision, low recall (strict threshold - few false positives but miss many positives)\n",
    "- **Right side of curve**: Low precision, high recall (lenient threshold - catch most positives but many false alarms)\n",
    "\n",
    "## Explaining Our Metrics\n",
    "\n",
    "In our case:\n",
    "\n",
    "- **PR-AUC (0.168)**: This is the area under the precision-recall curve. A perfect model would have a score of 1.0. Our score of 0.168 is relatively low, indicating the model struggles to balance precision and recall effectively.\n",
    "\n",
    "- **Best Threshold (0.72)**: We found that using a threshold of 0.72 (rather than the default 0.5) gives us the best F1 score. This is like saying \"only diagnose the disease if you're 72% confident\" instead of \"diagnose if you're 50% confident.\"\n",
    "\n",
    "- **Precision (0.19)**: When our model predicts the positive class, it's right about 19% of the time. This means about 81% of our positive predictions are false alarms.\n",
    "\n",
    "- **Recall (0.35)**: Our model identifies about 35% of the actual positive cases. This means we're missing about 65% of the positive cases.\n",
    "\n",
    "- **F1 Score (0.24)**: This is the harmonic mean of precision and recall. It gives us a single number that balances both concerns.\n",
    "\n",
    "The key insight is that we've chosen a threshold that gives us fewer false alarms (improved precision) at the cost of missing more positive cases (reduced recall). This is often a reasonable trade-off in many real-world scenarios, especially when false positives are costly or resource-intensive to investigate."
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
