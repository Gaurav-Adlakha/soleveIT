{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01584127",
   "metadata": {
    "input_tokens": 12,
    "output_tokens": 3442,
    "solveit_ai": true
   },
   "source": [
    "lest redo the first 10 cells again\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "## Building a Collaborative Filtering System from Scratch\n",
    "\n",
    "First, let's import the necessary libraries:\n",
    "\n",
    "```python\n",
    "from fastai.imports import *\n",
    "from fastai.collab import *\n",
    "from fastai.tabular.all import *\n",
    "set_seed(42)\n",
    "```\n",
    "\n",
    "We'll use the MovieLens 100k dataset, which contains 100,000 movie ratings:\n",
    "\n",
    "```python\n",
    "path = untar_data(URLs.ML_100k)\n",
    "path.ls()\n",
    "```\n",
    "\n",
    "```\n",
    "(#23) [Path('/Users/gaurav.adlakha/.fastai/data/ml-100k/u.item'),Path('/Users/gaurav.adlakha/.fastai/data/ml-100k/u3.test'),Path('/Users/gaurav.adlakha/.fastai/data/ml-100k/u1.base'),Path('/Users/gaurav.adlakha/.fastai/data/ml-100k/u.info'),Path('/Users/gaurav.adlakha/.fastai/data/ml-100k/u2.test'),Path('/Users/gaurav.adlakha/.fastai/data/ml-100k/u5.test'),Path('/Users/gaurav.adlakha/.fastai/data/ml-100k/u.genre'),Path('/Users/gaurav.adlakha/.fastai/data/ml-100k/ub.test'),Path('/Users/gaurav.adlakha/.fastai/data/ml-100k/ua.base'),Path('/Users/gaurav.adlakha/.fastai/data/ml-100k/u.data'),Path('/Users/gaurav.adlakha/.fastai/data/ml-100k/README'),Path('/Users/gaurav.adlakha/.fastai/data/ml-100k/u4.test'),Path('/Users/gaurav.adlakha/.fastai/data/ml-100k/u5.base'),Path('/Users/gaurav.adlakha/.fastai/data/ml-100k/ub.base'),Path('/Users/gaurav.adlakha/.fastai/data/ml-100k/ua.test'),Path('/Users/gaurav.adlakha/.fastai/data/ml-100k/u4.base'),Path('/Users/gaurav.adlakha/.fastai/data/ml-100k/u.user'),Path('/Users/gaurav.adlakha/.fastai/data/ml-100k/allbut.pl'),Path('/Users/gaurav.adlakha/.fastai/data/ml-100k/u3.base'),Path('/Users/gaurav.adlakha/.fastai/data/ml-100k/u1.test')...]\n",
    "```\n",
    "\n",
    "Let's load the ratings data:\n",
    "\n",
    "```python\n",
    "ratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None,\n",
    "                      names=['user','movie','rating','timestamp'])\n",
    "ratings.head()\n",
    "```\n",
    "\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>user</th>\n",
    "      <th>movie</th>\n",
    "      <th>rating</th>\n",
    "      <th>timestamp</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>196</td>\n",
    "      <td>242</td>\n",
    "      <td>3</td>\n",
    "      <td>881250949</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>186</td>\n",
    "      <td>302</td>\n",
    "      <td>3</td>\n",
    "      <td>891717742</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>22</td>\n",
    "      <td>377</td>\n",
    "      <td>1</td>\n",
    "      <td>878887116</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>244</td>\n",
    "      <td>51</td>\n",
    "      <td>2</td>\n",
    "      <td>880606923</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>166</td>\n",
    "      <td>346</td>\n",
    "      <td>1</td>\n",
    "      <td>886397596</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "## Visualizing User-Movie Ratings\n",
    "\n",
    "Let's create a cross-tabulation to see how specific users rated specific movies:\n",
    "\n",
    "```python\n",
    "def get_top_ratings(df, n_users=20, n_movies=20):\n",
    "    \"Return crosstab of ratings from top users for top movies\"\n",
    "    top_users = df.user.value_counts().index[:n_users]\n",
    "    top_movies = df.movie.value_counts().index[:n_movies]\n",
    "    filtered = df[(df.user.isin(top_users)) & (df.movie.isin(top_movies))]\n",
    "    return pd.crosstab(filtered.user, filtered.movie, filtered.rating, aggfunc='mean')\n",
    "\n",
    "ratings_matrix = get_top_ratings(ratings)\n",
    "```\n",
    "\n",
    "```python\n",
    "ratings_matrix\n",
    "```\n",
    "\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>movie</th>\n",
    "      <th>1</th>\n",
    "      <th>7</th>\n",
    "      <th>50</th>\n",
    "      <th>56</th>\n",
    "      <th>98</th>\n",
    "      <th>100</th>\n",
    "      <th>117</th>\n",
    "      <th>121</th>\n",
    "      <th>127</th>\n",
    "      <th>172</th>\n",
    "      <th>174</th>\n",
    "      <th>181</th>\n",
    "      <th>204</th>\n",
    "      <th>222</th>\n",
    "      <th>237</th>\n",
    "      <th>258</th>\n",
    "      <th>286</th>\n",
    "      <th>288</th>\n",
    "      <th>294</th>\n",
    "      <th>300</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>user</th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>NaN</td>\n",
    "      <td>5.0</td>\n",
    "      <td>5.0</td>\n",
    "      <td>5.0</td>\n",
    "      <td>4.0</td>\n",
    "      <td>5.0</td>\n",
    "      <td>NaN</td>\n",
    "      <td>5.0</td>\n",
    "      <td>5.0</td>\n",
    "      <td>4.0</td>\n",
    "      <td>5.0</td>\n",
    "      <td>3.0</td>\n",
    "      <td>5.0</td>\n",
    "      <td>NaN</td>\n",
    "      <td>5.0</td>\n",
    "      <td>4.0</td>\n",
    "      <td>4.0</td>\n",
    "      <td>4.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>4.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>13</th>\n",
    "      <td>3.0</td>\n",
    "      <td>2.0</td>\n",
    "      <td>5.0</td>\n",
    "      <td>5.0</td>\n",
    "      <td>4.0</td>\n",
    "      <td>5.0</td>\n",
    "      <td>3.0</td>\n",
    "      <td>5.0</td>\n",
    "      <td>5.0</td>\n",
    "      <td>5.0</td>\n",
    "      <td>4.0</td>\n",
    "      <td>5.0</td>\n",
    "      <td>5.0</td>\n",
    "      <td>3.0</td>\n",
    "      <td>5.0</td>\n",
    "      <td>4.0</td>\n",
    "      <td>3.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>2.0</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>92</th>\n",
    "      <td>4.0</td>\n",
    "      <td>4.0</td>\n",
    "      <td>5.0</td>\n",
    "      <td>5.0</td>\n",
    "      <td>5.0</td>\n",
    "      <td>5.0</td>\n",
    "      <td>4.0</td>\n",
    "      <td>5.0</td>\n",
    "      <td>NaN</td>\n",
    "      <td>4.0</td>\n",
    "      <td>5.0</td>\n",
    "      <td>4.0</td>\n",
    "      <td>4.0</td>\n",
    "      <td>4.0</td>\n",
    "      <td>4.0</td>\n",
    "      <td>4.0</td>\n",
    "      <td>NaN</td>\n",
    "      <td>3.0</td>\n",
    "      <td>3.0</td>\n",
    "      <td>NaN</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "## The Math Behind Collaborative Filtering\n",
    "\n",
    "We can represent users and movies as vectors in a latent feature space:\n",
    "\n",
    "```python\n",
    "movie = np.array([0.98, 0.9, -0.9])\n",
    "user = np.array([0.9, 0.8, -0.6])\n",
    "(user*movie).sum()\n",
    "```\n",
    "\n",
    "```\n",
    "np.float64(2.1420000000000003)\n",
    "```\n",
    "\n",
    "The positive dot product (2.142) suggests this user would enjoy this movie. Each dimension represents a latent feature (e.g., action, drama, romance) that the algorithm learns automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47959115",
   "metadata": {
    "input_tokens": 19
   },
   "outputs": [],
   "source": [
    "cells_11_20= nb.cells[11:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c449932",
   "metadata": {
    "input_tokens": 39,
    "output_tokens": 3369,
    "solveit_ai": true
   },
   "source": [
    "lest redo the next 10 cell $`cells_11_20` it okay to add more info if its adding values \n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "## Understanding User Preferences with Dot Products\n",
    "\n",
    "Let's see how a different user might feel about the same movie:\n",
    "\n",
    "```python\n",
    "movie = np.array([0.98, 0.9, -0.9])\n",
    "user = np.array([0.1, -1.0, -0.6])\n",
    "(user*movie).sum()\n",
    "```\n",
    "\n",
    "```\n",
    "np.float64(-0.262)\n",
    "```\n",
    "\n",
    "The negative dot product (-0.262) suggests this user would probably dislike this movie. Notice how this user has a negative value in the second dimension, opposing the movie's positive value there - this mismatch contributes to the negative overall score.\n",
    "\n",
    "## Adding Movie Titles to Our Dataset\n",
    "\n",
    "To make our analysis more interpretable, let's add movie titles to our dataset:\n",
    "\n",
    "```python\n",
    "movies = pd.read_csv(path/'u.item', delimiter='|', header=None, encoding='latin1', \n",
    "                    usecols=[0,1], names=['movie','title'])\n",
    "movies.head()\n",
    "```\n",
    "\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>movie</th>\n",
    "      <th>title</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>1</td>\n",
    "      <td>Toy Story (1995)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>2</td>\n",
    "      <td>GoldenEye (1995)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>3</td>\n",
    "      <td>Four Rooms (1995)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>4</td>\n",
    "      <td>Get Shorty (1995)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>5</td>\n",
    "      <td>Copycat (1995)</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "Now we can see our full ratings dataset:\n",
    "\n",
    "```python\n",
    "ratings\n",
    "```\n",
    "\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>user</th>\n",
    "      <th>movie</th>\n",
    "      <th>rating</th>\n",
    "      <th>timestamp</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>196</td>\n",
    "      <td>242</td>\n",
    "      <td>3</td>\n",
    "      <td>881250949</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>186</td>\n",
    "      <td>302</td>\n",
    "      <td>3</td>\n",
    "      <td>891717742</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>22</td>\n",
    "      <td>377</td>\n",
    "      <td>1</td>\n",
    "      <td>878887116</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>244</td>\n",
    "      <td>51</td>\n",
    "      <td>2</td>\n",
    "      <td>880606923</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>166</td>\n",
    "      <td>346</td>\n",
    "      <td>1</td>\n",
    "      <td>886397596</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>...</th>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>99995</th>\n",
    "      <td>880</td>\n",
    "      <td>476</td>\n",
    "      <td>3</td>\n",
    "      <td>880175444</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>99996</th>\n",
    "      <td>716</td>\n",
    "      <td>204</td>\n",
    "      <td>5</td>\n",
    "      <td>879795543</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>99997</th>\n",
    "      <td>276</td>\n",
    "      <td>1090</td>\n",
    "      <td>1</td>\n",
    "      <td>874795795</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>99998</th>\n",
    "      <td>13</td>\n",
    "      <td>225</td>\n",
    "      <td>2</td>\n",
    "      <td>882399156</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>99999</th>\n",
    "      <td>12</td>\n",
    "      <td>203</td>\n",
    "      <td>3</td>\n",
    "      <td>879959583</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "<p>100000 rows × 4 columns</p>\n",
    "</div>\n",
    "\n",
    "Let's merge the movie titles into our ratings dataframe:\n",
    "\n",
    "```python\n",
    "ratings = ratings.merge(movies)\n",
    "ratings\n",
    "```\n",
    "\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>user</th>\n",
    "      <th>movie</th>\n",
    "      <th>rating</th>\n",
    "      <th>timestamp</th>\n",
    "      <th>title</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>196</td>\n",
    "      <td>242</td>\n",
    "      <td>3</td>\n",
    "      <td>881250949</td>\n",
    "      <td>Kolya (1996)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>186</td>\n",
    "      <td>302</td>\n",
    "      <td>3</td>\n",
    "      <td>891717742</td>\n",
    "      <td>L.A. Confidential (1997)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>22</td>\n",
    "      <td>377</td>\n",
    "      <td>1</td>\n",
    "      <td>878887116</td>\n",
    "      <td>Heavyweights (1994)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>244</td>\n",
    "      <td>51</td>\n",
    "      <td>2</td>\n",
    "      <td>880606923</td>\n",
    "      <td>Legends of the Fall (1994)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>166</td>\n",
    "      <td>346</td>\n",
    "      <td>1</td>\n",
    "      <td>886397596</td>\n",
    "      <td>Jackie Brown (1997)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>...</th>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>99995</th>\n",
    "      <td>880</td>\n",
    "      <td>476</td>\n",
    "      <td>3</td>\n",
    "      <td>880175444</td>\n",
    "      <td>First Wives Club, The (1996)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>99996</th>\n",
    "      <td>716</td>\n",
    "      <td>204</td>\n",
    "      <td>5</td>\n",
    "      <td>879795543</td>\n",
    "      <td>Back to the Future (1985)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>99997</th>\n",
    "      <td>276</td>\n",
    "      <td>1090</td>\n",
    "      <td>1</td>\n",
    "      <td>874795795</td>\n",
    "      <td>Sliver (1993)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>99998</th>\n",
    "      <td>13</td>\n",
    "      <td>225</td>\n",
    "      <td>2</td>\n",
    "      <td>882399156</td>\n",
    "      <td>101 Dalmatians (1996)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>99999</th>\n",
    "      <td>12</td>\n",
    "      <td>203</td>\n",
    "      <td>3</td>\n",
    "      <td>879959583</td>\n",
    "      <td>Unforgiven (1992)</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "<p>100000 rows × 5 columns</p>\n",
    "</div>\n",
    "\n",
    "Now we can see that user 196 gave \"Kolya (1996)\" a rating of 3, and user 716 gave \"Back to the Future (1985)\" a perfect 5.\n",
    "\n",
    "## Building the Collaborative Filtering Model\n",
    "\n",
    "Let's try to create a DataLoader using fastai's collaborative filtering tools:\n",
    "\n",
    "```python\n",
    "dls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\n",
    "dls.show_batch()\n",
    "```\n",
    "\n",
    "Once the DataLoader is created, we would access information about our dataset:\n",
    "\n",
    "```python\n",
    "n_users = len(dls.classes['user'])\n",
    "n_movies = len(dls.classes['title'])\n",
    "n_movies\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005ec10d",
   "metadata": {
    "input_tokens": 19
   },
   "outputs": [],
   "source": [
    "cells_20_30= nb.cells[20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d7a42",
   "metadata": {
    "input_tokens": 39,
    "output_tokens": 1135,
    "solveit_ai": true
   },
   "source": [
    "lest redo the next 10 cell $`cells_20_30` always render the output cell as well like previous steps..\n",
    "\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "## Creating and Exploring Embedding Factors\n",
    "\n",
    "Once we have our DataLoader set up, let's examine the dataset dimensions:\n",
    "\n",
    "```python\n",
    "n_users\n",
    "```\n",
    "\n",
    "```\n",
    "944\n",
    "```\n",
    "\n",
    "We have 944 unique users in our dataset. Now let's create random embedding factors for both users and movies:\n",
    "\n",
    "```python\n",
    "user_factor = torch.randn(n_users, 5)\n",
    "movie_factor = torch.randn(n_movies, 5)\n",
    "```\n",
    "\n",
    "These are randomly initialized tensors that will serve as our latent factors. Each user and movie is represented by a 5-dimensional vector.\n",
    "\n",
    "Let's check the shapes of these tensors:\n",
    "\n",
    "```python\n",
    "user_factor.shape\n",
    "```\n",
    "\n",
    "```\n",
    "torch.Size([944, 5])\n",
    "```\n",
    "\n",
    "```python\n",
    "movie_factor.shape\n",
    "```\n",
    "\n",
    "```\n",
    "torch.Size([1665, 5])\n",
    "```\n",
    "\n",
    "This confirms we have 944 users and 1665 movies, each represented by 5 latent factors.\n",
    "\n",
    "Let's look at a specific user's embedding:\n",
    "\n",
    "```python\n",
    "torch.embedding(user_factor, torch.tensor(5))\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([-1.2018, -1.2946, -1.8869,  1.2259,  0.2970])\n",
    "```\n",
    "\n",
    "Instead of manually indexing, we can use PyTorch's Embedding layer, which is designed for this purpose:\n",
    "\n",
    "```python\n",
    "user_emb = nn.Embedding(n_users, 5)\n",
    "```\n",
    "\n",
    "Now we can access user embeddings with a cleaner syntax:\n",
    "\n",
    "```python\n",
    "user_emb.weight[10]\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([-1.0398, -1.7286, -0.6798,  2.5864, -0.1015],\n",
    "       grad_fn=<SelectBackward0>)\n",
    "```\n",
    "\n",
    "Let's compare with our original random factors:\n",
    "\n",
    "```python\n",
    "user_factor\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([[-1.0827,  0.2138,  0.9310, -0.2739, -0.4359],\n",
    "        [-0.5195,  0.7613, -0.4365,  0.1365,  1.3300],\n",
    "        [-1.2804,  0.0705,  0.6489, -1.2110,  1.8266],\n",
    "        ...,\n",
    "        [ 0.8009, -0.4734, -0.8962, -0.7348, -0.0246],\n",
    "        [ 0.3354, -0.8262, -0.1541,  0.4699,  0.4873],\n",
    "        [ 2.4054, -0.2156, -1.4126, -0.2467,  1.0571]])\n",
    "```\n",
    "\n",
    "And check a specific user:\n",
    "\n",
    "```python\n",
    "user_factor[10]\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([-0.5753,  0.1556, -0.3694,  0.4986, -2.5438])\n",
    "```\n",
    "\n",
    "The `nn.Embedding` layer is initialized with different random values than our manual initialization:\n",
    "\n",
    "```python\n",
    "user_emb\n",
    "```\n",
    "\n",
    "```\n",
    "Embedding(944, 5)\n",
    "```\n",
    "\n",
    "These embedding layers are the core of collaborative filtering models. During training, these initially random values will be updated to reflect the patterns in the rating data. Users with similar tastes will end up with similar embedding vectors, and movies with similar appeal will likewise have similar embeddings.\n",
    "\n",
    "The dot product between a user's embedding and a movie's embedding will predict the rating that user would give to that movie. By training on known ratings, the model learns to generalize to predict unknown ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3cb905",
   "metadata": {
    "input_tokens": 19
   },
   "outputs": [],
   "source": [
    "cells_30_40= nb.cells[30:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d11865",
   "metadata": {
    "input_tokens": 31,
    "output_tokens": 1771,
    "solveit_ai": true
   },
   "source": [
    "lets do the next 10 cell $`cells_30_40` always render the output cell \n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "## Understanding Embedding Operations\n",
    "\n",
    "Taking the dot product with a one-hot encoding of a vector is the same as looking up that vector at a particular index.\n",
    "\n",
    "Let's explore this concept:\n",
    "\n",
    "```python\n",
    "one_hot(3, 100)\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0], dtype=torch.uint8)\n",
    "```\n",
    "\n",
    "This creates a one-hot vector where the 3rd position (0-indexed) is 1 and all other positions are 0. Let's convert it to float for matrix operations:\n",
    "\n",
    "```python\n",
    "one_hot(3, 100).float()\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "```\n",
    "\n",
    "Now let's look at a specific embedding weight:\n",
    "\n",
    "```python\n",
    "user_emb.weight[10]\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([-1.0398, -1.7286, -0.6798,  2.5864, -0.1015],\n",
    "       grad_fn=<SelectBackward0>)\n",
    "```\n",
    "\n",
    "And check the embedding layer:\n",
    "\n",
    "```python\n",
    "user_emb\n",
    "```\n",
    "\n",
    "```\n",
    "Embedding(944, 5)\n",
    "```\n",
    "\n",
    "The weight matrix has shape:\n",
    "\n",
    "```python\n",
    "user_emb.weight.shape\n",
    "```\n",
    "\n",
    "```\n",
    "torch.Size([944, 5])\n",
    "```\n",
    "\n",
    "Let's create a one-hot vector for user 10:\n",
    "\n",
    "```python\n",
    "one_hot(10, n_users).float().shape\n",
    "```\n",
    "\n",
    "```\n",
    "torch.Size([944])\n",
    "```\n",
    "\n",
    "Now we can demonstrate that matrix multiplication with a one-hot vector is equivalent to selecting that row:\n",
    "\n",
    "```python\n",
    "user_emb.weight.t() @ one_hot(10, n_users).float()\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([-1.0398, -1.7286, -0.6798,  2.5864, -0.1015], grad_fn=<MvBackward0>)\n",
    "```\n",
    "\n",
    "This gives us the same result as directly indexing `user_emb.weight[10]`. This is the mathematical foundation for how embedding lookups work.\n",
    "\n",
    "## What is Embedding?\n",
    "\n",
    "An embedding is a learned mapping from discrete objects (like users or movies) to vectors of continuous numbers in a lower-dimensional space.\n",
    "\n",
    "```python\n",
    "u_e = Embedding(944, 5)\n",
    "```\n",
    "\n",
    "This creates an embedding layer for 944 users, each represented by a 5-dimensional vector. The embedding layer is essentially a lookup table where:\n",
    "- Each row corresponds to a user (or item)\n",
    "- Each column represents a learned latent feature\n",
    "- During training, these values are updated to capture patterns in the data\n",
    "\n",
    "Embeddings are powerful because they:\n",
    "1. Convert sparse, high-dimensional one-hot encodings into dense, low-dimensional vectors\n",
    "2. Learn meaningful relationships between entities\n",
    "3. Allow us to measure similarity between entities using vector operations like dot products\n",
    "\n",
    "In our collaborative filtering model, the dot product between a user embedding and a movie embedding produces a predicted rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164d5ef0",
   "metadata": {
    "input_tokens": 19
   },
   "outputs": [],
   "source": [
    "cells_40_50= nb.cells[40:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebf9293",
   "metadata": {
    "input_tokens": 30,
    "output_tokens": 1449,
    "solveit_ai": true
   },
   "source": [
    "lets do the next 10 cell $`cells_40_50` always render the output cell\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "## Building the Collaborative Filtering Model\n",
    "\n",
    "Let's examine the weights of our embedding layer:\n",
    "\n",
    "```python\n",
    "u_e.weight[10]\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([0.0063, 0.0057, 0.0053, 0.0052, 0.0039], grad_fn=<SelectBackward0>)\n",
    "```\n",
    "\n",
    "Now let's look at our data loaders:\n",
    "\n",
    "```python\n",
    "dls\n",
    "```\n",
    "\n",
    "```\n",
    "<fastai.tabular.data.TabularDataLoaders at 0x7a1ca6e53650>\n",
    "```\n",
    "\n",
    "We can access the training data loader:\n",
    "\n",
    "```python\n",
    "dls[0]\n",
    "```\n",
    "\n",
    "```\n",
    "<fastai.tabular.core.TabWeightedDL at 0x7a1d3812a410>\n",
    "```\n",
    "\n",
    "Let's confirm our data loaders again:\n",
    "\n",
    "```python\n",
    "dls\n",
    "```\n",
    "\n",
    "```\n",
    "<fastai.tabular.data.TabularDataLoaders at 0x7a1ca6e53650>\n",
    "```\n",
    "\n",
    "Now let's get a batch of data:\n",
    "\n",
    "```python\n",
    "batch = dls.one_batch()\n",
    "```\n",
    "\n",
    "Let's check the dimensions of our dataset:\n",
    "\n",
    "```python\n",
    "944, 1665\n",
    "```\n",
    "\n",
    "```\n",
    "(944, 1665)\n",
    "```\n",
    "\n",
    "This confirms we have 944 users and 1665 movies.\n",
    "\n",
    "Let's examine the movie indices in our batch:\n",
    "\n",
    "```python\n",
    "batch[0][:,1]\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([1330,  899,  230, 1391,  334, 1133,  897,  466,  668,  102,  236, 1443,\n",
    "         528,  320, 1247,  256,  769,  143,  271, 1397,  210, 1544, 1442,  529,\n",
    "          17,  611, 1052,  485,  623, 1525,  938,  503, 1544,   65,  816, 1227,\n",
    "          93,  499,  179, 1179,  588, 1019,  304,    5,  710,  457,  861, 1006,\n",
    "         320,  578,  899,   62,  177,  279,  328, 1496,  570, 1252, 1216, 1402,\n",
    "         884,  457,  738, 1121])\n",
    "```\n",
    "\n",
    "Let's create embeddings for users and movies and check their shapes:\n",
    "\n",
    "```python\n",
    "us_em = Embedding(944, 5)\n",
    "mo_em = Embedding(1665, 5)\n",
    "\n",
    "batch = dls.one_batch()\n",
    "\n",
    "x = batch[0][:,0]\n",
    "y = batch[0][:,1]\n",
    "\n",
    "print(us_em(x).shape, mo_em(y).shape)\n",
    "```\n",
    "\n",
    "```\n",
    "torch.Size([64, 5]) torch.Size([64, 5])\n",
    "```\n",
    "\n",
    "This shows that for a batch of 64 examples, we get 64 user embeddings and 64 movie embeddings, each with 5 dimensions.\n",
    "\n",
    "Now let's create a complete collaborative filtering model:\n",
    "\n",
    "```python\n",
    "class CollabNN(nn.Module):\n",
    "    \"Simple collaborative filtering model with embeddings\"\n",
    "    def __init__(self, n_users, n_items, n_factors=5):\n",
    "        super().__init__()\n",
    "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
    "        self.item_factors = nn.Embedding(n_items, n_factors)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users, items = x[:,0], x[:,1]\n",
    "        u_embs = self.user_factors(users)\n",
    "        i_embs = self.item_factors(items)\n",
    "        return (u_embs * i_embs).sum(dim=1)\n",
    "```\n",
    "\n",
    "This model:\n",
    "1. Creates embedding layers for users and items\n",
    "2. In the forward pass, extracts user and item indices from the input\n",
    "3. Looks up the corresponding embeddings\n",
    "4. Computes the element-wise product of user and item embeddings\n",
    "5. Sums across the embedding dimensions to get a predicted rating\n",
    "\n",
    "Finally, let's instantiate our model and prepare a batch for training:\n",
    "\n",
    "```python\n",
    "model = CollabNN(n_users, n_movies)\n",
    "batch = dls.one_batch()\n",
    "```\n",
    "\n",
    "This collaborative filtering model is remarkably simple yet powerful. It learns to represent users and movies in a shared latent space where the dot product of vectors predicts ratings. During training, the model will adjust these embeddings to minimize prediction error on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c291628b",
   "metadata": {
    "input_tokens": 19
   },
   "outputs": [],
   "source": [
    "cells_50_60= nb.cells[50:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16603ad",
   "metadata": {
    "input_tokens": 30,
    "output_tokens": 2452,
    "solveit_ai": true
   },
   "source": [
    "lets do the next 10 cell $`cells_50_60` always render the output cell\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "## Training and Evaluating the Model\n",
    "\n",
    "Now let's use our model to make predictions on a batch of data:\n",
    "\n",
    "```python\n",
    "model(batch[0])\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([ 7.0029,  2.1804,  0.4648, -0.3704, -3.6570, -2.5748, -1.2354, -0.4618,\n",
    "         0.8670, -0.7257, -1.3684,  0.6179,  0.6468,  2.9299,  1.4910, -0.4101,\n",
    "         0.6326, -0.6567,  2.2093,  2.4966, -5.0130, -3.6183,  3.9021,  4.4451,\n",
    "        -0.0432, -2.0280, -3.6852, -5.9757, -1.5701, -1.1312,  0.8875, -1.5192,\n",
    "        -1.2604, -0.9187, -1.3469, -0.6555, -1.2011, -0.6149,  0.3042,  1.4095,\n",
    "        -1.7217,  0.3008, -0.0148,  0.3080,  2.2792,  3.7195, -0.1592, -0.6061,\n",
    "         1.7568, -0.7674,  0.2440, -0.9074, -1.0106, -3.1345,  0.0641,  1.2300,\n",
    "         3.4579, -0.4415, -1.4399, -3.0345,  1.6182, -1.2363, -1.8696, -1.8537],\n",
    "       grad_fn=<SumBackward1>)\n",
    "```\n",
    "\n",
    "Let's check the shape of our predictions:\n",
    "\n",
    "```python\n",
    "model(batch[0]).shape\n",
    "```\n",
    "\n",
    "```\n",
    "torch.Size([64])\n",
    "```\n",
    "\n",
    "The model outputs a single prediction for each of the 64 examples in our batch.\n",
    "\n",
    "Now let's create a learner to train our model:\n",
    "\n",
    "```python\n",
    "mdl = CollabNN(n_users, n_movies)\n",
    "learner = Learner(dls, mdl, loss_func=MSELossFlat())\n",
    "```\n",
    "\n",
    "Let's train the model for 5 epochs using the one-cycle policy:\n",
    "\n",
    "```python\n",
    "learner.fit_one_cycle(5, 5e-3)\n",
    "```\n",
    "\n",
    "```\n",
    "epoch     train_loss  valid_loss  time    \n",
    "0         16.511957   16.503958   00:05     \n",
    "1         12.656537   13.187530   00:05     \n",
    "2         4.729345    5.090328    00:06     \n",
    "3         2.651412    3.003635    00:05     \n",
    "4         2.324853    2.789970    00:05     \n",
    "```\n",
    "\n",
    "Let's get a batch of data to make predictions on:\n",
    "\n",
    "```python\n",
    "x, y = dls.one_batch()\n",
    "```\n",
    "\n",
    "Now let's examine the training history:\n",
    "\n",
    "Let's create a more readable DataFrame of our training history:\n",
    "\n",
    "```python\n",
    "# Get the recorded losses\n",
    "losses = learner.recorder.values\n",
    "\n",
    "# Get training and validation losses (first two columns of values)\n",
    "train_losses = [x[0] for x in losses]\n",
    "valid_losses = [x[1] for x in losses]\n",
    "\n",
    "# Create a dataframe to display them\n",
    "pd.DataFrame({\n",
    "    'Epoch': range(1, len(train_losses)+1),\n",
    "    'Training Loss': train_losses,\n",
    "    'Validation Loss': valid_losses\n",
    "})\n",
    "```\n",
    "\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Epoch</th>\n",
    "      <th>Training Loss</th>\n",
    "      <th>Validation Loss</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>1</td>\n",
    "      <td>16.511957</td>\n",
    "      <td>16.503958</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>2</td>\n",
    "      <td>12.656537</td>\n",
    "      <td>13.187530</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>3</td>\n",
    "      <td>4.729345</td>\n",
    "      <td>5.090328</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>4</td>\n",
    "      <td>2.651412</td>\n",
    "      <td>3.003635</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>5</td>\n",
    "      <td>2.324853</td>\n",
    "      <td>2.789970</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "We can see that both training and validation loss decrease significantly over the epochs, indicating that our model is learning patterns in the data.\n",
    "\n",
    "Now let's improve our model by constraining the output range to be between 0 and 5.5 (since ratings are on a 1-5 scale):\n",
    "\n",
    "```python\n",
    "class CollabNN(nn.Module):\n",
    "    \"Simple collaborative filtering model with embeddings\"\n",
    "    def __init__(self, n_users, n_items, n_factors=5, y_range=(0, 5.5)):\n",
    "        super().__init__()\n",
    "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
    "        self.item_factors = nn.Embedding(n_items, n_factors)\n",
    "        self.y_range = y_range\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users, items = x[:,0], x[:,1]\n",
    "        u_embs = self.user_factors(users)\n",
    "        i_embs = self.item_factors(items)\n",
    "        return sigmoid_range((u_embs * i_embs).sum(dim=1), *self.y_range)\n",
    "```\n",
    "\n",
    "Let's create a new model and learner with this improved architecture:\n",
    "\n",
    "```python\n",
    "mdl = CollabNN(n_users, n_movies)\n",
    "learner = Learner(dls, mdl, loss_func=MSELossFlat())\n",
    "```\n",
    "\n",
    "And train it again:\n",
    "\n",
    "```python\n",
    "learner.fit_one_cycle(5, 5e-3)\n",
    "```\n",
    "\n",
    "```\n",
    "epoch     train_loss  valid_loss  time    \n",
    "0         3.651534    3.777338    00:06     \n",
    "1         2.176786    2.387183    00:06     \n",
    "2         1.485974    1.752936    00:06     \n",
    "3         1.194254    1.400067    00:06     \n",
    "4         1.213866    1.356161    00:06     \n",
    "```\n",
    "\n",
    "This model converges much faster and achieves a lower loss. The sigmoid_range function constrains the output to be within the specified range, which helps the model learn more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b3089e",
   "metadata": {
    "input_tokens": 19
   },
   "outputs": [],
   "source": [
    "cells_60_70= nb.cells[60:70]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbfe5ae",
   "metadata": {
    "input_tokens": 30,
    "output_tokens": 2410,
    "solveit_ai": true
   },
   "source": [
    "lets do the next 10 cell $`cells_60_70` always render the output cell\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "## Further Model Analysis and Improvements\n",
    "\n",
    "Let's take a look at the training results of our modified model:\n",
    "\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Epoch</th>\n",
    "      <th>Training Loss</th>\n",
    "      <th>Validation Loss</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>1</td>\n",
    "      <td>3.529655</td>\n",
    "      <td>3.523691</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>2</td>\n",
    "      <td>1.782526</td>\n",
    "      <td>1.922908</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>3</td>\n",
    "      <td>1.169784</td>\n",
    "      <td>1.352898</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>4</td>\n",
    "      <td>1.037823</td>\n",
    "      <td>1.212110</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>5</td>\n",
    "      <td>1.031219</td>\n",
    "      <td>1.192636</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "Let's test the sigmoid_range function with different values:\n",
    "\n",
    "```python\n",
    "x = torch.tensor([1, 2, 3, 4, 5, 10, 15, 100, 5.5])\n",
    "```\n",
    "\n",
    "```python\n",
    "sigmoid_range(x, *(0, 5.5))\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([4.0208, 4.8444, 5.2392, 5.4011, 5.4632, 5.4998, 5.5000, 5.5000, 5.4776])\n",
    "```\n",
    "\n",
    "This shows how the sigmoid_range function maps values to the range [0, 5.5]. Note that extreme values (like 100) get clamped to the maximum of 5.5.\n",
    "\n",
    "\n",
    "```python\n",
    "class ModifiedCollabNN(nn.Module):\n",
    "    \"Collaborative filtering model with embeddings and bias terms\"\n",
    "    def __init__(self, n_users, n_items, n_factors=5, y_range=(0,5.5)):\n",
    "        super().__init__()\n",
    "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
    "        self.item_factors = nn.Embedding(n_items, n_factors)\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.item_bias = nn.Embedding(n_items, 1)\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        self.y_range = y_range\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users, items = x[:,0], x[:,1]\n",
    "        u_embs = self.user_factors(users)\n",
    "        i_embs = self.item_factors(items)\n",
    "        u_bias = self.user_bias(users).squeeze()\n",
    "        i_bias = self.item_bias(items).squeeze()\n",
    "        dot = (u_embs * i_embs).sum(dim=1)\n",
    "        return sigmoid_range(dot + u_bias + i_bias + self.bias, *self.y_range)\n",
    "```\n",
    "\n",
    "This enhanced model includes:\n",
    "1. User and movie embeddings (as before)\n",
    "2. User bias terms (some users rate everything higher/lower)\n",
    "3. Movie bias terms (some movies are generally rated higher/lower)\n",
    "4. Global bias term (overall average rating)\n",
    "\n",
    "Let's create a new model and learner with this architecture:\n",
    "\n",
    "```python\n",
    "mdl = ModifiedCollabNN(n_users, n_movies)\n",
    "learner = Learner(dls, mdl, loss_func=MSELossFlat())\n",
    "```\n",
    "\n",
    "Let's train this improved model:\n",
    "\n",
    "```python\n",
    "learner.fit_one_cycle(5, 5e-3)\n",
    "```\n",
    "\n",
    "```\n",
    "epoch     train_loss  valid_loss  time    \n",
    "0         3.529655    3.523691    00:06     \n",
    "1         1.782526    1.922908    00:06     \n",
    "2         1.169784    1.352898    00:06     \n",
    "3         1.037823    1.212110    00:06     \n",
    "4         1.031219    1.192636    00:06     \n",
    "```\n",
    "\n",
    "Let's check the final training results:\n",
    "\n",
    "```python\n",
    "losses = learner.recorder.values\n",
    "\n",
    "# Get training and validation losses (first two columns of values)\n",
    "train_losses = [x[0] for x in losses]\n",
    "valid_losses = [x[1] for x in losses]\n",
    "\n",
    "# Create a dataframe to display them\n",
    "pd.DataFrame({\n",
    "    'Epoch': range(1, len(train_losses)+1),\n",
    "    'Training Loss': train_losses,\n",
    "    'Validation Loss': valid_losses\n",
    "})\n",
    "```\n",
    "\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Epoch</th>\n",
    "      <th>Training Loss</th>\n",
    "      <th>Validation Loss</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>1</td>\n",
    "      <td>3.529655</td>\n",
    "      <td>3.523691</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>2</td>\n",
    "      <td>1.782526</td>\n",
    "      <td>1.922908</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>3</td>\n",
    "      <td>1.169784</td>\n",
    "      <td>1.352898</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>4</td>\n",
    "      <td>1.037823</td>\n",
    "      <td>1.212110</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>5</td>\n",
    "      <td>1.031219</td>\n",
    "      <td>1.192636</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "Our modified model with bias terms achieves a validation loss of about 1.19, which is a significant improvement over the initial model. This demonstrates the effectiveness of including bias terms in collaborative filtering models.\n",
    "\n",
    "The bias terms allow the model to capture general tendencies:\n",
    "- Some users consistently rate movies higher or lower than average\n",
    "- Some movies are generally rated better or worse than average\n",
    "- There's an overall baseline rating in the system\n",
    "\n",
    "By combining these bias terms with the dot product of user and movie embeddings, we get a more accurate and nuanced prediction system."
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "concise",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
