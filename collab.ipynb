{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2343fd7",
   "metadata": {
    "input_tokens": 48,
    "output_tokens": 226,
    "solveit_ai": true
   },
   "source": [
    "Create two 10x10 square matrices. Then, calculate the root mean square between them, followed by the gradient using the last loss backward method.(torch)\n",
    "\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "I'll create two 10x10 matrices in PyTorch and calculate the root mean square difference between them, then compute the gradient.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Create two 10x10 matrices\n",
    "a = torch.randn(10, 10, requires_grad=True)\n",
    "b = torch.randn(10, 10)\n",
    "\n",
    "# Calculate root mean square difference\n",
    "diff = a - b\n",
    "rms = torch.sqrt(torch.mean(diff**2))\n",
    "print(f\"RMS: {rms.item():.4f}\")\n",
    "\n",
    "# Compute gradient\n",
    "rms.backward()\n",
    "print(f\"Gradient shape: {a.grad.shape}\")\n",
    "```\n",
    "\n",
    "This creates random matrices a and b, computes their difference, calculates the RMS, and then computes the gradient with respect to a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cfff2f",
   "metadata": {
    "input_tokens": 141,
    "output_tokens": 48,
    "time_run": "7:24:10a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS: 1.4285\n",
      "Gradient shape: torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create two 10x10 matrices\n",
    "a = torch.randn(10, 10, requires_grad=True)\n",
    "b = torch.randn(10, 10)\n",
    "\n",
    "# Calculate root mean square difference\n",
    "diff = a - b\n",
    "rms = torch.sqrt(torch.mean(diff**2))\n",
    "print(f\"RMS: {rms.item():.4f}\")\n",
    "\n",
    "# Compute gradient\n",
    "rms.backward()\n",
    "print(f\"Gradient shape: {a.grad.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5769daa",
   "metadata": {
    "input_tokens": 7,
    "output_tokens": 721,
    "time_run": "7:25:25a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-e1df496515ab>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  dir(rms.grad)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__bool__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(rms.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ab1d38",
   "metadata": {
    "input_tokens": 12,
    "output_tokens": 168,
    "solveit_ai": true,
    "time_run": "7:26:01a"
   },
   "source": [
    "How do I print the grad values?\n",
    "\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "To print the gradient values, you need to look at `a.grad` since `a` is the tensor we set `requires_grad=True` for. Let's print those values:\n",
    "\n",
    "```python\n",
    "print(a.grad)\n",
    "```\n",
    "\n",
    "If you want a more formatted output or just specific values:\n",
    "\n",
    "```python\n",
    "print(f\"Gradient mean: {a.grad.mean().item():.6f}\")\n",
    "print(f\"Gradient min: {a.grad.min().item():.6f}\")\n",
    "print(f\"Gradient max: {a.grad.max().item():.6f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882d4ff4",
   "metadata": {
    "input_tokens": 6,
    "output_tokens": 1032,
    "time_run": "7:26:21a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0059, -0.0072,  0.0110,  0.0027,  0.0015,  0.0162, -0.0002,  0.0055,\n",
      "         -0.0213, -0.0074],\n",
      "        [ 0.0012,  0.0182,  0.0012, -0.0165,  0.0139,  0.0134,  0.0083,  0.0085,\n",
      "         -0.0044,  0.0003],\n",
      "        [ 0.0087, -0.0096,  0.0120,  0.0168,  0.0132,  0.0074,  0.0006, -0.0205,\n",
      "         -0.0078, -0.0168],\n",
      "        [ 0.0062, -0.0025,  0.0058, -0.0031, -0.0046,  0.0021,  0.0058,  0.0050,\n",
      "         -0.0034,  0.0151],\n",
      "        [-0.0115,  0.0086, -0.0039,  0.0074,  0.0037, -0.0072, -0.0071,  0.0206,\n",
      "         -0.0138, -0.0025],\n",
      "        [ 0.0054, -0.0060,  0.0121,  0.0015,  0.0055, -0.0108,  0.0053, -0.0234,\n",
      "          0.0152,  0.0099],\n",
      "        [ 0.0023,  0.0033, -0.0066,  0.0088, -0.0023, -0.0118, -0.0166,  0.0020,\n",
      "          0.0104,  0.0069],\n",
      "        [-0.0037,  0.0052,  0.0162,  0.0091, -0.0089,  0.0167,  0.0059,  0.0126,\n",
      "         -0.0208,  0.0053],\n",
      "        [-0.0034, -0.0005,  0.0036,  0.0144, -0.0107,  0.0054, -0.0042, -0.0040,\n",
      "          0.0040, -0.0103],\n",
      "        [ 0.0106, -0.0095, -0.0193,  0.0071, -0.0047,  0.0015,  0.0054, -0.0143,\n",
      "          0.0046,  0.0015]])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fadb0e",
   "metadata": {
    "input_tokens": 76,
    "output_tokens": 58,
    "time_run": "7:26:31a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient mean: 0.000898\n",
      "Gradient min: -0.023412\n",
      "Gradient max: 0.020619\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient mean: {a.grad.mean().item():.6f}\")\n",
    "print(f\"Gradient min: {a.grad.min().item():.6f}\")\n",
    "print(f\"Gradient max: {a.grad.max().item():.6f}\")"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "concise",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
